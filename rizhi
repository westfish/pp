nohup: ignoring input
============================= test session starts ==============================
platform linux -- Python 3.7.16, pytest-7.2.2, pluggy-1.0.0 -- /root/anaconda3/envs/benchmark/bin/python
cachedir: .pytest_cache
rootdir: /root/mttest/testall
plugins: anyio-3.6.2
collecting ... collected 590 items

tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_all_is_compatible PASSED [  0%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_all_is_compatible_variant PASSED [  0%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_compatible PASSED [  0%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_compatible_variant PASSED [  0%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_compatible_variant_partial PASSED [  0%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_not_compatible PASSED [  1%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_not_compatible_variant PASSED [  1%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_compatible PASSED [  1%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_compatible_variant PASSED [  1%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_compatible_variant_partial PASSED [  1%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_not_compatible PASSED [  1%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_not_compatible_variant PASSED [  2%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_alt_diffusion_ddim PASSED [  2%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_alt_diffusion_pndm PASSED [  2%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_attention_slicing_forward_pass PASSED [  2%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_components_function PASSED [  2%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [  2%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_float16_inference PASSED [  3%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_inference_batch_consistent PASSED [  3%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_inference_batch_single_identical PASSED [  3%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_pipeline_call_signature PASSED [  3%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_progress_bar PASSED [  3%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_save_load_float16 PASSED [  3%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_save_load_local PASSED [  4%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_save_load_optional_components PASSED [  4%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [  4%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineIntegrationTests::test_alt_diffusion PASSED [  4%]
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineIntegrationTests::test_alt_diffusion_fast_ddim PASSED [  4%]
tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py::AltDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_default_case PASSED [  4%]
tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py::AltDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_fp16 PASSED [  5%]
tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py::AltDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_pipeline_multiple_of_8 PASSED [  5%]
tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py::AltDiffusionImg2ImgPipelineIntegrationTests::test_stable_diffusion_img2img_pipeline_default PASSED [  5%]
tests/pipelines/audio_diffusion/test_audio_diffusion.py::PipelineFastTests::test_audio_diffusion PASSED [  5%]
tests/pipelines/audio_diffusion/test_audio_diffusion.py::PipelineIntegrationTests::test_audio_diffusion PASSED [  5%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_attention_slicing_forward_pass PASSED [  5%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_components_function PASSED [  6%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_dance_diffusion PASSED [  6%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [  6%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_float16_inference PASSED [  6%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_inference_batch_consistent PASSED [  6%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_inference_batch_single_identical PASSED [  6%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_pipeline_call_signature PASSED [  7%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_progress_bar PASSED [  7%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_save_load_float16 PASSED [  7%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_save_load_local PASSED [  7%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_save_load_optional_components PASSED [  7%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::DanceDiffusionPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [  7%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::PipelineIntegrationTests::test_dance_diffusion PASSED [  8%]
tests/pipelines/dance_diffusion/test_dance_diffusion.py::PipelineIntegrationTests::test_dance_diffusion_fp16 PASSED [  8%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_attention_slicing_forward_pass PASSED [  8%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_components_function PASSED [  8%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [  8%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_float16_inference PASSED [  8%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_inference PASSED [  9%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_inference_batch_consistent PASSED [  9%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_inference_batch_single_identical PASSED [  9%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_pipeline_call_signature PASSED [  9%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_progress_bar PASSED [  9%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_save_load_float16 PASSED [ 10%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_save_load_local PASSED [ 10%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_save_load_optional_components PASSED [ 10%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 10%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineIntegrationTests::test_inference_cifar10 PASSED [ 10%]
tests/pipelines/ddim/test_ddim.py::DDIMPipelineIntegrationTests::test_inference_ema_bedroom PASSED [ 10%]
tests/pipelines/ddpm/test_ddpm.py::DDPMPipelineFastTests::test_fast_inference PASSED [ 11%]
tests/pipelines/ddpm/test_ddpm.py::DDPMPipelineFastTests::test_inference_predict_sample PASSED [ 11%]
tests/pipelines/ddpm/test_ddpm.py::DDPMPipelineIntegrationTests::test_inference_cifar10 PASSED [ 11%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 11%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_components_function PASSED [ 11%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 11%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_float16_inference PASSED [ 12%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_inference PASSED [ 12%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_inference_batch_consistent PASSED [ 12%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_inference_batch_single_identical PASSED [ 12%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_pipeline_call_signature PASSED [ 12%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_progress_bar PASSED [ 12%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_save_load_float16 PASSED [ 13%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_save_load_local PASSED [ 13%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_save_load_optional_components PASSED [ 13%]
tests/pipelines/dit/test_dit.py::DiTPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 13%]
tests/pipelines/dit/test_dit.py::DiTPipelineIntegrationTests::test_dit_256 PASSED [ 13%]
tests/pipelines/dit/test_dit.py::DiTPipelineIntegrationTests::test_dit_512_fp16 PASSED [ 13%]
tests/pipelines/karras_ve/test_karras_ve.py::KarrasVePipelineFastTests::test_inference FAILED [ 14%]
tests/pipelines/karras_ve/test_karras_ve.py::KarrasVePipelineIntegrationTests::test_inference PASSED [ 14%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_attention_slicing_forward_pass PASSED [ 14%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_components_function PASSED [ 14%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 14%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_float16_inference PASSED [ 14%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_inference_batch_consistent PASSED [ 15%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_inference_batch_single_identical PASSED [ 15%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_inference_text2img PASSED [ 15%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_pipeline_call_signature PASSED [ 15%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_progress_bar PASSED [ 15%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_save_load_float16 PASSED [ 15%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_save_load_local PASSED [ 16%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_save_load_optional_components PASSED [ 16%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 16%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineSlowTests::test_ldm_default_ddim PASSED [ 16%]
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineNightlyTests::test_ldm_default_ddim PASSED [ 16%]
tests/pipelines/latent_diffusion/test_latent_diffusion_superresolution.py::LDMSuperResolutionPipelineFastTests::test_inference_superresolution PASSED [ 16%]
tests/pipelines/latent_diffusion/test_latent_diffusion_superresolution.py::LDMSuperResolutionPipelineFastTests::test_inference_superresolution_fp16 PASSED [ 17%]
tests/pipelines/latent_diffusion/test_latent_diffusion_superresolution.py::LDMSuperResolutionPipelineIntegrationTests::test_inference_superresolution PASSED [ 17%]
tests/pipelines/latent_diffusion/test_latent_diffusion_uncond.py::LDMPipelineFastTests::test_inference_uncond PASSED [ 17%]
tests/pipelines/latent_diffusion/test_latent_diffusion_uncond.py::LDMPipelineIntegrationTests::test_inference_uncond PASSED [ 17%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_attention_slicing_forward_pass PASSED [ 17%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_components_function PASSED [ 17%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 18%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_float16_inference PASSED [ 18%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_inference_batch_consistent PASSED [ 18%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_inference_batch_single_identical PASSED [ 18%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_paint_by_example_image_tensor PASSED [ 18%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_paint_by_example_inpaint PASSED [ 18%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_paint_by_example_inpaint_with_num_images_per_prompt PASSED [ 19%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_pipeline_call_signature PASSED [ 19%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_progress_bar PASSED [ 19%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_save_load_float16 PASSED [ 19%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_save_load_local PASSED [ 19%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_save_load_optional_components PASSED [ 20%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 20%]
tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineIntegrationTests::test_paint_by_example PASSED [ 20%]
tests/pipelines/pndm/test_pndm.py::PNDMPipelineFastTests::test_inference PASSED [ 20%]
tests/pipelines/pndm/test_pndm.py::PNDMPipelineIntegrationTests::test_inference_cifar10 PASSED [ 20%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 20%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_components_function PASSED [ 21%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 21%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_float16_inference PASSED [ 21%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_inference_batch_consistent PASSED [ 21%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_inference_batch_single_identical SKIPPED [ 21%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_pipeline_call_signature PASSED [ 21%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_progress_bar PASSED [ 22%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_repaint PASSED [ 22%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_save_load_float16 PASSED [ 22%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_save_load_local PASSED [ 22%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_save_load_optional_components PASSED [ 22%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 22%]
tests/pipelines/repaint/test_repaint.py::RepaintPipelineNightlyTests::test_celebahq PASSED [ 23%]
tests/pipelines/score_sde_ve/test_score_sde_ve.py::ScoreSdeVeipelineFastTests::test_inference PASSED [ 23%]
tests/pipelines/score_sde_ve/test_score_sde_ve.py::ScoreSdeVePipelineIntegrationTests::test_inference PASSED [ 23%]
tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py::SafeDiffusionPipelineFastTests::test_semantic_diffusion_ddim FAILED [ 23%]
tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py::SafeDiffusionPipelineFastTests::test_semantic_diffusion_fp16 PASSED [ 23%]
tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py::SafeDiffusionPipelineFastTests::test_semantic_diffusion_no_safety_checker PASSED [ 23%]
tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py::SafeDiffusionPipelineFastTests::test_semantic_diffusion_pndm PASSED [ 24%]
tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py::SemanticDiffusionPipelineIntegrationTests::test_guidance_fp16 PASSED [ 24%]
tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py::SemanticDiffusionPipelineIntegrationTests::test_multi_cond_guidance PASSED [ 24%]
tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py::SemanticDiffusionPipelineIntegrationTests::test_negative_guidance PASSED [ 24%]
tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py::SemanticDiffusionPipelineIntegrationTests::test_positive_guidance PASSED [ 24%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 24%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_components_function PASSED [ 25%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 25%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_float16_inference PASSED [ 25%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_inference_batch_consistent PASSED [ 25%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_inference_batch_single_identical SKIPPED [ 25%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_pipeline_call_signature PASSED [ 25%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_progress_bar PASSED [ 26%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_save_load_float16 PASSED [ 26%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_save_load_local PASSED [ 26%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_save_load_optional_components PASSED [ 26%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_stable_diffusion_cycle PASSED [ 26%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_stable_diffusion_cycle_fp16 PASSED [ 26%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 27%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineIntegrationTests::test_cycle_diffusion_pipeline PASSED [ 27%]
tests/pipelines/stable_diffusion/test_cycle_diffusion.py::CycleDiffusionPipelineIntegrationTests::test_cycle_diffusion_pipeline_fp16 PASSED [ 27%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 27%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_components_function PASSED [ 27%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 27%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_float16_inference PASSED [ 28%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_inference_batch_consistent PASSED [ 28%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_inference_batch_single_identical PASSED [ 28%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_pipeline_call_signature PASSED [ 28%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_progress_bar PASSED [ 28%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_save_load_float16 PASSED [ 28%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_save_load_local PASSED [ 29%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_save_load_optional_components PASSED [ 29%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_ddim PASSED [ 29%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_ddim_factor_8 PASSED [ 29%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_height_width_opt PASSED [ 29%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_k_euler PASSED [ 30%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_k_euler_ancestral PASSED [ 30%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_k_lms PASSED [ 30%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_long_prompt PASSED [ 30%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_lora PASSED [ 30%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_negative_prompt PASSED [ 30%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_negative_prompt_embeds PASSED [ 31%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_no_safety_checker PASSED [ 31%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_num_images_per_prompt PASSED [ 31%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_pndm PASSED [ 31%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_prompt_embeds PASSED [ 31%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_vae_slicing PASSED [ 31%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_vae_tiling PASSED [ 32%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 32%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_1_1_pndm PASSED [ 32%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_1_4_pndm PASSED [ 32%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_attention_slicing FAILED [ 32%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_ddim PASSED [ 32%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_dpm PASSED [ 33%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_fp16_vs_autocast FAILED [ 33%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_intermediate_state PASSED [ 33%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_lms PASSED [ 33%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineNightlyTests::test_stable_diffusion_1_4_pndm PASSED [ 33%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineNightlyTests::test_stable_diffusion_1_5_pndm PASSED [ 33%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineNightlyTests::test_stable_diffusion_ddim PASSED [ 34%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineNightlyTests::test_stable_diffusion_dpm PASSED [ 34%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineNightlyTests::test_stable_diffusion_euler PASSED [ 34%]
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineNightlyTests::test_stable_diffusion_lms PASSED [ 34%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 34%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_components_function PASSED [ 34%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 35%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_float16_inference PASSED [ 35%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_inference_batch_consistent PASSED [ 35%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_inference_batch_single_identical PASSED [ 35%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_pipeline_call_signature PASSED [ 35%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_progress_bar PASSED [ 35%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_save_load_float16 PASSED [ 36%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_save_load_local PASSED [ 36%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_save_load_optional_components PASSED [ 36%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 36%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_canny FAILED [ 36%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_depth FAILED [ 36%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_hed FAILED [ 37%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_mlsd FAILED [ 37%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_normal FAILED [ 37%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_openpose FAILED [ 37%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_scribble FAILED [ 37%]
tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_seg FAILED [ 37%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 38%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_components_function PASSED [ 38%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 38%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_float16_inference PASSED [ 38%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_inference_batch_consistent PASSED [ 38%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_inference_batch_single_identical PASSED [ 38%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_pipeline_call_signature PASSED [ 39%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_progress_bar PASSED [ 39%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_save_load_float16 PASSED [ 39%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_save_load_local PASSED [ 39%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_save_load_optional_components PASSED [ 39%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_stable_diffusion_img_variation_default_case PASSED [ 40%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_stable_diffusion_img_variation_multiple_images PASSED [ 40%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_stable_diffusion_img_variation_num_images_per_prompt PASSED [ 40%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 40%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineSlowTests::test_stable_diffusion_img_variation_intermediate_state FAILED [ 40%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineSlowTests::test_stable_diffusion_img_variation_pipeline_default FAILED [ 40%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineNightlyTests::test_img_variation_dpm FAILED [ 41%]
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineNightlyTests::test_img_variation_pndm FAILED [ 41%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 41%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_components_function PASSED [ 41%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 41%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_float16_inference PASSED [ 41%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_inference_batch_consistent PASSED [ 42%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_inference_batch_single_identical PASSED [ 42%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_pipeline_call_signature PASSED [ 42%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_progress_bar PASSED [ 42%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_save_load_float16 PASSED [ 42%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_save_load_local PASSED [ 42%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_save_load_optional_components PASSED [ 43%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_default_case PASSED [ 43%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_k_lms PASSED [ 43%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_multiple_init_images PASSED [ 43%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_negative_prompt PASSED [ 43%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_num_images_per_prompt PASSED [ 43%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 44%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineSlowTests::test_stable_diffusion_img2img_ddim PASSED [ 44%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineSlowTests::test_stable_diffusion_img2img_default PASSED [ 44%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineSlowTests::test_stable_diffusion_img2img_intermediate_state PASSED [ 44%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineSlowTests::test_stable_diffusion_img2img_k_lms PASSED [ 44%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineSlowTests::test_stable_diffusion_img2img_pipeline_multiple_of_8 PASSED [ 44%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineNightlyTests::test_img2img_ddim PASSED [ 45%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineNightlyTests::test_img2img_dpm PASSED [ 45%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineNightlyTests::test_img2img_lms PASSED [ 45%]
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineNightlyTests::test_img2img_pndm FAILED [ 45%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 45%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_components_function PASSED [ 45%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 46%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_float16_inference PASSED [ 46%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_inference_batch_consistent PASSED [ 46%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_inference_batch_single_identical PASSED [ 46%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_pipeline_call_signature PASSED [ 46%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_progress_bar PASSED [ 46%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_save_load_float16 PASSED [ 47%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_save_load_local PASSED [ 47%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_save_load_optional_components PASSED [ 47%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_stable_diffusion_inpaint PASSED [ 47%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_stable_diffusion_inpaint_image_tensor PASSED [ 47%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_stable_diffusion_inpaint_with_num_images_per_prompt PASSED [ 47%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 48%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineSlowTests::test_stable_diffusion_inpaint_ddim PASSED [ 48%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineSlowTests::test_stable_diffusion_inpaint_fp16 FAILED [ 48%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineSlowTests::test_stable_diffusion_inpaint_k_lms PASSED [ 48%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineSlowTests::test_stable_diffusion_inpaint_pndm PASSED [ 48%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineNightlyTests::test_inpaint_ddim PASSED [ 48%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineNightlyTests::test_inpaint_dpm FAILED [ 49%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineNightlyTests::test_inpaint_lms PASSED [ 49%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineNightlyTests::test_inpaint_pndm PASSED [ 49%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_channels_first PASSED [ 49%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_np_inputs PASSED [ 49%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_paddle_3D_2D_inputs PASSED [ 50%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_paddle_3D_3D_inputs PASSED [ 50%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_paddle_4D_2D_inputs PASSED [ 50%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_paddle_4D_3D_inputs PASSED [ 50%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_paddle_4D_4D_inputs PASSED [ 50%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_paddle_batch_4D_3D PASSED [ 50%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_paddle_batch_4D_4D PASSED [ 51%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_pil_inputs PASSED [ 51%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_shape_mismatch PASSED [ 51%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_tensor_range PASSED [ 51%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintingPrepareMaskAndMaskedImageTests::test_type_mismatch PASSED [ 51%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineFastTests::test_stable_diffusion_inpaint_legacy PASSED [ 51%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineFastTests::test_stable_diffusion_inpaint_legacy_negative_prompt PASSED [ 52%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineFastTests::test_stable_diffusion_inpaint_legacy_num_images_per_prompt PASSED [ 52%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineSlowTests::test_stable_diffusion_inpaint_legacy_intermediate_state PASSED [ 52%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineSlowTests::test_stable_diffusion_inpaint_legacy_k_lms PASSED [ 52%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineSlowTests::test_stable_diffusion_inpaint_legacy_pndm PASSED [ 52%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineNightlyTests::test_inpaint_ddim PASSED [ 52%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineNightlyTests::test_inpaint_dpm PASSED [ 53%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineNightlyTests::test_inpaint_lms PASSED [ 53%]
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineNightlyTests::test_inpaint_pndm PASSED [ 53%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 53%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_components_function PASSED [ 53%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 53%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_float16_inference PASSED [ 54%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_inference_batch_consistent PASSED [ 54%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_inference_batch_single_identical PASSED [ 54%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_pipeline_call_signature PASSED [ 54%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_progress_bar PASSED [ 54%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_save_load_float16 PASSED [ 54%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_save_load_local PASSED [ 55%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_save_load_optional_components PASSED [ 55%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_stable_diffusion_pix2pix_default_case PASSED [ 55%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_stable_diffusion_pix2pix_euler PASSED [ 55%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_stable_diffusion_pix2pix_multiple_init_images PASSED [ 55%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_stable_diffusion_pix2pix_negative_prompt PASSED [ 55%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_stable_diffusion_pix2pix_num_images_per_prompt PASSED [ 56%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 56%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_ddim FAILED [ 56%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_default FAILED [ 56%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_intermediate_state FAILED [ 56%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_k_lms FAILED [ 56%]
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_pipeline_multiple_of_8 FAILED [ 57%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 57%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_components_function PASSED [ 57%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 57%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_float16_inference PASSED [ 57%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_inference_batch_consistent PASSED [ 57%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_inference_batch_single_identical PASSED [ 58%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_pipeline_call_signature PASSED [ 58%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_progress_bar PASSED [ 58%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_save_load_float16 PASSED [ 58%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_save_load_local PASSED [ 58%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_save_load_optional_components PASSED [ 58%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_stable_diffusion_panorama_default_case PASSED [ 59%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_stable_diffusion_panorama_euler PASSED [ 59%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_stable_diffusion_panorama_negative_prompt PASSED [ 59%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_stable_diffusion_panorama_num_images_per_prompt PASSED [ 59%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_stable_diffusion_panorama_pndm PASSED [ 59%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 60%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaSlowTests::test_stable_diffusion_panorama_default PASSED [ 60%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaSlowTests::test_stable_diffusion_panorama_intermediate_state PASSED [ 60%]
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py::StableDiffusionPanoramaSlowTests::test_stable_diffusion_panorama_k_lms PASSED [ 60%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 60%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_components_function PASSED [ 60%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 61%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_float16_inference PASSED [ 61%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_inference_batch_consistent PASSED [ 61%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_inference_batch_single_identical SKIPPED [ 61%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_pipeline_call_signature PASSED [ 61%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_progress_bar PASSED [ 61%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_save_load_float16 PASSED [ 62%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_save_load_local PASSED [ 62%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_save_load_optional_components PASSED [ 62%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_stable_diffusion_pix2pix_zero_ddpm PASSED [ 62%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_stable_diffusion_pix2pix_zero_default_case PASSED [ 62%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_stable_diffusion_pix2pix_zero_euler PASSED [ 62%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_stable_diffusion_pix2pix_zero_negative_prompt PASSED [ 63%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_stable_diffusion_pix2pix_zero_num_images_per_prompt PASSED [ 63%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 63%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineSlowTests::test_stable_diffusion_pix2pix_zero_default ERROR [ 63%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineSlowTests::test_stable_diffusion_pix2pix_zero_intermediate_state ERROR [ 63%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineSlowTests::test_stable_diffusion_pix2pix_zero_k_lms ERROR [ 63%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::InversionPipelineSlowTests::test_stable_diffusion_pix2pix_full PASSED [ 64%]
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::InversionPipelineSlowTests::test_stable_diffusion_pix2pix_inversion PASSED [ 64%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 64%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_components_function PASSED [ 64%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 64%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_float16_inference PASSED [ 64%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_inference_batch_consistent PASSED [ 65%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_inference_batch_single_identical PASSED [ 65%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_pipeline_call_signature PASSED [ 65%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_progress_bar PASSED [ 65%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_save_load_float16 PASSED [ 65%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_save_load_local PASSED [ 65%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_save_load_optional_components PASSED [ 66%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionSAGPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 66%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionPipelineIntegrationTests::test_stable_diffusion_1 PASSED [ 66%]
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py::StableDiffusionPipelineIntegrationTests::test_stable_diffusion_2 PASSED [ 66%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_attention_slicing_forward_pass PASSED [ 66%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_components_function PASSED [ 66%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 67%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_float16_inference PASSED [ 67%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_inference_batch_consistent PASSED [ 67%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_inference_batch_single_identical PASSED [ 67%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_pipeline_call_signature PASSED [ 67%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_progress_bar PASSED [ 67%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_save_load_float16 PASSED [ 68%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_save_load_local PASSED [ 68%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_save_load_optional_components PASSED [ 68%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_ddim FAILED [ 68%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_k_euler FAILED [ 68%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_k_euler_ancestral FAILED [ 68%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_k_lms PASSED [ 69%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_long_prompt PASSED [ 69%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_pndm PASSED [ 69%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 69%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineSlowTests::test_stable_diffusion_default_ddim PASSED [ 69%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineSlowTests::test_stable_diffusion_k_lms PASSED [ 70%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineSlowTests::test_stable_diffusion_pndm PASSED [ 70%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineSlowTests::test_stable_diffusion_text2img_intermediate_state PASSED [ 70%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineNightlyTests::test_stable_diffusion_2_0_default_ddim FAILED [ 70%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineNightlyTests::test_stable_diffusion_2_1_default_pndm PASSED [ 70%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineNightlyTests::test_stable_diffusion_ddim PASSED [ 70%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineNightlyTests::test_stable_diffusion_dpm PASSED [ 71%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineNightlyTests::test_stable_diffusion_euler PASSED [ 71%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineNightlyTests::test_stable_diffusion_lms PASSED [ 71%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_attention_slicing_forward_pass PASSED [ 71%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_components_function PASSED [ 71%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 71%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_float16_inference PASSED [ 72%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_inference PASSED [ 72%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_inference_batch_consistent PASSED [ 72%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_inference_batch_single_identical PASSED [ 72%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_pipeline_call_signature PASSED [ 72%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_progress_bar PASSED [ 72%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_save_load_float16 PASSED [ 73%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_save_load_local PASSED [ 73%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_save_load_optional_components PASSED [ 73%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 73%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineIntegrationTests::test_attend_and_excite_fp16 FAILED [ 73%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 73%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_components_function PASSED [ 74%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 74%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_float16_inference PASSED [ 74%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_inference_batch_consistent PASSED [ 74%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_inference_batch_single_identical PASSED [ 74%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_pipeline_call_signature PASSED [ 74%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_progress_bar PASSED [ 75%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_save_load_float16 PASSED [ 75%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_save_load_local PASSED [ 75%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_stable_diffusion_depth2img_default_case PASSED [ 75%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_stable_diffusion_depth2img_multiple_init_images FAILED [ 75%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_stable_diffusion_depth2img_negative_prompt PASSED [ 75%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_stable_diffusion_depth2img_num_images_per_prompt PASSED [ 76%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_stable_diffusion_depth2img_pil PASSED [ 76%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 76%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineSlowTests::test_stable_diffusion_depth2img_intermediate_state PASSED [ 76%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineSlowTests::test_stable_diffusion_depth2img_pipeline_ddim PASSED [ 76%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineSlowTests::test_stable_diffusion_depth2img_pipeline_default PASSED [ 76%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineSlowTests::test_stable_diffusion_depth2img_pipeline_k_lms PASSED [ 77%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionImg2ImgPipelineNightlyTests::test_depth2img_ddim FAILED [ 77%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionImg2ImgPipelineNightlyTests::test_depth2img_pndm FAILED [ 77%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionImg2ImgPipelineNightlyTests::test_img2img_dpm FAILED [ 77%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionImg2ImgPipelineNightlyTests::test_img2img_lms FAILED [ 77%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 77%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_components_function PASSED [ 78%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 78%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_float16_inference PASSED [ 78%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_inference_batch_consistent PASSED [ 78%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_inference_batch_single_identical PASSED [ 78%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_pipeline_call_signature PASSED [ 78%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_progress_bar PASSED [ 79%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_save_load_float16 PASSED [ 79%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_save_load_local PASSED [ 79%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_save_load_optional_components PASSED [ 79%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_stable_diffusion_inpaint PASSED [ 79%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusion2InpaintPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 80%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineIntegrationTests::test_stable_diffusion_inpaint_pipeline FAILED [ 80%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineIntegrationTests::test_stable_diffusion_inpaint_pipeline_fp16 PASSED [ 80%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_attention_slicing_forward_pass PASSED [ 80%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_components_function PASSED [ 80%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 80%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_float16_inference PASSED [ 81%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_inference FAILED [ 81%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_inference_batch_consistent PASSED [ 81%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_inference_batch_single_identical PASSED [ 81%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_pipeline_call_signature PASSED [ 81%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_progress_bar PASSED [ 81%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_save_load_float16 PASSED [ 82%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_save_load_local PASSED [ 82%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_save_load_optional_components PASSED [ 82%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 82%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineIntegrationTests::test_latent_upscaler_fp16 PASSED [ 82%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineIntegrationTests::test_latent_upscaler_fp16_image PASSED [ 82%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py::StableDiffusionUpscalePipelineFastTests::test_stable_diffusion_upscale FAILED [ 83%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py::StableDiffusionUpscalePipelineFastTests::test_stable_diffusion_upscale_batch PASSED [ 83%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py::StableDiffusionUpscalePipelineFastTests::test_stable_diffusion_upscale_fp16 PASSED [ 83%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py::StableDiffusionUpscalePipelineIntegrationTests::test_stable_diffusion_upscale_pipeline PASSED [ 83%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineFastTests::test_stable_diffusion_v_pred_ddim PASSED [ 83%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineFastTests::test_stable_diffusion_v_pred_fp16 PASSED [ 83%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineFastTests::test_stable_diffusion_v_pred_k_euler PASSED [ 84%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineIntegrationTests::test_stable_diffusion_attention_slicing_v_pred FAILED [ 84%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineIntegrationTests::test_stable_diffusion_text2img_intermediate_state_v_pred PASSED [ 84%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineIntegrationTests::test_stable_diffusion_text2img_pipeline_v_pred_default PASSED [ 84%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineIntegrationTests::test_stable_diffusion_text2img_pipeline_v_pred_fp16 PASSED [ 84%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineIntegrationTests::test_stable_diffusion_v_pred_default PASSED [ 84%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineIntegrationTests::test_stable_diffusion_v_pred_dpm PASSED [ 85%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineIntegrationTests::test_stable_diffusion_v_pred_euler PASSED [ 85%]
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineIntegrationTests::test_stable_diffusion_v_pred_upcast_attention PASSED [ 85%]
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineFastTests::test_safe_diffusion_ddim PASSED [ 85%]
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineFastTests::test_stable_diffusion_fp16 PASSED [ 85%]
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineFastTests::test_stable_diffusion_no_safety_checker PASSED [ 85%]
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineFastTests::test_stable_diffusion_pndm PASSED [ 86%]
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineIntegrationTests::test_harm_safe_stable_diffusion PASSED [ 86%]
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineIntegrationTests::test_nudity_safe_stable_diffusion PASSED [ 86%]
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineIntegrationTests::test_nudity_safetychecker_safe_stable_diffusion PASSED [ 86%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 86%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_components_function PASSED [ 86%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 87%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_float16_inference PASSED [ 87%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_inference_batch_consistent PASSED [ 87%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_inference_batch_single_identical PASSED [ 87%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_pipeline_call_signature PASSED [ 87%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_progress_bar PASSED [ 87%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_save_load_float16 PASSED [ 88%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_save_load_local PASSED [ 88%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_save_load_optional_components PASSED [ 88%]
tests/pipelines/stable_unclip/test_stable_unclip.py::StableUnCLIPPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 88%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 88%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_components_function PASSED [ 88%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 89%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_float16_inference PASSED [ 89%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_inference_batch_consistent PASSED [ 89%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_inference_batch_single_identical PASSED [ 89%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_pipeline_call_signature PASSED [ 89%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_progress_bar PASSED [ 90%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_save_load_float16 PASSED [ 90%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_save_load_local PASSED [ 90%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_save_load_optional_components PASSED [ 90%]
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py::StableUnCLIPImg2ImgPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_all_is_compatible PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_all_is_compatible_variant PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_compatible PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_compatible_variant PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_compatible_variant_partial PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_not_compatible PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_diffusers_model_is_not_compatible_variant PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_compatible PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_compatible_variant PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_compatible_variant_partial PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_not_compatible PASSED [ 90%]
tests/pipelines/test_pipeline_utils.py::IsSafetensorsCompatibleTests::test_transformer_model_is_not_compatible_variant PASSED [ 90%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 90%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_components_function PASSED [ 91%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 91%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_float16_inference PASSED [ 91%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_inference_batch_consistent PASSED [ 91%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_inference_batch_single_identical PASSED [ 91%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_pipeline_call_signature PASSED [ 91%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_progress_bar PASSED [ 92%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_save_load_float16 PASSED [ 92%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_save_load_local PASSED [ 92%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_save_load_optional_components PASSED [ 92%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_unclip PASSED [ 92%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_unclip_passed_text_embed PASSED [ 92%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 93%]
tests/pipelines/unclip/test_unclip.py::UnCLIPPipelineIntegrationTests::test_unclip_karlo PASSED [ 93%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_attention_slicing_forward_pass PASSED [ 93%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_components_function PASSED [ 93%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_dict_tuple_outputs_equivalent PASSED [ 93%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_float16_inference PASSED [ 93%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_inference_batch_consistent PASSED [ 94%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_inference_batch_single_identical PASSED [ 94%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_pipeline_call_signature PASSED [ 94%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_progress_bar PASSED [ 94%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_save_load_float16 PASSED [ 94%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_save_load_local PASSED [ 94%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_save_load_optional_components PASSED [ 95%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_unclip_image_variation_input_image PASSED [ 95%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_unclip_image_variation_input_list_images PASSED [ 95%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_unclip_image_variation_input_num_images_per_prompt PASSED [ 95%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_unclip_image_variation_input_tensor PASSED [ 95%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_unclip_passed_image_embed PASSED [ 95%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineFastTests::test_xformers_attention_forwardGenerator_pass PASSED [ 96%]
tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineIntegrationTests::test_unclip_image_variation_karlo FAILED [ 96%]
tests/pipelines/versatile_diffusion/test_versatile_diffusion_dual_guided.py::VersatileDiffusionDualGuidedPipelineIntegrationTests::test_inference_dual_guided PASSED [ 96%]
tests/pipelines/versatile_diffusion/test_versatile_diffusion_dual_guided.py::VersatileDiffusionDualGuidedPipelineIntegrationTests::test_remove_unused_weights_save_load PASSED [ 96%]
tests/pipelines/versatile_diffusion/test_versatile_diffusion_image_variation.py::VersatileDiffusionImageVariationPipelineIntegrationTests::test_inference_image_variations PASSED [ 96%]
tests/pipelines/versatile_diffusion/test_versatile_diffusion_mega.py::VersatileDiffusionMegaPipelineIntegrationTests::test_from_save_pretrained PASSED [ 96%]
tests/pipelines/versatile_diffusion/test_versatile_diffusion_mega.py::VersatileDiffusionMegaPipelineIntegrationTests::test_inference_dual_guided_then_text_to_image PASSED [ 97%]
tests/pipelines/versatile_diffusion/test_versatile_diffusion_text_to_image.py::VersatileDiffusionTextToImagePipelineIntegrationTests::test_inference_text2img PASSED [ 97%]
tests/pipelines/versatile_diffusion/test_versatile_diffusion_text_to_image.py::VersatileDiffusionTextToImagePipelineIntegrationTests::test_remove_unused_weights_save_load PASSED [ 97%]
tests/pipelines/vq_diffusion/test_vq_diffusion.py::VQDiffusionPipelineFastTests::test_vq_diffusion PASSED [ 97%]
tests/pipelines/vq_diffusion/test_vq_diffusion.py::VQDiffusionPipelineFastTests::test_vq_diffusion_classifier_free_sampling PASSED [ 97%]
tests/pipelines/vq_diffusion/test_vq_diffusion.py::VQDiffusionPipelineIntegrationTests::test_vq_diffusion_classifier_free_sampling PASSED [ 97%]

==================================== ERRORS ====================================
_ ERROR at setup of StableDiffusionPix2PixZeroPipelineSlowTests.test_stable_diffusion_pix2pix_zero_default _

cls = <class 'tests.pipelines.stable_diffusion.test_stable_diffusion_pix2pix_zero.StableDiffusionPix2PixZeroPipelineSlowTests'>

    @classmethod
    def setUpClass(cls):
        cls.source_embeds = to_paddle(load_pt(
>           "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat.pt"
        ))

tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],
         [-0.2808, -0.8671,  0.7025,  ..., -0.5...49,  0.2600, -0.7236],
         [-0.8951,  0.0275,  0.4287,  ..., -0.3856,  0.2722, -0.7647]]],
       device='cuda:0')

    def to_paddle(x):
        if hasattr(x, "numpy"):
>           x = x.numpy()
E           TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.

tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py:42: TypeError
_ ERROR at setup of StableDiffusionPix2PixZeroPipelineSlowTests.test_stable_diffusion_pix2pix_zero_intermediate_state _

cls = <class 'tests.pipelines.stable_diffusion.test_stable_diffusion_pix2pix_zero.StableDiffusionPix2PixZeroPipelineSlowTests'>

    @classmethod
    def setUpClass(cls):
        cls.source_embeds = to_paddle(load_pt(
>           "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat.pt"
        ))

tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],
         [-0.2808, -0.8671,  0.7025,  ..., -0.5...49,  0.2600, -0.7236],
         [-0.8951,  0.0275,  0.4287,  ..., -0.3856,  0.2722, -0.7647]]],
       device='cuda:0')

    def to_paddle(x):
        if hasattr(x, "numpy"):
>           x = x.numpy()
E           TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.

tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py:42: TypeError
_ ERROR at setup of StableDiffusionPix2PixZeroPipelineSlowTests.test_stable_diffusion_pix2pix_zero_k_lms _

cls = <class 'tests.pipelines.stable_diffusion.test_stable_diffusion_pix2pix_zero.StableDiffusionPix2PixZeroPipelineSlowTests'>

    @classmethod
    def setUpClass(cls):
        cls.source_embeds = to_paddle(load_pt(
>           "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/pix2pix/cat.pt"
        ))

tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py:185: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = tensor([[[-0.3884,  0.0229, -0.0522,  ..., -0.4899, -0.3066,  0.0675],
         [-0.2808, -0.8671,  0.7025,  ..., -0.5...49,  0.2600, -0.7236],
         [-0.8951,  0.0275,  0.4287,  ..., -0.3856,  0.2722, -0.7647]]],
       device='cuda:0')

    def to_paddle(x):
        if hasattr(x, "numpy"):
>           x = x.numpy()
E           TypeError: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.

tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py:42: TypeError
=================================== FAILURES ===================================
___________________ KarrasVePipelineFastTests.test_inference ___________________

self = <tests.pipelines.karras_ve.test_karras_ve.KarrasVePipelineFastTests testMethod=test_inference>

    def test_inference(self):
        unet = self.dummy_uncond_unet
        scheduler = KarrasVeScheduler()
        pipe = KarrasVePipeline(unet=unet, scheduler=scheduler)
        pipe.set_progress_bar_config(disable=None)
        generator = paddle.Generator().manual_seed(0)
        image = pipe(num_inference_steps=2, generator=generator, output_type="numpy").images
        generator = paddle.Generator().manual_seed(0)
        image_from_tuple = pipe(num_inference_steps=2, generator=generator, output_type="numpy", return_dict=False)[0]
        image_slice = image[0, -3:, -3:, -1]
        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]
        assert image.shape == (1, 32, 32, 3)
        expected_slice = np.array([0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0])
>       assert np.abs(image_slice.flatten() - expected_slice).max() < 0.01
E       AssertionError: assert 1.0 < 0.01
E        +  where 1.0 = <built-in method max of numpy.ndarray object at 0x7f6ea40803f0>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6ea40803f0> = array([0., 0., 1., 0., 1., 0., 1., 1., 1.]).max
E        +      where array([0., 0., 1., 0., 1., 0., 1., 1., 1.]) = <ufunc 'absolute'>((array([0., 1., 1., 0., 1., 1., 1., 1., 1.], dtype=float32) - array([0., 1., 0., 0., 0., 1., 0., 0., 0.])))
E        +        where <ufunc 'absolute'> = np.abs
E        +        and   array([0., 1., 1., 0., 1., 1., 1., 1., 1.], dtype=float32) = <built-in method flatten of numpy.ndarray object at 0x7f6ea4080810>()
E        +          where <built-in method flatten of numpy.ndarray object at 0x7f6ea4080810> = array([[0., 1., 1.],\n       [0., 1., 1.],\n       [1., 1., 1.]], dtype=float32).flatten

tests/pipelines/karras_ve/test_karras_ve.py:52: AssertionError
_________ SafeDiffusionPipelineFastTests.test_semantic_diffusion_ddim __________

self = <tests.pipelines.semantic_stable_diffusion.test_semantic_diffusion.SafeDiffusionPipelineFastTests testMethod=test_semantic_diffusion_ddim>

    def test_semantic_diffusion_ddim(self):
        unet = self.dummy_cond_unet
        scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012,
            beta_schedule='scaled_linear', clip_sample=False,
            set_alpha_to_one=False)
        vae = self.dummy_vae
        bert = self.dummy_text_encoder
        tokenizer = CLIPTokenizer.from_pretrained(
            'hf-internal-testing/tiny-random-clip')
        sd_pipe = StableDiffusionPipeline(unet=unet, scheduler=scheduler,
            vae=vae, text_encoder=bert, tokenizer=tokenizer, safety_checker
            =None, feature_extractor=self.dummy_extractor)
        sd_pipe.set_progress_bar_config(disable=None)
        prompt = 'A painting of a squirrel eating a burger'
        generator = paddle.Generator().manual_seed(0)
        output = sd_pipe([prompt], generator=generator, guidance_scale=6.0,
            num_inference_steps=2, output_type='np')
        image = output.images
        generator = paddle.Generator().manual_seed(0)
        image_from_tuple = sd_pipe([prompt], generator=generator,
            guidance_scale=6.0, num_inference_steps=2, output_type='np',
            return_dict=False)[0]
        image_slice = image[0, -3:, -3:, -1]
        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]
        assert image.shape == (1, 64, 64, 3)
        expected_slice = np.array([0.5644, 0.6018, 0.4799, 0.5267, 0.5585,
            0.4641, 0.516, 0.4964, 0.4792])
>       assert np.abs(image_slice.flatten() - expected_slice).max() < 0.01
E       AssertionError: assert 0.39799085788726807 < 0.01
E        +  where 0.39799085788726807 = <built-in method max of numpy.ndarray object at 0x7f6e73dd4c90>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6e73dd4c90> = array([0.27626838, 0.39799086, 0.18732419, 0.33177308, 0.31485844,\n       0.00766039, 0.38906565, 0.35424122, 0.12766724]).max
E        +      where array([0.27626838, 0.39799086, 0.18732419, 0.33177308, 0.31485844,\n       0.00766039, 0.38906565, 0.35424122, 0.12766724]) = <ufunc 'absolute'>((array([0.28813162, 0.20380914, 0.2925758 , 0.19492692, 0.24364156,\n       0.4564396 , 0.12693435, 0.14215878, 0.35153276], dtype=float32) - array([0.5644, 0.6018, 0.4799, 0.5267, 0.5585, 0.4641, 0.516 , 0.4964,\n       0.4792])))
E        +        where <ufunc 'absolute'> = np.abs
E        +        and   array([0.28813162, 0.20380914, 0.2925758 , 0.19492692, 0.24364156,\n       0.4564396 , 0.12693435, 0.14215878, 0.35153276], dtype=float32) = <built-in method flatten of numpy.ndarray object at 0x7f6e73daa210>()
E        +          where <built-in method flatten of numpy.ndarray object at 0x7f6e73daa210> = array([[0.28813162, 0.20380914, 0.2925758 ],\n       [0.19492692, 0.24364156, 0.4564396 ],\n       [0.12693435, 0.14215878, 0.35153276]], dtype=float32).flatten

tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py:126: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 13:58:24,988] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/vocab.json[0m
[32m[2023-03-15 13:58:24,988] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/merges.txt[0m
[32m[2023-03-15 13:58:24,988] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json and saved to /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip[0m
[33m[2023-03-15 13:58:25,025] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json> not exist[0m
[32m[2023-03-15 13:58:25,025] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/special_tokens_map.json[0m
[32m[2023-03-15 13:58:25,025] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/tokenizer_config.json[0m
You have disabled the safety checker for <class 'ppdiffusers.pipelines.semantic_stable_diffusion.pipeline_semantic_stable_diffusion.SemanticStableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. PaddleNLP team, diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
___ StableDiffusionPipelineSlowTests.test_stable_diffusion_attention_slicing ___

self = <tests.pipelines.stable_diffusion.test_stable_diffusion.StableDiffusionPipelineSlowTests testMethod=test_stable_diffusion_attention_slicing>

    def test_stable_diffusion_attention_slicing(self):
        pipe = StableDiffusionPipeline.from_pretrained(
            'CompVis/stable-diffusion-v1-4', paddle_dtype=paddle.float16)
        pipe.set_progress_bar_config(disable=None)
        pipe.enable_attention_slicing()
        inputs = self.get_inputs(dtype='float16')
        image_sliced = pipe(**inputs).images
        mem_bytes = paddle.device.cuda.memory_allocated()
        assert mem_bytes < 3.75 * 10 ** 9
        pipe.disable_attention_slicing()
        inputs = self.get_inputs(dtype='float16')
        image = pipe(**inputs).images
        mem_bytes = paddle.device.cuda.memory_allocated()
>       assert mem_bytes > 3.75 * 10 ** 9
E       assert 2740606720 > (3.75 * (10 ** 9))

tests/pipelines/stable_diffusion/test_stable_diffusion.py:481: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:03:04,421] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/text_encoder/config.json[0m
[32m[2023-03-15 14:03:04,422] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.21.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:03:07,346] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/safety_checker/config.json[0m
[32m[2023-03-15 14:03:07,348] [    INFO][0m - Model config CLIPVisionConfig {
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": null,
  "torchscript": false,
  "transformers_version": "4.21.0.dev0",
  "use_bfloat16": false
}
[0m
[32m[2023-03-15 14:03:20,574] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:03:20,575] [    INFO][0m - size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.[0m
[32m[2023-03-15 14:03:20,575] [    INFO][0m - crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.[0m
[32m[2023-03-15 14:03:20,575] [    INFO][0m - Image processor CLIPFeatureExtractor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPFeatureExtractor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
___ StableDiffusionPipelineSlowTests.test_stable_diffusion_fp16_vs_autocast ____

self = <tests.pipelines.stable_diffusion.test_stable_diffusion.StableDiffusionPipelineSlowTests testMethod=test_stable_diffusion_fp16_vs_autocast>

    def test_stable_diffusion_fp16_vs_autocast(self):
        pipe = StableDiffusionPipeline.from_pretrained(
            'CompVis/stable-diffusion-v1-4', paddle_dtype=paddle.float16)
        pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs(dtype='float16')
        image_fp16 = pipe(**inputs).images
        with paddle.amp.auto_cast(True, level="O2"):
            inputs = self.get_inputs()
            image_autocast = pipe(**inputs).images
        diff = np.abs(image_fp16.flatten() - image_autocast.flatten())
>       assert diff.mean() < 0.02
E       assert 0.048244953 < 0.02
E        +  where 0.048244953 = <built-in method mean of numpy.ndarray object at 0x7f6deb541b10>()
E        +    where <built-in method mean of numpy.ndarray object at 0x7f6deb541b10> = array([0.02392578, 0.02001953, 0.01806641, ..., 0.01855469, 0.01855469,\n       0.01269531], dtype=float32).mean

tests/pipelines/stable_diffusion/test_stable_diffusion.py:515: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:03:41,457] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/text_encoder/config.json[0m
[32m[2023-03-15 14:03:41,458] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.21.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:03:44,016] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/safety_checker/config.json[0m
[32m[2023-03-15 14:03:44,017] [    INFO][0m - Model config CLIPVisionConfig {
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": null,
  "torchscript": false,
  "transformers_version": "4.21.0.dev0",
  "use_bfloat16": false
}
[0m
[32m[2023-03-15 14:03:56,389] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:03:56,390] [    INFO][0m - size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.[0m
[32m[2023-03-15 14:03:56,390] [    INFO][0m - crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.[0m
[32m[2023-03-15 14:03:56,390] [    INFO][0m - Image processor CLIPFeatureExtractor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPFeatureExtractor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
____________ StableDiffusionControlNetPipelineSlowTests.test_canny _____________

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-canny'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
>                   revision=revision,
                )

ppdiffusers/utils/download_utils.py:417: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('lllyasviel/sd-controlnet-canny',)
kwargs = {'cache_dir': '/root/mttest/test_caches/diffusers', 'filename': 'config.json', 'force_download': False, 'local_files_only': True, ...}
has_token = False, arg_name = 'revision', arg_value = None

    @wraps(fn)
    def _inner_fn(*args, **kwargs):
        has_token = False
        for arg_name, arg_value in chain(
            zip(signature.parameters, args),  # Args values
            kwargs.items(),  # Kwargs values
        ):
            if arg_name == "repo_id":
                validate_repo_id(arg_value)
    
            elif arg_name == "token" and arg_value is not None:
                has_token = True
    
        if check_use_auth_token:
            kwargs = smoothly_deprecate_use_auth_token(
                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
            )
    
>       return fn(*args, **kwargs)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

repo_id = 'lllyasviel/sd-controlnet-canny', filename = 'config.json'

    @validate_hf_hub_args
    def hf_hub_download(
        repo_id: str,
        filename: str,
        *,
        subfolder: Optional[str] = None,
        repo_type: Optional[str] = None,
        revision: Optional[str] = None,
        library_name: Optional[str] = None,
        library_version: Optional[str] = None,
        cache_dir: Union[str, Path, None] = None,
        user_agent: Union[Dict, str, None] = None,
        force_download: bool = False,
        force_filename: Optional[str] = None,
        proxies: Optional[Dict] = None,
        etag_timeout: float = 10,
        resume_download: bool = False,
        token: Union[bool, str, None] = None,
        local_files_only: bool = False,
        legacy_cache_layout: bool = False,
    ):
        """Download a given file if it's not already present in the local cache.
    
        The new cache file layout looks like this:
        - The cache directory contains one subfolder per repo_id (namespaced by repo type)
        - inside each repo folder:
            - refs is a list of the latest known revision => commit_hash pairs
            - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on
              whether they're LFS files or not)
            - snapshots contains one subfolder per commit, each "commit" contains the subset of the files
              that have been resolved at that particular commit. Each filename is a symlink to the blob
              at that particular commit.
    
        ```
        [  96]  .
        └── [ 160]  models--julien-c--EsperBERTo-small
            ├── [ 160]  blobs
            │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
            │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e
            │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812
            ├── [  96]  refs
            │   └── [  40]  main
            └── [ 128]  snapshots
                ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f
                │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812
                │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
                └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48
                    ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e
                    └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
        ```
    
        Args:
            repo_id (`str`):
                A user or an organization name and a repo name separated by a `/`.
            filename (`str`):
                The name of the file in the repo.
            subfolder (`str`, *optional*):
                An optional value corresponding to a folder inside the model repo.
            repo_type (`str`, *optional*):
                Set to `"dataset"` or `"space"` if uploading to a dataset or space,
                `None` or `"model"` if uploading to a model. Default is `None`.
            revision (`str`, *optional*):
                An optional Git revision id which can be a branch name, a tag, or a
                commit hash.
            library_name (`str`, *optional*):
                The name of the library to which the object corresponds.
            library_version (`str`, *optional*):
                The version of the library.
            cache_dir (`str`, `Path`, *optional*):
                Path to the folder where cached files are stored.
            user_agent (`dict`, `str`, *optional*):
                The user-agent info in the form of a dictionary or a string.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether the file should be downloaded even if it already exists in
                the local cache.
            proxies (`dict`, *optional*):
                Dictionary mapping protocol to the URL of the proxy passed to
                `requests.request`.
            etag_timeout (`float`, *optional*, defaults to `10`):
                When fetching ETag, how many seconds to wait for the server to send
                data before giving up which is passed to `requests.request`.
            resume_download (`bool`, *optional*, defaults to `False`):
                If `True`, resume a previously interrupted download.
            token (`str`, `bool`, *optional*):
                A token to be used for the download.
                    - If `True`, the token is read from the HuggingFace config
                      folder.
                    - If a string, it's used as the authentication token.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, avoid downloading the file and return the path to the
                local cached file if it exists.
            legacy_cache_layout (`bool`, *optional*, defaults to `False`):
                If `True`, uses the legacy file cache layout i.e. just call [`hf_hub_url`]
                then `cached_download`. This is deprecated as the new cache layout is
                more powerful.
    
        Returns:
            Local path (string) of file or if networking is off, last version of
            file cached on disk.
    
        <Tip>
    
        Raises the following errors:
    
            - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)
              if `token=True` and the token cannot be found.
            - [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)
              if ETag cannot be determined.
            - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
              if some parameter value is invalid
            - [`~utils.RepositoryNotFoundError`]
              If the repository to download from cannot be found. This may be because it doesn't exist,
              or because it is set to `private` and you do not have access.
            - [`~utils.RevisionNotFoundError`]
              If the revision to download from cannot be found.
            - [`~utils.EntryNotFoundError`]
              If the file to download cannot be found.
            - [`~utils.LocalEntryNotFoundError`]
              If network is disabled or unavailable and file is not found in cache.
    
        </Tip>
        """
        if force_filename is not None:
            warnings.warn(
                "The `force_filename` parameter is deprecated as a new caching system, "
                "which keeps the filenames as they are on the Hub, is now in place.",
                FutureWarning,
            )
            legacy_cache_layout = True
    
        if legacy_cache_layout:
            url = hf_hub_url(
                repo_id,
                filename,
                subfolder=subfolder,
                repo_type=repo_type,
                revision=revision,
            )
    
            return cached_download(
                url,
                library_name=library_name,
                library_version=library_version,
                cache_dir=cache_dir,
                user_agent=user_agent,
                force_download=force_download,
                force_filename=force_filename,
                proxies=proxies,
                etag_timeout=etag_timeout,
                resume_download=resume_download,
                token=token,
                local_files_only=local_files_only,
                legacy_cache_layout=legacy_cache_layout,
            )
    
        if cache_dir is None:
            cache_dir = HUGGINGFACE_HUB_CACHE
        if revision is None:
            revision = DEFAULT_REVISION
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)
    
        if subfolder == "":
            subfolder = None
        if subfolder is not None:
            # This is used to create a URL, and not a local path, hence the forward slash.
            filename = f"{subfolder}/{filename}"
    
        if repo_type is None:
            repo_type = "model"
        if repo_type not in REPO_TYPES:
            raise ValueError(
                f"Invalid repo type: {repo_type}. Accepted repo types are:"
                f" {str(REPO_TYPES)}"
            )
    
        storage_folder = os.path.join(
            cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
        )
        os.makedirs(storage_folder, exist_ok=True)
    
        # cross platform transcription of filename, to be used as a local file path.
        relative_filename = os.path.join(*filename.split("/"))
    
        # if user provides a commit_hash and they already have the file on disk,
        # shortcut everything.
        if REGEX_COMMIT_HASH.match(revision):
            pointer_path = os.path.join(
                storage_folder, "snapshots", revision, relative_filename
            )
            if os.path.exists(pointer_path):
                return pointer_path
    
        url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
    
        headers = build_hf_headers(
            token=token,
            library_name=library_name,
            library_version=library_version,
            user_agent=user_agent,
        )
    
        url_to_download = url
        etag = None
        commit_hash = None
        if not local_files_only:
            try:
                try:
                    metadata = get_hf_file_metadata(
                        url=url,
                        token=token,
                        proxies=proxies,
                        timeout=etag_timeout,
                    )
                except EntryNotFoundError as http_error:
                    # Cache the non-existence of the file and raise
                    commit_hash = http_error.response.headers.get(
                        HUGGINGFACE_HEADER_X_REPO_COMMIT
                    )
                    if commit_hash is not None and not legacy_cache_layout:
                        no_exist_file_path = (
                            Path(storage_folder)
                            / ".no_exist"
                            / commit_hash
                            / relative_filename
                        )
                        no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                        no_exist_file_path.touch()
                        _cache_commit_hash_for_specific_revision(
                            storage_folder, revision, commit_hash
                        )
                    raise
    
                # Commit hash must exist
                commit_hash = metadata.commit_hash
                if commit_hash is None:
                    raise OSError(
                        "Distant resource does not seem to be on huggingface.co (missing"
                        " commit header)."
                    )
    
                # Etag must exist
                etag = metadata.etag
                # We favor a custom header indicating the etag of the linked resource, and
                # we fallback to the regular etag header.
                # If we don't have any of those, raise an error.
                if etag is None:
                    raise OSError(
                        "Distant resource does not have an ETag, we won't be able to"
                        " reliably ensure reproducibility."
                    )
    
                # In case of a redirect, save an extra redirect on the request.get call,
                # and ensure we download the exact atomic version even if it changed
                # between the HEAD and the GET (unlikely, but hey).
                # Useful for lfs blobs that are stored on a CDN.
                if metadata.location != url:
                    url_to_download = metadata.location
                    # Remove authorization header when downloading a LFS blob
                    headers.pop("authorization", None)
            except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
                # Actually raise for those subclasses of ConnectionError
                raise
            except (
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                OfflineModeIsEnabled,
            ):
                # Otherwise, our Internet connection is down.
                # etag is None
                pass
    
        # etag is None == we don't have a connection or we passed local_files_only.
        # try to get the last downloaded one from the specified revision.
        # If the specified revision is a commit hash, look inside "snapshots".
        # If the specified revision is a branch or tag, look inside "refs".
        if etag is None:
            # In those cases, we cannot force download.
            if force_download:
                raise ValueError(
                    "We have no connection or you passed local_files_only, so"
                    " force_download is not an accepted option."
                )
    
            # Try to get "commit_hash" from "revision"
            commit_hash = None
            if REGEX_COMMIT_HASH.match(revision):
                commit_hash = revision
            else:
                ref_path = os.path.join(storage_folder, "refs", revision)
                if os.path.isfile(ref_path):
                    with open(ref_path) as f:
                        commit_hash = f.read()
    
            # Return pointer file if exists
            if commit_hash is not None:
                pointer_path = os.path.join(
                    storage_folder, "snapshots", commit_hash, relative_filename
                )
                if os.path.exists(pointer_path):
                    return pointer_path
    
            # If we couldn't find an appropriate file on disk,
            # raise an error.
            # If files cannot be found and local_files_only=True,
            # the models might've been found if local_files_only=False
            # Notify the user about that
            if local_files_only:
                raise LocalEntryNotFoundError(
>                   "Cannot find the requested files in the disk cache and"
                    " outgoing traffic has been disabled. To enable hf.co look-ups"
                    " and downloads online, set 'local_files_only' to False."
                )
E               huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/file_download.py:1206: LocalEntryNotFoundError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_controlnet.StableDiffusionControlNetPipelineSlowTests testMethod=test_canny>

    def test_canny(self):
>       controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-canny")

tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py:151: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/models/modeling_utils.py:387: in from_pretrained
    **kwargs,
ppdiffusers/configuration_utils.py:343: in load_config
    from_hf_hub=from_hf_hub,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-canny'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
                    revision=revision,
                )
                return model_file
    
            except RepositoryNotFoundError:
                raise EnvironmentError(
                    f"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a "
                    "token having permission to this repo with `use_auth_token` or log in with `huggingface-cli "
                    "login`."
                )
            except RevisionNotFoundError:
                raise EnvironmentError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for "
                    "this model name. Check the model page at "
                    f"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions."
                )
            except EntryNotFoundError:
>               raise EnvironmentError(f"{pretrained_model_name_or_path} does not appear to have a file named {filename}.")
E               OSError: lllyasviel/sd-controlnet-canny does not appear to have a file named config.json.

ppdiffusers/utils/download_utils.py:435: OSError
____________ StableDiffusionControlNetPipelineSlowTests.test_depth _____________

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-depth'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
>                   revision=revision,
                )

ppdiffusers/utils/download_utils.py:417: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('lllyasviel/sd-controlnet-depth',)
kwargs = {'cache_dir': '/root/mttest/test_caches/diffusers', 'filename': 'config.json', 'force_download': False, 'local_files_only': True, ...}
has_token = False, arg_name = 'revision', arg_value = None

    @wraps(fn)
    def _inner_fn(*args, **kwargs):
        has_token = False
        for arg_name, arg_value in chain(
            zip(signature.parameters, args),  # Args values
            kwargs.items(),  # Kwargs values
        ):
            if arg_name == "repo_id":
                validate_repo_id(arg_value)
    
            elif arg_name == "token" and arg_value is not None:
                has_token = True
    
        if check_use_auth_token:
            kwargs = smoothly_deprecate_use_auth_token(
                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
            )
    
>       return fn(*args, **kwargs)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

repo_id = 'lllyasviel/sd-controlnet-depth', filename = 'config.json'

    @validate_hf_hub_args
    def hf_hub_download(
        repo_id: str,
        filename: str,
        *,
        subfolder: Optional[str] = None,
        repo_type: Optional[str] = None,
        revision: Optional[str] = None,
        library_name: Optional[str] = None,
        library_version: Optional[str] = None,
        cache_dir: Union[str, Path, None] = None,
        user_agent: Union[Dict, str, None] = None,
        force_download: bool = False,
        force_filename: Optional[str] = None,
        proxies: Optional[Dict] = None,
        etag_timeout: float = 10,
        resume_download: bool = False,
        token: Union[bool, str, None] = None,
        local_files_only: bool = False,
        legacy_cache_layout: bool = False,
    ):
        """Download a given file if it's not already present in the local cache.
    
        The new cache file layout looks like this:
        - The cache directory contains one subfolder per repo_id (namespaced by repo type)
        - inside each repo folder:
            - refs is a list of the latest known revision => commit_hash pairs
            - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on
              whether they're LFS files or not)
            - snapshots contains one subfolder per commit, each "commit" contains the subset of the files
              that have been resolved at that particular commit. Each filename is a symlink to the blob
              at that particular commit.
    
        ```
        [  96]  .
        └── [ 160]  models--julien-c--EsperBERTo-small
            ├── [ 160]  blobs
            │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
            │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e
            │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812
            ├── [  96]  refs
            │   └── [  40]  main
            └── [ 128]  snapshots
                ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f
                │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812
                │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
                └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48
                    ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e
                    └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
        ```
    
        Args:
            repo_id (`str`):
                A user or an organization name and a repo name separated by a `/`.
            filename (`str`):
                The name of the file in the repo.
            subfolder (`str`, *optional*):
                An optional value corresponding to a folder inside the model repo.
            repo_type (`str`, *optional*):
                Set to `"dataset"` or `"space"` if uploading to a dataset or space,
                `None` or `"model"` if uploading to a model. Default is `None`.
            revision (`str`, *optional*):
                An optional Git revision id which can be a branch name, a tag, or a
                commit hash.
            library_name (`str`, *optional*):
                The name of the library to which the object corresponds.
            library_version (`str`, *optional*):
                The version of the library.
            cache_dir (`str`, `Path`, *optional*):
                Path to the folder where cached files are stored.
            user_agent (`dict`, `str`, *optional*):
                The user-agent info in the form of a dictionary or a string.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether the file should be downloaded even if it already exists in
                the local cache.
            proxies (`dict`, *optional*):
                Dictionary mapping protocol to the URL of the proxy passed to
                `requests.request`.
            etag_timeout (`float`, *optional*, defaults to `10`):
                When fetching ETag, how many seconds to wait for the server to send
                data before giving up which is passed to `requests.request`.
            resume_download (`bool`, *optional*, defaults to `False`):
                If `True`, resume a previously interrupted download.
            token (`str`, `bool`, *optional*):
                A token to be used for the download.
                    - If `True`, the token is read from the HuggingFace config
                      folder.
                    - If a string, it's used as the authentication token.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, avoid downloading the file and return the path to the
                local cached file if it exists.
            legacy_cache_layout (`bool`, *optional*, defaults to `False`):
                If `True`, uses the legacy file cache layout i.e. just call [`hf_hub_url`]
                then `cached_download`. This is deprecated as the new cache layout is
                more powerful.
    
        Returns:
            Local path (string) of file or if networking is off, last version of
            file cached on disk.
    
        <Tip>
    
        Raises the following errors:
    
            - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)
              if `token=True` and the token cannot be found.
            - [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)
              if ETag cannot be determined.
            - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
              if some parameter value is invalid
            - [`~utils.RepositoryNotFoundError`]
              If the repository to download from cannot be found. This may be because it doesn't exist,
              or because it is set to `private` and you do not have access.
            - [`~utils.RevisionNotFoundError`]
              If the revision to download from cannot be found.
            - [`~utils.EntryNotFoundError`]
              If the file to download cannot be found.
            - [`~utils.LocalEntryNotFoundError`]
              If network is disabled or unavailable and file is not found in cache.
    
        </Tip>
        """
        if force_filename is not None:
            warnings.warn(
                "The `force_filename` parameter is deprecated as a new caching system, "
                "which keeps the filenames as they are on the Hub, is now in place.",
                FutureWarning,
            )
            legacy_cache_layout = True
    
        if legacy_cache_layout:
            url = hf_hub_url(
                repo_id,
                filename,
                subfolder=subfolder,
                repo_type=repo_type,
                revision=revision,
            )
    
            return cached_download(
                url,
                library_name=library_name,
                library_version=library_version,
                cache_dir=cache_dir,
                user_agent=user_agent,
                force_download=force_download,
                force_filename=force_filename,
                proxies=proxies,
                etag_timeout=etag_timeout,
                resume_download=resume_download,
                token=token,
                local_files_only=local_files_only,
                legacy_cache_layout=legacy_cache_layout,
            )
    
        if cache_dir is None:
            cache_dir = HUGGINGFACE_HUB_CACHE
        if revision is None:
            revision = DEFAULT_REVISION
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)
    
        if subfolder == "":
            subfolder = None
        if subfolder is not None:
            # This is used to create a URL, and not a local path, hence the forward slash.
            filename = f"{subfolder}/{filename}"
    
        if repo_type is None:
            repo_type = "model"
        if repo_type not in REPO_TYPES:
            raise ValueError(
                f"Invalid repo type: {repo_type}. Accepted repo types are:"
                f" {str(REPO_TYPES)}"
            )
    
        storage_folder = os.path.join(
            cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
        )
        os.makedirs(storage_folder, exist_ok=True)
    
        # cross platform transcription of filename, to be used as a local file path.
        relative_filename = os.path.join(*filename.split("/"))
    
        # if user provides a commit_hash and they already have the file on disk,
        # shortcut everything.
        if REGEX_COMMIT_HASH.match(revision):
            pointer_path = os.path.join(
                storage_folder, "snapshots", revision, relative_filename
            )
            if os.path.exists(pointer_path):
                return pointer_path
    
        url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
    
        headers = build_hf_headers(
            token=token,
            library_name=library_name,
            library_version=library_version,
            user_agent=user_agent,
        )
    
        url_to_download = url
        etag = None
        commit_hash = None
        if not local_files_only:
            try:
                try:
                    metadata = get_hf_file_metadata(
                        url=url,
                        token=token,
                        proxies=proxies,
                        timeout=etag_timeout,
                    )
                except EntryNotFoundError as http_error:
                    # Cache the non-existence of the file and raise
                    commit_hash = http_error.response.headers.get(
                        HUGGINGFACE_HEADER_X_REPO_COMMIT
                    )
                    if commit_hash is not None and not legacy_cache_layout:
                        no_exist_file_path = (
                            Path(storage_folder)
                            / ".no_exist"
                            / commit_hash
                            / relative_filename
                        )
                        no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                        no_exist_file_path.touch()
                        _cache_commit_hash_for_specific_revision(
                            storage_folder, revision, commit_hash
                        )
                    raise
    
                # Commit hash must exist
                commit_hash = metadata.commit_hash
                if commit_hash is None:
                    raise OSError(
                        "Distant resource does not seem to be on huggingface.co (missing"
                        " commit header)."
                    )
    
                # Etag must exist
                etag = metadata.etag
                # We favor a custom header indicating the etag of the linked resource, and
                # we fallback to the regular etag header.
                # If we don't have any of those, raise an error.
                if etag is None:
                    raise OSError(
                        "Distant resource does not have an ETag, we won't be able to"
                        " reliably ensure reproducibility."
                    )
    
                # In case of a redirect, save an extra redirect on the request.get call,
                # and ensure we download the exact atomic version even if it changed
                # between the HEAD and the GET (unlikely, but hey).
                # Useful for lfs blobs that are stored on a CDN.
                if metadata.location != url:
                    url_to_download = metadata.location
                    # Remove authorization header when downloading a LFS blob
                    headers.pop("authorization", None)
            except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
                # Actually raise for those subclasses of ConnectionError
                raise
            except (
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                OfflineModeIsEnabled,
            ):
                # Otherwise, our Internet connection is down.
                # etag is None
                pass
    
        # etag is None == we don't have a connection or we passed local_files_only.
        # try to get the last downloaded one from the specified revision.
        # If the specified revision is a commit hash, look inside "snapshots".
        # If the specified revision is a branch or tag, look inside "refs".
        if etag is None:
            # In those cases, we cannot force download.
            if force_download:
                raise ValueError(
                    "We have no connection or you passed local_files_only, so"
                    " force_download is not an accepted option."
                )
    
            # Try to get "commit_hash" from "revision"
            commit_hash = None
            if REGEX_COMMIT_HASH.match(revision):
                commit_hash = revision
            else:
                ref_path = os.path.join(storage_folder, "refs", revision)
                if os.path.isfile(ref_path):
                    with open(ref_path) as f:
                        commit_hash = f.read()
    
            # Return pointer file if exists
            if commit_hash is not None:
                pointer_path = os.path.join(
                    storage_folder, "snapshots", commit_hash, relative_filename
                )
                if os.path.exists(pointer_path):
                    return pointer_path
    
            # If we couldn't find an appropriate file on disk,
            # raise an error.
            # If files cannot be found and local_files_only=True,
            # the models might've been found if local_files_only=False
            # Notify the user about that
            if local_files_only:
                raise LocalEntryNotFoundError(
>                   "Cannot find the requested files in the disk cache and"
                    " outgoing traffic has been disabled. To enable hf.co look-ups"
                    " and downloads online, set 'local_files_only' to False."
                )
E               huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/file_download.py:1206: LocalEntryNotFoundError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_controlnet.StableDiffusionControlNetPipelineSlowTests testMethod=test_depth>

    def test_depth(self):
>       controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-depth")

tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py:177: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/models/modeling_utils.py:387: in from_pretrained
    **kwargs,
ppdiffusers/configuration_utils.py:343: in load_config
    from_hf_hub=from_hf_hub,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-depth'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
                    revision=revision,
                )
                return model_file
    
            except RepositoryNotFoundError:
                raise EnvironmentError(
                    f"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a "
                    "token having permission to this repo with `use_auth_token` or log in with `huggingface-cli "
                    "login`."
                )
            except RevisionNotFoundError:
                raise EnvironmentError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for "
                    "this model name. Check the model page at "
                    f"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions."
                )
            except EntryNotFoundError:
>               raise EnvironmentError(f"{pretrained_model_name_or_path} does not appear to have a file named {filename}.")
E               OSError: lllyasviel/sd-controlnet-depth does not appear to have a file named config.json.

ppdiffusers/utils/download_utils.py:435: OSError
_____________ StableDiffusionControlNetPipelineSlowTests.test_hed ______________

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-hed'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
>                   revision=revision,
                )

ppdiffusers/utils/download_utils.py:417: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('lllyasviel/sd-controlnet-hed',)
kwargs = {'cache_dir': '/root/mttest/test_caches/diffusers', 'filename': 'config.json', 'force_download': False, 'local_files_only': True, ...}
has_token = False, arg_name = 'revision', arg_value = None

    @wraps(fn)
    def _inner_fn(*args, **kwargs):
        has_token = False
        for arg_name, arg_value in chain(
            zip(signature.parameters, args),  # Args values
            kwargs.items(),  # Kwargs values
        ):
            if arg_name == "repo_id":
                validate_repo_id(arg_value)
    
            elif arg_name == "token" and arg_value is not None:
                has_token = True
    
        if check_use_auth_token:
            kwargs = smoothly_deprecate_use_auth_token(
                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
            )
    
>       return fn(*args, **kwargs)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

repo_id = 'lllyasviel/sd-controlnet-hed', filename = 'config.json'

    @validate_hf_hub_args
    def hf_hub_download(
        repo_id: str,
        filename: str,
        *,
        subfolder: Optional[str] = None,
        repo_type: Optional[str] = None,
        revision: Optional[str] = None,
        library_name: Optional[str] = None,
        library_version: Optional[str] = None,
        cache_dir: Union[str, Path, None] = None,
        user_agent: Union[Dict, str, None] = None,
        force_download: bool = False,
        force_filename: Optional[str] = None,
        proxies: Optional[Dict] = None,
        etag_timeout: float = 10,
        resume_download: bool = False,
        token: Union[bool, str, None] = None,
        local_files_only: bool = False,
        legacy_cache_layout: bool = False,
    ):
        """Download a given file if it's not already present in the local cache.
    
        The new cache file layout looks like this:
        - The cache directory contains one subfolder per repo_id (namespaced by repo type)
        - inside each repo folder:
            - refs is a list of the latest known revision => commit_hash pairs
            - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on
              whether they're LFS files or not)
            - snapshots contains one subfolder per commit, each "commit" contains the subset of the files
              that have been resolved at that particular commit. Each filename is a symlink to the blob
              at that particular commit.
    
        ```
        [  96]  .
        └── [ 160]  models--julien-c--EsperBERTo-small
            ├── [ 160]  blobs
            │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
            │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e
            │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812
            ├── [  96]  refs
            │   └── [  40]  main
            └── [ 128]  snapshots
                ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f
                │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812
                │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
                └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48
                    ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e
                    └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
        ```
    
        Args:
            repo_id (`str`):
                A user or an organization name and a repo name separated by a `/`.
            filename (`str`):
                The name of the file in the repo.
            subfolder (`str`, *optional*):
                An optional value corresponding to a folder inside the model repo.
            repo_type (`str`, *optional*):
                Set to `"dataset"` or `"space"` if uploading to a dataset or space,
                `None` or `"model"` if uploading to a model. Default is `None`.
            revision (`str`, *optional*):
                An optional Git revision id which can be a branch name, a tag, or a
                commit hash.
            library_name (`str`, *optional*):
                The name of the library to which the object corresponds.
            library_version (`str`, *optional*):
                The version of the library.
            cache_dir (`str`, `Path`, *optional*):
                Path to the folder where cached files are stored.
            user_agent (`dict`, `str`, *optional*):
                The user-agent info in the form of a dictionary or a string.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether the file should be downloaded even if it already exists in
                the local cache.
            proxies (`dict`, *optional*):
                Dictionary mapping protocol to the URL of the proxy passed to
                `requests.request`.
            etag_timeout (`float`, *optional*, defaults to `10`):
                When fetching ETag, how many seconds to wait for the server to send
                data before giving up which is passed to `requests.request`.
            resume_download (`bool`, *optional*, defaults to `False`):
                If `True`, resume a previously interrupted download.
            token (`str`, `bool`, *optional*):
                A token to be used for the download.
                    - If `True`, the token is read from the HuggingFace config
                      folder.
                    - If a string, it's used as the authentication token.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, avoid downloading the file and return the path to the
                local cached file if it exists.
            legacy_cache_layout (`bool`, *optional*, defaults to `False`):
                If `True`, uses the legacy file cache layout i.e. just call [`hf_hub_url`]
                then `cached_download`. This is deprecated as the new cache layout is
                more powerful.
    
        Returns:
            Local path (string) of file or if networking is off, last version of
            file cached on disk.
    
        <Tip>
    
        Raises the following errors:
    
            - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)
              if `token=True` and the token cannot be found.
            - [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)
              if ETag cannot be determined.
            - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
              if some parameter value is invalid
            - [`~utils.RepositoryNotFoundError`]
              If the repository to download from cannot be found. This may be because it doesn't exist,
              or because it is set to `private` and you do not have access.
            - [`~utils.RevisionNotFoundError`]
              If the revision to download from cannot be found.
            - [`~utils.EntryNotFoundError`]
              If the file to download cannot be found.
            - [`~utils.LocalEntryNotFoundError`]
              If network is disabled or unavailable and file is not found in cache.
    
        </Tip>
        """
        if force_filename is not None:
            warnings.warn(
                "The `force_filename` parameter is deprecated as a new caching system, "
                "which keeps the filenames as they are on the Hub, is now in place.",
                FutureWarning,
            )
            legacy_cache_layout = True
    
        if legacy_cache_layout:
            url = hf_hub_url(
                repo_id,
                filename,
                subfolder=subfolder,
                repo_type=repo_type,
                revision=revision,
            )
    
            return cached_download(
                url,
                library_name=library_name,
                library_version=library_version,
                cache_dir=cache_dir,
                user_agent=user_agent,
                force_download=force_download,
                force_filename=force_filename,
                proxies=proxies,
                etag_timeout=etag_timeout,
                resume_download=resume_download,
                token=token,
                local_files_only=local_files_only,
                legacy_cache_layout=legacy_cache_layout,
            )
    
        if cache_dir is None:
            cache_dir = HUGGINGFACE_HUB_CACHE
        if revision is None:
            revision = DEFAULT_REVISION
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)
    
        if subfolder == "":
            subfolder = None
        if subfolder is not None:
            # This is used to create a URL, and not a local path, hence the forward slash.
            filename = f"{subfolder}/{filename}"
    
        if repo_type is None:
            repo_type = "model"
        if repo_type not in REPO_TYPES:
            raise ValueError(
                f"Invalid repo type: {repo_type}. Accepted repo types are:"
                f" {str(REPO_TYPES)}"
            )
    
        storage_folder = os.path.join(
            cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
        )
        os.makedirs(storage_folder, exist_ok=True)
    
        # cross platform transcription of filename, to be used as a local file path.
        relative_filename = os.path.join(*filename.split("/"))
    
        # if user provides a commit_hash and they already have the file on disk,
        # shortcut everything.
        if REGEX_COMMIT_HASH.match(revision):
            pointer_path = os.path.join(
                storage_folder, "snapshots", revision, relative_filename
            )
            if os.path.exists(pointer_path):
                return pointer_path
    
        url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
    
        headers = build_hf_headers(
            token=token,
            library_name=library_name,
            library_version=library_version,
            user_agent=user_agent,
        )
    
        url_to_download = url
        etag = None
        commit_hash = None
        if not local_files_only:
            try:
                try:
                    metadata = get_hf_file_metadata(
                        url=url,
                        token=token,
                        proxies=proxies,
                        timeout=etag_timeout,
                    )
                except EntryNotFoundError as http_error:
                    # Cache the non-existence of the file and raise
                    commit_hash = http_error.response.headers.get(
                        HUGGINGFACE_HEADER_X_REPO_COMMIT
                    )
                    if commit_hash is not None and not legacy_cache_layout:
                        no_exist_file_path = (
                            Path(storage_folder)
                            / ".no_exist"
                            / commit_hash
                            / relative_filename
                        )
                        no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                        no_exist_file_path.touch()
                        _cache_commit_hash_for_specific_revision(
                            storage_folder, revision, commit_hash
                        )
                    raise
    
                # Commit hash must exist
                commit_hash = metadata.commit_hash
                if commit_hash is None:
                    raise OSError(
                        "Distant resource does not seem to be on huggingface.co (missing"
                        " commit header)."
                    )
    
                # Etag must exist
                etag = metadata.etag
                # We favor a custom header indicating the etag of the linked resource, and
                # we fallback to the regular etag header.
                # If we don't have any of those, raise an error.
                if etag is None:
                    raise OSError(
                        "Distant resource does not have an ETag, we won't be able to"
                        " reliably ensure reproducibility."
                    )
    
                # In case of a redirect, save an extra redirect on the request.get call,
                # and ensure we download the exact atomic version even if it changed
                # between the HEAD and the GET (unlikely, but hey).
                # Useful for lfs blobs that are stored on a CDN.
                if metadata.location != url:
                    url_to_download = metadata.location
                    # Remove authorization header when downloading a LFS blob
                    headers.pop("authorization", None)
            except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
                # Actually raise for those subclasses of ConnectionError
                raise
            except (
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                OfflineModeIsEnabled,
            ):
                # Otherwise, our Internet connection is down.
                # etag is None
                pass
    
        # etag is None == we don't have a connection or we passed local_files_only.
        # try to get the last downloaded one from the specified revision.
        # If the specified revision is a commit hash, look inside "snapshots".
        # If the specified revision is a branch or tag, look inside "refs".
        if etag is None:
            # In those cases, we cannot force download.
            if force_download:
                raise ValueError(
                    "We have no connection or you passed local_files_only, so"
                    " force_download is not an accepted option."
                )
    
            # Try to get "commit_hash" from "revision"
            commit_hash = None
            if REGEX_COMMIT_HASH.match(revision):
                commit_hash = revision
            else:
                ref_path = os.path.join(storage_folder, "refs", revision)
                if os.path.isfile(ref_path):
                    with open(ref_path) as f:
                        commit_hash = f.read()
    
            # Return pointer file if exists
            if commit_hash is not None:
                pointer_path = os.path.join(
                    storage_folder, "snapshots", commit_hash, relative_filename
                )
                if os.path.exists(pointer_path):
                    return pointer_path
    
            # If we couldn't find an appropriate file on disk,
            # raise an error.
            # If files cannot be found and local_files_only=True,
            # the models might've been found if local_files_only=False
            # Notify the user about that
            if local_files_only:
                raise LocalEntryNotFoundError(
>                   "Cannot find the requested files in the disk cache and"
                    " outgoing traffic has been disabled. To enable hf.co look-ups"
                    " and downloads online, set 'local_files_only' to False."
                )
E               huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/file_download.py:1206: LocalEntryNotFoundError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_controlnet.StableDiffusionControlNetPipelineSlowTests testMethod=test_hed>

    def test_hed(self):
>       controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-hed")

tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py:203: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/models/modeling_utils.py:387: in from_pretrained
    **kwargs,
ppdiffusers/configuration_utils.py:343: in load_config
    from_hf_hub=from_hf_hub,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-hed'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
                    revision=revision,
                )
                return model_file
    
            except RepositoryNotFoundError:
                raise EnvironmentError(
                    f"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a "
                    "token having permission to this repo with `use_auth_token` or log in with `huggingface-cli "
                    "login`."
                )
            except RevisionNotFoundError:
                raise EnvironmentError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for "
                    "this model name. Check the model page at "
                    f"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions."
                )
            except EntryNotFoundError:
>               raise EnvironmentError(f"{pretrained_model_name_or_path} does not appear to have a file named {filename}.")
E               OSError: lllyasviel/sd-controlnet-hed does not appear to have a file named config.json.

ppdiffusers/utils/download_utils.py:435: OSError
_____________ StableDiffusionControlNetPipelineSlowTests.test_mlsd _____________

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-mlsd'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
>                   revision=revision,
                )

ppdiffusers/utils/download_utils.py:417: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('lllyasviel/sd-controlnet-mlsd',)
kwargs = {'cache_dir': '/root/mttest/test_caches/diffusers', 'filename': 'config.json', 'force_download': False, 'local_files_only': True, ...}
has_token = False, arg_name = 'revision', arg_value = None

    @wraps(fn)
    def _inner_fn(*args, **kwargs):
        has_token = False
        for arg_name, arg_value in chain(
            zip(signature.parameters, args),  # Args values
            kwargs.items(),  # Kwargs values
        ):
            if arg_name == "repo_id":
                validate_repo_id(arg_value)
    
            elif arg_name == "token" and arg_value is not None:
                has_token = True
    
        if check_use_auth_token:
            kwargs = smoothly_deprecate_use_auth_token(
                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
            )
    
>       return fn(*args, **kwargs)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

repo_id = 'lllyasviel/sd-controlnet-mlsd', filename = 'config.json'

    @validate_hf_hub_args
    def hf_hub_download(
        repo_id: str,
        filename: str,
        *,
        subfolder: Optional[str] = None,
        repo_type: Optional[str] = None,
        revision: Optional[str] = None,
        library_name: Optional[str] = None,
        library_version: Optional[str] = None,
        cache_dir: Union[str, Path, None] = None,
        user_agent: Union[Dict, str, None] = None,
        force_download: bool = False,
        force_filename: Optional[str] = None,
        proxies: Optional[Dict] = None,
        etag_timeout: float = 10,
        resume_download: bool = False,
        token: Union[bool, str, None] = None,
        local_files_only: bool = False,
        legacy_cache_layout: bool = False,
    ):
        """Download a given file if it's not already present in the local cache.
    
        The new cache file layout looks like this:
        - The cache directory contains one subfolder per repo_id (namespaced by repo type)
        - inside each repo folder:
            - refs is a list of the latest known revision => commit_hash pairs
            - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on
              whether they're LFS files or not)
            - snapshots contains one subfolder per commit, each "commit" contains the subset of the files
              that have been resolved at that particular commit. Each filename is a symlink to the blob
              at that particular commit.
    
        ```
        [  96]  .
        └── [ 160]  models--julien-c--EsperBERTo-small
            ├── [ 160]  blobs
            │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
            │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e
            │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812
            ├── [  96]  refs
            │   └── [  40]  main
            └── [ 128]  snapshots
                ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f
                │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812
                │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
                └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48
                    ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e
                    └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
        ```
    
        Args:
            repo_id (`str`):
                A user or an organization name and a repo name separated by a `/`.
            filename (`str`):
                The name of the file in the repo.
            subfolder (`str`, *optional*):
                An optional value corresponding to a folder inside the model repo.
            repo_type (`str`, *optional*):
                Set to `"dataset"` or `"space"` if uploading to a dataset or space,
                `None` or `"model"` if uploading to a model. Default is `None`.
            revision (`str`, *optional*):
                An optional Git revision id which can be a branch name, a tag, or a
                commit hash.
            library_name (`str`, *optional*):
                The name of the library to which the object corresponds.
            library_version (`str`, *optional*):
                The version of the library.
            cache_dir (`str`, `Path`, *optional*):
                Path to the folder where cached files are stored.
            user_agent (`dict`, `str`, *optional*):
                The user-agent info in the form of a dictionary or a string.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether the file should be downloaded even if it already exists in
                the local cache.
            proxies (`dict`, *optional*):
                Dictionary mapping protocol to the URL of the proxy passed to
                `requests.request`.
            etag_timeout (`float`, *optional*, defaults to `10`):
                When fetching ETag, how many seconds to wait for the server to send
                data before giving up which is passed to `requests.request`.
            resume_download (`bool`, *optional*, defaults to `False`):
                If `True`, resume a previously interrupted download.
            token (`str`, `bool`, *optional*):
                A token to be used for the download.
                    - If `True`, the token is read from the HuggingFace config
                      folder.
                    - If a string, it's used as the authentication token.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, avoid downloading the file and return the path to the
                local cached file if it exists.
            legacy_cache_layout (`bool`, *optional*, defaults to `False`):
                If `True`, uses the legacy file cache layout i.e. just call [`hf_hub_url`]
                then `cached_download`. This is deprecated as the new cache layout is
                more powerful.
    
        Returns:
            Local path (string) of file or if networking is off, last version of
            file cached on disk.
    
        <Tip>
    
        Raises the following errors:
    
            - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)
              if `token=True` and the token cannot be found.
            - [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)
              if ETag cannot be determined.
            - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
              if some parameter value is invalid
            - [`~utils.RepositoryNotFoundError`]
              If the repository to download from cannot be found. This may be because it doesn't exist,
              or because it is set to `private` and you do not have access.
            - [`~utils.RevisionNotFoundError`]
              If the revision to download from cannot be found.
            - [`~utils.EntryNotFoundError`]
              If the file to download cannot be found.
            - [`~utils.LocalEntryNotFoundError`]
              If network is disabled or unavailable and file is not found in cache.
    
        </Tip>
        """
        if force_filename is not None:
            warnings.warn(
                "The `force_filename` parameter is deprecated as a new caching system, "
                "which keeps the filenames as they are on the Hub, is now in place.",
                FutureWarning,
            )
            legacy_cache_layout = True
    
        if legacy_cache_layout:
            url = hf_hub_url(
                repo_id,
                filename,
                subfolder=subfolder,
                repo_type=repo_type,
                revision=revision,
            )
    
            return cached_download(
                url,
                library_name=library_name,
                library_version=library_version,
                cache_dir=cache_dir,
                user_agent=user_agent,
                force_download=force_download,
                force_filename=force_filename,
                proxies=proxies,
                etag_timeout=etag_timeout,
                resume_download=resume_download,
                token=token,
                local_files_only=local_files_only,
                legacy_cache_layout=legacy_cache_layout,
            )
    
        if cache_dir is None:
            cache_dir = HUGGINGFACE_HUB_CACHE
        if revision is None:
            revision = DEFAULT_REVISION
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)
    
        if subfolder == "":
            subfolder = None
        if subfolder is not None:
            # This is used to create a URL, and not a local path, hence the forward slash.
            filename = f"{subfolder}/{filename}"
    
        if repo_type is None:
            repo_type = "model"
        if repo_type not in REPO_TYPES:
            raise ValueError(
                f"Invalid repo type: {repo_type}. Accepted repo types are:"
                f" {str(REPO_TYPES)}"
            )
    
        storage_folder = os.path.join(
            cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
        )
        os.makedirs(storage_folder, exist_ok=True)
    
        # cross platform transcription of filename, to be used as a local file path.
        relative_filename = os.path.join(*filename.split("/"))
    
        # if user provides a commit_hash and they already have the file on disk,
        # shortcut everything.
        if REGEX_COMMIT_HASH.match(revision):
            pointer_path = os.path.join(
                storage_folder, "snapshots", revision, relative_filename
            )
            if os.path.exists(pointer_path):
                return pointer_path
    
        url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
    
        headers = build_hf_headers(
            token=token,
            library_name=library_name,
            library_version=library_version,
            user_agent=user_agent,
        )
    
        url_to_download = url
        etag = None
        commit_hash = None
        if not local_files_only:
            try:
                try:
                    metadata = get_hf_file_metadata(
                        url=url,
                        token=token,
                        proxies=proxies,
                        timeout=etag_timeout,
                    )
                except EntryNotFoundError as http_error:
                    # Cache the non-existence of the file and raise
                    commit_hash = http_error.response.headers.get(
                        HUGGINGFACE_HEADER_X_REPO_COMMIT
                    )
                    if commit_hash is not None and not legacy_cache_layout:
                        no_exist_file_path = (
                            Path(storage_folder)
                            / ".no_exist"
                            / commit_hash
                            / relative_filename
                        )
                        no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                        no_exist_file_path.touch()
                        _cache_commit_hash_for_specific_revision(
                            storage_folder, revision, commit_hash
                        )
                    raise
    
                # Commit hash must exist
                commit_hash = metadata.commit_hash
                if commit_hash is None:
                    raise OSError(
                        "Distant resource does not seem to be on huggingface.co (missing"
                        " commit header)."
                    )
    
                # Etag must exist
                etag = metadata.etag
                # We favor a custom header indicating the etag of the linked resource, and
                # we fallback to the regular etag header.
                # If we don't have any of those, raise an error.
                if etag is None:
                    raise OSError(
                        "Distant resource does not have an ETag, we won't be able to"
                        " reliably ensure reproducibility."
                    )
    
                # In case of a redirect, save an extra redirect on the request.get call,
                # and ensure we download the exact atomic version even if it changed
                # between the HEAD and the GET (unlikely, but hey).
                # Useful for lfs blobs that are stored on a CDN.
                if metadata.location != url:
                    url_to_download = metadata.location
                    # Remove authorization header when downloading a LFS blob
                    headers.pop("authorization", None)
            except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
                # Actually raise for those subclasses of ConnectionError
                raise
            except (
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                OfflineModeIsEnabled,
            ):
                # Otherwise, our Internet connection is down.
                # etag is None
                pass
    
        # etag is None == we don't have a connection or we passed local_files_only.
        # try to get the last downloaded one from the specified revision.
        # If the specified revision is a commit hash, look inside "snapshots".
        # If the specified revision is a branch or tag, look inside "refs".
        if etag is None:
            # In those cases, we cannot force download.
            if force_download:
                raise ValueError(
                    "We have no connection or you passed local_files_only, so"
                    " force_download is not an accepted option."
                )
    
            # Try to get "commit_hash" from "revision"
            commit_hash = None
            if REGEX_COMMIT_HASH.match(revision):
                commit_hash = revision
            else:
                ref_path = os.path.join(storage_folder, "refs", revision)
                if os.path.isfile(ref_path):
                    with open(ref_path) as f:
                        commit_hash = f.read()
    
            # Return pointer file if exists
            if commit_hash is not None:
                pointer_path = os.path.join(
                    storage_folder, "snapshots", commit_hash, relative_filename
                )
                if os.path.exists(pointer_path):
                    return pointer_path
    
            # If we couldn't find an appropriate file on disk,
            # raise an error.
            # If files cannot be found and local_files_only=True,
            # the models might've been found if local_files_only=False
            # Notify the user about that
            if local_files_only:
                raise LocalEntryNotFoundError(
>                   "Cannot find the requested files in the disk cache and"
                    " outgoing traffic has been disabled. To enable hf.co look-ups"
                    " and downloads online, set 'local_files_only' to False."
                )
E               huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/file_download.py:1206: LocalEntryNotFoundError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_controlnet.StableDiffusionControlNetPipelineSlowTests testMethod=test_mlsd>

    def test_mlsd(self):
>       controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-mlsd")

tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py:229: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/models/modeling_utils.py:387: in from_pretrained
    **kwargs,
ppdiffusers/configuration_utils.py:343: in load_config
    from_hf_hub=from_hf_hub,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-mlsd'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
                    revision=revision,
                )
                return model_file
    
            except RepositoryNotFoundError:
                raise EnvironmentError(
                    f"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a "
                    "token having permission to this repo with `use_auth_token` or log in with `huggingface-cli "
                    "login`."
                )
            except RevisionNotFoundError:
                raise EnvironmentError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for "
                    "this model name. Check the model page at "
                    f"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions."
                )
            except EntryNotFoundError:
>               raise EnvironmentError(f"{pretrained_model_name_or_path} does not appear to have a file named {filename}.")
E               OSError: lllyasviel/sd-controlnet-mlsd does not appear to have a file named config.json.

ppdiffusers/utils/download_utils.py:435: OSError
____________ StableDiffusionControlNetPipelineSlowTests.test_normal ____________

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-normal'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
>                   revision=revision,
                )

ppdiffusers/utils/download_utils.py:417: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('lllyasviel/sd-controlnet-normal',)
kwargs = {'cache_dir': '/root/mttest/test_caches/diffusers', 'filename': 'config.json', 'force_download': False, 'local_files_only': True, ...}
has_token = False, arg_name = 'revision', arg_value = None

    @wraps(fn)
    def _inner_fn(*args, **kwargs):
        has_token = False
        for arg_name, arg_value in chain(
            zip(signature.parameters, args),  # Args values
            kwargs.items(),  # Kwargs values
        ):
            if arg_name == "repo_id":
                validate_repo_id(arg_value)
    
            elif arg_name == "token" and arg_value is not None:
                has_token = True
    
        if check_use_auth_token:
            kwargs = smoothly_deprecate_use_auth_token(
                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
            )
    
>       return fn(*args, **kwargs)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

repo_id = 'lllyasviel/sd-controlnet-normal', filename = 'config.json'

    @validate_hf_hub_args
    def hf_hub_download(
        repo_id: str,
        filename: str,
        *,
        subfolder: Optional[str] = None,
        repo_type: Optional[str] = None,
        revision: Optional[str] = None,
        library_name: Optional[str] = None,
        library_version: Optional[str] = None,
        cache_dir: Union[str, Path, None] = None,
        user_agent: Union[Dict, str, None] = None,
        force_download: bool = False,
        force_filename: Optional[str] = None,
        proxies: Optional[Dict] = None,
        etag_timeout: float = 10,
        resume_download: bool = False,
        token: Union[bool, str, None] = None,
        local_files_only: bool = False,
        legacy_cache_layout: bool = False,
    ):
        """Download a given file if it's not already present in the local cache.
    
        The new cache file layout looks like this:
        - The cache directory contains one subfolder per repo_id (namespaced by repo type)
        - inside each repo folder:
            - refs is a list of the latest known revision => commit_hash pairs
            - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on
              whether they're LFS files or not)
            - snapshots contains one subfolder per commit, each "commit" contains the subset of the files
              that have been resolved at that particular commit. Each filename is a symlink to the blob
              at that particular commit.
    
        ```
        [  96]  .
        └── [ 160]  models--julien-c--EsperBERTo-small
            ├── [ 160]  blobs
            │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
            │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e
            │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812
            ├── [  96]  refs
            │   └── [  40]  main
            └── [ 128]  snapshots
                ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f
                │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812
                │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
                └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48
                    ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e
                    └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
        ```
    
        Args:
            repo_id (`str`):
                A user or an organization name and a repo name separated by a `/`.
            filename (`str`):
                The name of the file in the repo.
            subfolder (`str`, *optional*):
                An optional value corresponding to a folder inside the model repo.
            repo_type (`str`, *optional*):
                Set to `"dataset"` or `"space"` if uploading to a dataset or space,
                `None` or `"model"` if uploading to a model. Default is `None`.
            revision (`str`, *optional*):
                An optional Git revision id which can be a branch name, a tag, or a
                commit hash.
            library_name (`str`, *optional*):
                The name of the library to which the object corresponds.
            library_version (`str`, *optional*):
                The version of the library.
            cache_dir (`str`, `Path`, *optional*):
                Path to the folder where cached files are stored.
            user_agent (`dict`, `str`, *optional*):
                The user-agent info in the form of a dictionary or a string.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether the file should be downloaded even if it already exists in
                the local cache.
            proxies (`dict`, *optional*):
                Dictionary mapping protocol to the URL of the proxy passed to
                `requests.request`.
            etag_timeout (`float`, *optional*, defaults to `10`):
                When fetching ETag, how many seconds to wait for the server to send
                data before giving up which is passed to `requests.request`.
            resume_download (`bool`, *optional*, defaults to `False`):
                If `True`, resume a previously interrupted download.
            token (`str`, `bool`, *optional*):
                A token to be used for the download.
                    - If `True`, the token is read from the HuggingFace config
                      folder.
                    - If a string, it's used as the authentication token.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, avoid downloading the file and return the path to the
                local cached file if it exists.
            legacy_cache_layout (`bool`, *optional*, defaults to `False`):
                If `True`, uses the legacy file cache layout i.e. just call [`hf_hub_url`]
                then `cached_download`. This is deprecated as the new cache layout is
                more powerful.
    
        Returns:
            Local path (string) of file or if networking is off, last version of
            file cached on disk.
    
        <Tip>
    
        Raises the following errors:
    
            - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)
              if `token=True` and the token cannot be found.
            - [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)
              if ETag cannot be determined.
            - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
              if some parameter value is invalid
            - [`~utils.RepositoryNotFoundError`]
              If the repository to download from cannot be found. This may be because it doesn't exist,
              or because it is set to `private` and you do not have access.
            - [`~utils.RevisionNotFoundError`]
              If the revision to download from cannot be found.
            - [`~utils.EntryNotFoundError`]
              If the file to download cannot be found.
            - [`~utils.LocalEntryNotFoundError`]
              If network is disabled or unavailable and file is not found in cache.
    
        </Tip>
        """
        if force_filename is not None:
            warnings.warn(
                "The `force_filename` parameter is deprecated as a new caching system, "
                "which keeps the filenames as they are on the Hub, is now in place.",
                FutureWarning,
            )
            legacy_cache_layout = True
    
        if legacy_cache_layout:
            url = hf_hub_url(
                repo_id,
                filename,
                subfolder=subfolder,
                repo_type=repo_type,
                revision=revision,
            )
    
            return cached_download(
                url,
                library_name=library_name,
                library_version=library_version,
                cache_dir=cache_dir,
                user_agent=user_agent,
                force_download=force_download,
                force_filename=force_filename,
                proxies=proxies,
                etag_timeout=etag_timeout,
                resume_download=resume_download,
                token=token,
                local_files_only=local_files_only,
                legacy_cache_layout=legacy_cache_layout,
            )
    
        if cache_dir is None:
            cache_dir = HUGGINGFACE_HUB_CACHE
        if revision is None:
            revision = DEFAULT_REVISION
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)
    
        if subfolder == "":
            subfolder = None
        if subfolder is not None:
            # This is used to create a URL, and not a local path, hence the forward slash.
            filename = f"{subfolder}/{filename}"
    
        if repo_type is None:
            repo_type = "model"
        if repo_type not in REPO_TYPES:
            raise ValueError(
                f"Invalid repo type: {repo_type}. Accepted repo types are:"
                f" {str(REPO_TYPES)}"
            )
    
        storage_folder = os.path.join(
            cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
        )
        os.makedirs(storage_folder, exist_ok=True)
    
        # cross platform transcription of filename, to be used as a local file path.
        relative_filename = os.path.join(*filename.split("/"))
    
        # if user provides a commit_hash and they already have the file on disk,
        # shortcut everything.
        if REGEX_COMMIT_HASH.match(revision):
            pointer_path = os.path.join(
                storage_folder, "snapshots", revision, relative_filename
            )
            if os.path.exists(pointer_path):
                return pointer_path
    
        url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
    
        headers = build_hf_headers(
            token=token,
            library_name=library_name,
            library_version=library_version,
            user_agent=user_agent,
        )
    
        url_to_download = url
        etag = None
        commit_hash = None
        if not local_files_only:
            try:
                try:
                    metadata = get_hf_file_metadata(
                        url=url,
                        token=token,
                        proxies=proxies,
                        timeout=etag_timeout,
                    )
                except EntryNotFoundError as http_error:
                    # Cache the non-existence of the file and raise
                    commit_hash = http_error.response.headers.get(
                        HUGGINGFACE_HEADER_X_REPO_COMMIT
                    )
                    if commit_hash is not None and not legacy_cache_layout:
                        no_exist_file_path = (
                            Path(storage_folder)
                            / ".no_exist"
                            / commit_hash
                            / relative_filename
                        )
                        no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                        no_exist_file_path.touch()
                        _cache_commit_hash_for_specific_revision(
                            storage_folder, revision, commit_hash
                        )
                    raise
    
                # Commit hash must exist
                commit_hash = metadata.commit_hash
                if commit_hash is None:
                    raise OSError(
                        "Distant resource does not seem to be on huggingface.co (missing"
                        " commit header)."
                    )
    
                # Etag must exist
                etag = metadata.etag
                # We favor a custom header indicating the etag of the linked resource, and
                # we fallback to the regular etag header.
                # If we don't have any of those, raise an error.
                if etag is None:
                    raise OSError(
                        "Distant resource does not have an ETag, we won't be able to"
                        " reliably ensure reproducibility."
                    )
    
                # In case of a redirect, save an extra redirect on the request.get call,
                # and ensure we download the exact atomic version even if it changed
                # between the HEAD and the GET (unlikely, but hey).
                # Useful for lfs blobs that are stored on a CDN.
                if metadata.location != url:
                    url_to_download = metadata.location
                    # Remove authorization header when downloading a LFS blob
                    headers.pop("authorization", None)
            except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
                # Actually raise for those subclasses of ConnectionError
                raise
            except (
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                OfflineModeIsEnabled,
            ):
                # Otherwise, our Internet connection is down.
                # etag is None
                pass
    
        # etag is None == we don't have a connection or we passed local_files_only.
        # try to get the last downloaded one from the specified revision.
        # If the specified revision is a commit hash, look inside "snapshots".
        # If the specified revision is a branch or tag, look inside "refs".
        if etag is None:
            # In those cases, we cannot force download.
            if force_download:
                raise ValueError(
                    "We have no connection or you passed local_files_only, so"
                    " force_download is not an accepted option."
                )
    
            # Try to get "commit_hash" from "revision"
            commit_hash = None
            if REGEX_COMMIT_HASH.match(revision):
                commit_hash = revision
            else:
                ref_path = os.path.join(storage_folder, "refs", revision)
                if os.path.isfile(ref_path):
                    with open(ref_path) as f:
                        commit_hash = f.read()
    
            # Return pointer file if exists
            if commit_hash is not None:
                pointer_path = os.path.join(
                    storage_folder, "snapshots", commit_hash, relative_filename
                )
                if os.path.exists(pointer_path):
                    return pointer_path
    
            # If we couldn't find an appropriate file on disk,
            # raise an error.
            # If files cannot be found and local_files_only=True,
            # the models might've been found if local_files_only=False
            # Notify the user about that
            if local_files_only:
                raise LocalEntryNotFoundError(
>                   "Cannot find the requested files in the disk cache and"
                    " outgoing traffic has been disabled. To enable hf.co look-ups"
                    " and downloads online, set 'local_files_only' to False."
                )
E               huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/file_download.py:1206: LocalEntryNotFoundError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_controlnet.StableDiffusionControlNetPipelineSlowTests testMethod=test_normal>

    def test_normal(self):
>       controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-normal")

tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py:255: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/models/modeling_utils.py:387: in from_pretrained
    **kwargs,
ppdiffusers/configuration_utils.py:343: in load_config
    from_hf_hub=from_hf_hub,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-normal'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
                    revision=revision,
                )
                return model_file
    
            except RepositoryNotFoundError:
                raise EnvironmentError(
                    f"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a "
                    "token having permission to this repo with `use_auth_token` or log in with `huggingface-cli "
                    "login`."
                )
            except RevisionNotFoundError:
                raise EnvironmentError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for "
                    "this model name. Check the model page at "
                    f"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions."
                )
            except EntryNotFoundError:
>               raise EnvironmentError(f"{pretrained_model_name_or_path} does not appear to have a file named {filename}.")
E               OSError: lllyasviel/sd-controlnet-normal does not appear to have a file named config.json.

ppdiffusers/utils/download_utils.py:435: OSError
___________ StableDiffusionControlNetPipelineSlowTests.test_openpose ___________

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-openpose'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
>                   revision=revision,
                )

ppdiffusers/utils/download_utils.py:417: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('lllyasviel/sd-controlnet-openpose',)
kwargs = {'cache_dir': '/root/mttest/test_caches/diffusers', 'filename': 'config.json', 'force_download': False, 'local_files_only': True, ...}
has_token = False, arg_name = 'revision', arg_value = None

    @wraps(fn)
    def _inner_fn(*args, **kwargs):
        has_token = False
        for arg_name, arg_value in chain(
            zip(signature.parameters, args),  # Args values
            kwargs.items(),  # Kwargs values
        ):
            if arg_name == "repo_id":
                validate_repo_id(arg_value)
    
            elif arg_name == "token" and arg_value is not None:
                has_token = True
    
        if check_use_auth_token:
            kwargs = smoothly_deprecate_use_auth_token(
                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
            )
    
>       return fn(*args, **kwargs)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

repo_id = 'lllyasviel/sd-controlnet-openpose', filename = 'config.json'

    @validate_hf_hub_args
    def hf_hub_download(
        repo_id: str,
        filename: str,
        *,
        subfolder: Optional[str] = None,
        repo_type: Optional[str] = None,
        revision: Optional[str] = None,
        library_name: Optional[str] = None,
        library_version: Optional[str] = None,
        cache_dir: Union[str, Path, None] = None,
        user_agent: Union[Dict, str, None] = None,
        force_download: bool = False,
        force_filename: Optional[str] = None,
        proxies: Optional[Dict] = None,
        etag_timeout: float = 10,
        resume_download: bool = False,
        token: Union[bool, str, None] = None,
        local_files_only: bool = False,
        legacy_cache_layout: bool = False,
    ):
        """Download a given file if it's not already present in the local cache.
    
        The new cache file layout looks like this:
        - The cache directory contains one subfolder per repo_id (namespaced by repo type)
        - inside each repo folder:
            - refs is a list of the latest known revision => commit_hash pairs
            - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on
              whether they're LFS files or not)
            - snapshots contains one subfolder per commit, each "commit" contains the subset of the files
              that have been resolved at that particular commit. Each filename is a symlink to the blob
              at that particular commit.
    
        ```
        [  96]  .
        └── [ 160]  models--julien-c--EsperBERTo-small
            ├── [ 160]  blobs
            │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
            │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e
            │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812
            ├── [  96]  refs
            │   └── [  40]  main
            └── [ 128]  snapshots
                ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f
                │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812
                │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
                └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48
                    ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e
                    └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
        ```
    
        Args:
            repo_id (`str`):
                A user or an organization name and a repo name separated by a `/`.
            filename (`str`):
                The name of the file in the repo.
            subfolder (`str`, *optional*):
                An optional value corresponding to a folder inside the model repo.
            repo_type (`str`, *optional*):
                Set to `"dataset"` or `"space"` if uploading to a dataset or space,
                `None` or `"model"` if uploading to a model. Default is `None`.
            revision (`str`, *optional*):
                An optional Git revision id which can be a branch name, a tag, or a
                commit hash.
            library_name (`str`, *optional*):
                The name of the library to which the object corresponds.
            library_version (`str`, *optional*):
                The version of the library.
            cache_dir (`str`, `Path`, *optional*):
                Path to the folder where cached files are stored.
            user_agent (`dict`, `str`, *optional*):
                The user-agent info in the form of a dictionary or a string.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether the file should be downloaded even if it already exists in
                the local cache.
            proxies (`dict`, *optional*):
                Dictionary mapping protocol to the URL of the proxy passed to
                `requests.request`.
            etag_timeout (`float`, *optional*, defaults to `10`):
                When fetching ETag, how many seconds to wait for the server to send
                data before giving up which is passed to `requests.request`.
            resume_download (`bool`, *optional*, defaults to `False`):
                If `True`, resume a previously interrupted download.
            token (`str`, `bool`, *optional*):
                A token to be used for the download.
                    - If `True`, the token is read from the HuggingFace config
                      folder.
                    - If a string, it's used as the authentication token.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, avoid downloading the file and return the path to the
                local cached file if it exists.
            legacy_cache_layout (`bool`, *optional*, defaults to `False`):
                If `True`, uses the legacy file cache layout i.e. just call [`hf_hub_url`]
                then `cached_download`. This is deprecated as the new cache layout is
                more powerful.
    
        Returns:
            Local path (string) of file or if networking is off, last version of
            file cached on disk.
    
        <Tip>
    
        Raises the following errors:
    
            - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)
              if `token=True` and the token cannot be found.
            - [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)
              if ETag cannot be determined.
            - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
              if some parameter value is invalid
            - [`~utils.RepositoryNotFoundError`]
              If the repository to download from cannot be found. This may be because it doesn't exist,
              or because it is set to `private` and you do not have access.
            - [`~utils.RevisionNotFoundError`]
              If the revision to download from cannot be found.
            - [`~utils.EntryNotFoundError`]
              If the file to download cannot be found.
            - [`~utils.LocalEntryNotFoundError`]
              If network is disabled or unavailable and file is not found in cache.
    
        </Tip>
        """
        if force_filename is not None:
            warnings.warn(
                "The `force_filename` parameter is deprecated as a new caching system, "
                "which keeps the filenames as they are on the Hub, is now in place.",
                FutureWarning,
            )
            legacy_cache_layout = True
    
        if legacy_cache_layout:
            url = hf_hub_url(
                repo_id,
                filename,
                subfolder=subfolder,
                repo_type=repo_type,
                revision=revision,
            )
    
            return cached_download(
                url,
                library_name=library_name,
                library_version=library_version,
                cache_dir=cache_dir,
                user_agent=user_agent,
                force_download=force_download,
                force_filename=force_filename,
                proxies=proxies,
                etag_timeout=etag_timeout,
                resume_download=resume_download,
                token=token,
                local_files_only=local_files_only,
                legacy_cache_layout=legacy_cache_layout,
            )
    
        if cache_dir is None:
            cache_dir = HUGGINGFACE_HUB_CACHE
        if revision is None:
            revision = DEFAULT_REVISION
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)
    
        if subfolder == "":
            subfolder = None
        if subfolder is not None:
            # This is used to create a URL, and not a local path, hence the forward slash.
            filename = f"{subfolder}/{filename}"
    
        if repo_type is None:
            repo_type = "model"
        if repo_type not in REPO_TYPES:
            raise ValueError(
                f"Invalid repo type: {repo_type}. Accepted repo types are:"
                f" {str(REPO_TYPES)}"
            )
    
        storage_folder = os.path.join(
            cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
        )
        os.makedirs(storage_folder, exist_ok=True)
    
        # cross platform transcription of filename, to be used as a local file path.
        relative_filename = os.path.join(*filename.split("/"))
    
        # if user provides a commit_hash and they already have the file on disk,
        # shortcut everything.
        if REGEX_COMMIT_HASH.match(revision):
            pointer_path = os.path.join(
                storage_folder, "snapshots", revision, relative_filename
            )
            if os.path.exists(pointer_path):
                return pointer_path
    
        url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
    
        headers = build_hf_headers(
            token=token,
            library_name=library_name,
            library_version=library_version,
            user_agent=user_agent,
        )
    
        url_to_download = url
        etag = None
        commit_hash = None
        if not local_files_only:
            try:
                try:
                    metadata = get_hf_file_metadata(
                        url=url,
                        token=token,
                        proxies=proxies,
                        timeout=etag_timeout,
                    )
                except EntryNotFoundError as http_error:
                    # Cache the non-existence of the file and raise
                    commit_hash = http_error.response.headers.get(
                        HUGGINGFACE_HEADER_X_REPO_COMMIT
                    )
                    if commit_hash is not None and not legacy_cache_layout:
                        no_exist_file_path = (
                            Path(storage_folder)
                            / ".no_exist"
                            / commit_hash
                            / relative_filename
                        )
                        no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                        no_exist_file_path.touch()
                        _cache_commit_hash_for_specific_revision(
                            storage_folder, revision, commit_hash
                        )
                    raise
    
                # Commit hash must exist
                commit_hash = metadata.commit_hash
                if commit_hash is None:
                    raise OSError(
                        "Distant resource does not seem to be on huggingface.co (missing"
                        " commit header)."
                    )
    
                # Etag must exist
                etag = metadata.etag
                # We favor a custom header indicating the etag of the linked resource, and
                # we fallback to the regular etag header.
                # If we don't have any of those, raise an error.
                if etag is None:
                    raise OSError(
                        "Distant resource does not have an ETag, we won't be able to"
                        " reliably ensure reproducibility."
                    )
    
                # In case of a redirect, save an extra redirect on the request.get call,
                # and ensure we download the exact atomic version even if it changed
                # between the HEAD and the GET (unlikely, but hey).
                # Useful for lfs blobs that are stored on a CDN.
                if metadata.location != url:
                    url_to_download = metadata.location
                    # Remove authorization header when downloading a LFS blob
                    headers.pop("authorization", None)
            except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
                # Actually raise for those subclasses of ConnectionError
                raise
            except (
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                OfflineModeIsEnabled,
            ):
                # Otherwise, our Internet connection is down.
                # etag is None
                pass
    
        # etag is None == we don't have a connection or we passed local_files_only.
        # try to get the last downloaded one from the specified revision.
        # If the specified revision is a commit hash, look inside "snapshots".
        # If the specified revision is a branch or tag, look inside "refs".
        if etag is None:
            # In those cases, we cannot force download.
            if force_download:
                raise ValueError(
                    "We have no connection or you passed local_files_only, so"
                    " force_download is not an accepted option."
                )
    
            # Try to get "commit_hash" from "revision"
            commit_hash = None
            if REGEX_COMMIT_HASH.match(revision):
                commit_hash = revision
            else:
                ref_path = os.path.join(storage_folder, "refs", revision)
                if os.path.isfile(ref_path):
                    with open(ref_path) as f:
                        commit_hash = f.read()
    
            # Return pointer file if exists
            if commit_hash is not None:
                pointer_path = os.path.join(
                    storage_folder, "snapshots", commit_hash, relative_filename
                )
                if os.path.exists(pointer_path):
                    return pointer_path
    
            # If we couldn't find an appropriate file on disk,
            # raise an error.
            # If files cannot be found and local_files_only=True,
            # the models might've been found if local_files_only=False
            # Notify the user about that
            if local_files_only:
                raise LocalEntryNotFoundError(
>                   "Cannot find the requested files in the disk cache and"
                    " outgoing traffic has been disabled. To enable hf.co look-ups"
                    " and downloads online, set 'local_files_only' to False."
                )
E               huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/file_download.py:1206: LocalEntryNotFoundError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_controlnet.StableDiffusionControlNetPipelineSlowTests testMethod=test_openpose>

    def test_openpose(self):
>       controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-openpose")

tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py:281: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/models/modeling_utils.py:387: in from_pretrained
    **kwargs,
ppdiffusers/configuration_utils.py:343: in load_config
    from_hf_hub=from_hf_hub,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-openpose'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
                    revision=revision,
                )
                return model_file
    
            except RepositoryNotFoundError:
                raise EnvironmentError(
                    f"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a "
                    "token having permission to this repo with `use_auth_token` or log in with `huggingface-cli "
                    "login`."
                )
            except RevisionNotFoundError:
                raise EnvironmentError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for "
                    "this model name. Check the model page at "
                    f"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions."
                )
            except EntryNotFoundError:
>               raise EnvironmentError(f"{pretrained_model_name_or_path} does not appear to have a file named {filename}.")
E               OSError: lllyasviel/sd-controlnet-openpose does not appear to have a file named config.json.

ppdiffusers/utils/download_utils.py:435: OSError
___________ StableDiffusionControlNetPipelineSlowTests.test_scribble ___________

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-scribble'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
>                   revision=revision,
                )

ppdiffusers/utils/download_utils.py:417: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('lllyasviel/sd-controlnet-scribble',)
kwargs = {'cache_dir': '/root/mttest/test_caches/diffusers', 'filename': 'config.json', 'force_download': False, 'local_files_only': True, ...}
has_token = False, arg_name = 'revision', arg_value = None

    @wraps(fn)
    def _inner_fn(*args, **kwargs):
        has_token = False
        for arg_name, arg_value in chain(
            zip(signature.parameters, args),  # Args values
            kwargs.items(),  # Kwargs values
        ):
            if arg_name == "repo_id":
                validate_repo_id(arg_value)
    
            elif arg_name == "token" and arg_value is not None:
                has_token = True
    
        if check_use_auth_token:
            kwargs = smoothly_deprecate_use_auth_token(
                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
            )
    
>       return fn(*args, **kwargs)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

repo_id = 'lllyasviel/sd-controlnet-scribble', filename = 'config.json'

    @validate_hf_hub_args
    def hf_hub_download(
        repo_id: str,
        filename: str,
        *,
        subfolder: Optional[str] = None,
        repo_type: Optional[str] = None,
        revision: Optional[str] = None,
        library_name: Optional[str] = None,
        library_version: Optional[str] = None,
        cache_dir: Union[str, Path, None] = None,
        user_agent: Union[Dict, str, None] = None,
        force_download: bool = False,
        force_filename: Optional[str] = None,
        proxies: Optional[Dict] = None,
        etag_timeout: float = 10,
        resume_download: bool = False,
        token: Union[bool, str, None] = None,
        local_files_only: bool = False,
        legacy_cache_layout: bool = False,
    ):
        """Download a given file if it's not already present in the local cache.
    
        The new cache file layout looks like this:
        - The cache directory contains one subfolder per repo_id (namespaced by repo type)
        - inside each repo folder:
            - refs is a list of the latest known revision => commit_hash pairs
            - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on
              whether they're LFS files or not)
            - snapshots contains one subfolder per commit, each "commit" contains the subset of the files
              that have been resolved at that particular commit. Each filename is a symlink to the blob
              at that particular commit.
    
        ```
        [  96]  .
        └── [ 160]  models--julien-c--EsperBERTo-small
            ├── [ 160]  blobs
            │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
            │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e
            │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812
            ├── [  96]  refs
            │   └── [  40]  main
            └── [ 128]  snapshots
                ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f
                │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812
                │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
                └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48
                    ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e
                    └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
        ```
    
        Args:
            repo_id (`str`):
                A user or an organization name and a repo name separated by a `/`.
            filename (`str`):
                The name of the file in the repo.
            subfolder (`str`, *optional*):
                An optional value corresponding to a folder inside the model repo.
            repo_type (`str`, *optional*):
                Set to `"dataset"` or `"space"` if uploading to a dataset or space,
                `None` or `"model"` if uploading to a model. Default is `None`.
            revision (`str`, *optional*):
                An optional Git revision id which can be a branch name, a tag, or a
                commit hash.
            library_name (`str`, *optional*):
                The name of the library to which the object corresponds.
            library_version (`str`, *optional*):
                The version of the library.
            cache_dir (`str`, `Path`, *optional*):
                Path to the folder where cached files are stored.
            user_agent (`dict`, `str`, *optional*):
                The user-agent info in the form of a dictionary or a string.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether the file should be downloaded even if it already exists in
                the local cache.
            proxies (`dict`, *optional*):
                Dictionary mapping protocol to the URL of the proxy passed to
                `requests.request`.
            etag_timeout (`float`, *optional*, defaults to `10`):
                When fetching ETag, how many seconds to wait for the server to send
                data before giving up which is passed to `requests.request`.
            resume_download (`bool`, *optional*, defaults to `False`):
                If `True`, resume a previously interrupted download.
            token (`str`, `bool`, *optional*):
                A token to be used for the download.
                    - If `True`, the token is read from the HuggingFace config
                      folder.
                    - If a string, it's used as the authentication token.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, avoid downloading the file and return the path to the
                local cached file if it exists.
            legacy_cache_layout (`bool`, *optional*, defaults to `False`):
                If `True`, uses the legacy file cache layout i.e. just call [`hf_hub_url`]
                then `cached_download`. This is deprecated as the new cache layout is
                more powerful.
    
        Returns:
            Local path (string) of file or if networking is off, last version of
            file cached on disk.
    
        <Tip>
    
        Raises the following errors:
    
            - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)
              if `token=True` and the token cannot be found.
            - [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)
              if ETag cannot be determined.
            - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
              if some parameter value is invalid
            - [`~utils.RepositoryNotFoundError`]
              If the repository to download from cannot be found. This may be because it doesn't exist,
              or because it is set to `private` and you do not have access.
            - [`~utils.RevisionNotFoundError`]
              If the revision to download from cannot be found.
            - [`~utils.EntryNotFoundError`]
              If the file to download cannot be found.
            - [`~utils.LocalEntryNotFoundError`]
              If network is disabled or unavailable and file is not found in cache.
    
        </Tip>
        """
        if force_filename is not None:
            warnings.warn(
                "The `force_filename` parameter is deprecated as a new caching system, "
                "which keeps the filenames as they are on the Hub, is now in place.",
                FutureWarning,
            )
            legacy_cache_layout = True
    
        if legacy_cache_layout:
            url = hf_hub_url(
                repo_id,
                filename,
                subfolder=subfolder,
                repo_type=repo_type,
                revision=revision,
            )
    
            return cached_download(
                url,
                library_name=library_name,
                library_version=library_version,
                cache_dir=cache_dir,
                user_agent=user_agent,
                force_download=force_download,
                force_filename=force_filename,
                proxies=proxies,
                etag_timeout=etag_timeout,
                resume_download=resume_download,
                token=token,
                local_files_only=local_files_only,
                legacy_cache_layout=legacy_cache_layout,
            )
    
        if cache_dir is None:
            cache_dir = HUGGINGFACE_HUB_CACHE
        if revision is None:
            revision = DEFAULT_REVISION
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)
    
        if subfolder == "":
            subfolder = None
        if subfolder is not None:
            # This is used to create a URL, and not a local path, hence the forward slash.
            filename = f"{subfolder}/{filename}"
    
        if repo_type is None:
            repo_type = "model"
        if repo_type not in REPO_TYPES:
            raise ValueError(
                f"Invalid repo type: {repo_type}. Accepted repo types are:"
                f" {str(REPO_TYPES)}"
            )
    
        storage_folder = os.path.join(
            cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
        )
        os.makedirs(storage_folder, exist_ok=True)
    
        # cross platform transcription of filename, to be used as a local file path.
        relative_filename = os.path.join(*filename.split("/"))
    
        # if user provides a commit_hash and they already have the file on disk,
        # shortcut everything.
        if REGEX_COMMIT_HASH.match(revision):
            pointer_path = os.path.join(
                storage_folder, "snapshots", revision, relative_filename
            )
            if os.path.exists(pointer_path):
                return pointer_path
    
        url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
    
        headers = build_hf_headers(
            token=token,
            library_name=library_name,
            library_version=library_version,
            user_agent=user_agent,
        )
    
        url_to_download = url
        etag = None
        commit_hash = None
        if not local_files_only:
            try:
                try:
                    metadata = get_hf_file_metadata(
                        url=url,
                        token=token,
                        proxies=proxies,
                        timeout=etag_timeout,
                    )
                except EntryNotFoundError as http_error:
                    # Cache the non-existence of the file and raise
                    commit_hash = http_error.response.headers.get(
                        HUGGINGFACE_HEADER_X_REPO_COMMIT
                    )
                    if commit_hash is not None and not legacy_cache_layout:
                        no_exist_file_path = (
                            Path(storage_folder)
                            / ".no_exist"
                            / commit_hash
                            / relative_filename
                        )
                        no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                        no_exist_file_path.touch()
                        _cache_commit_hash_for_specific_revision(
                            storage_folder, revision, commit_hash
                        )
                    raise
    
                # Commit hash must exist
                commit_hash = metadata.commit_hash
                if commit_hash is None:
                    raise OSError(
                        "Distant resource does not seem to be on huggingface.co (missing"
                        " commit header)."
                    )
    
                # Etag must exist
                etag = metadata.etag
                # We favor a custom header indicating the etag of the linked resource, and
                # we fallback to the regular etag header.
                # If we don't have any of those, raise an error.
                if etag is None:
                    raise OSError(
                        "Distant resource does not have an ETag, we won't be able to"
                        " reliably ensure reproducibility."
                    )
    
                # In case of a redirect, save an extra redirect on the request.get call,
                # and ensure we download the exact atomic version even if it changed
                # between the HEAD and the GET (unlikely, but hey).
                # Useful for lfs blobs that are stored on a CDN.
                if metadata.location != url:
                    url_to_download = metadata.location
                    # Remove authorization header when downloading a LFS blob
                    headers.pop("authorization", None)
            except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
                # Actually raise for those subclasses of ConnectionError
                raise
            except (
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                OfflineModeIsEnabled,
            ):
                # Otherwise, our Internet connection is down.
                # etag is None
                pass
    
        # etag is None == we don't have a connection or we passed local_files_only.
        # try to get the last downloaded one from the specified revision.
        # If the specified revision is a commit hash, look inside "snapshots".
        # If the specified revision is a branch or tag, look inside "refs".
        if etag is None:
            # In those cases, we cannot force download.
            if force_download:
                raise ValueError(
                    "We have no connection or you passed local_files_only, so"
                    " force_download is not an accepted option."
                )
    
            # Try to get "commit_hash" from "revision"
            commit_hash = None
            if REGEX_COMMIT_HASH.match(revision):
                commit_hash = revision
            else:
                ref_path = os.path.join(storage_folder, "refs", revision)
                if os.path.isfile(ref_path):
                    with open(ref_path) as f:
                        commit_hash = f.read()
    
            # Return pointer file if exists
            if commit_hash is not None:
                pointer_path = os.path.join(
                    storage_folder, "snapshots", commit_hash, relative_filename
                )
                if os.path.exists(pointer_path):
                    return pointer_path
    
            # If we couldn't find an appropriate file on disk,
            # raise an error.
            # If files cannot be found and local_files_only=True,
            # the models might've been found if local_files_only=False
            # Notify the user about that
            if local_files_only:
                raise LocalEntryNotFoundError(
>                   "Cannot find the requested files in the disk cache and"
                    " outgoing traffic has been disabled. To enable hf.co look-ups"
                    " and downloads online, set 'local_files_only' to False."
                )
E               huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/file_download.py:1206: LocalEntryNotFoundError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_controlnet.StableDiffusionControlNetPipelineSlowTests testMethod=test_scribble>

    def test_scribble(self):
>       controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-scribble")

tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py:307: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/models/modeling_utils.py:387: in from_pretrained
    **kwargs,
ppdiffusers/configuration_utils.py:343: in load_config
    from_hf_hub=from_hf_hub,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-scribble'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
                    revision=revision,
                )
                return model_file
    
            except RepositoryNotFoundError:
                raise EnvironmentError(
                    f"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a "
                    "token having permission to this repo with `use_auth_token` or log in with `huggingface-cli "
                    "login`."
                )
            except RevisionNotFoundError:
                raise EnvironmentError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for "
                    "this model name. Check the model page at "
                    f"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions."
                )
            except EntryNotFoundError:
>               raise EnvironmentError(f"{pretrained_model_name_or_path} does not appear to have a file named {filename}.")
E               OSError: lllyasviel/sd-controlnet-scribble does not appear to have a file named config.json.

ppdiffusers/utils/download_utils.py:435: OSError
_____________ StableDiffusionControlNetPipelineSlowTests.test_seg ______________

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-seg'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
>                   revision=revision,
                )

ppdiffusers/utils/download_utils.py:417: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

args = ('lllyasviel/sd-controlnet-seg',)
kwargs = {'cache_dir': '/root/mttest/test_caches/diffusers', 'filename': 'config.json', 'force_download': False, 'local_files_only': True, ...}
has_token = False, arg_name = 'revision', arg_value = None

    @wraps(fn)
    def _inner_fn(*args, **kwargs):
        has_token = False
        for arg_name, arg_value in chain(
            zip(signature.parameters, args),  # Args values
            kwargs.items(),  # Kwargs values
        ):
            if arg_name == "repo_id":
                validate_repo_id(arg_value)
    
            elif arg_name == "token" and arg_value is not None:
                has_token = True
    
        if check_use_auth_token:
            kwargs = smoothly_deprecate_use_auth_token(
                fn_name=fn.__name__, has_token=has_token, kwargs=kwargs
            )
    
>       return fn(*args, **kwargs)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/utils/_validators.py:124: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

repo_id = 'lllyasviel/sd-controlnet-seg', filename = 'config.json'

    @validate_hf_hub_args
    def hf_hub_download(
        repo_id: str,
        filename: str,
        *,
        subfolder: Optional[str] = None,
        repo_type: Optional[str] = None,
        revision: Optional[str] = None,
        library_name: Optional[str] = None,
        library_version: Optional[str] = None,
        cache_dir: Union[str, Path, None] = None,
        user_agent: Union[Dict, str, None] = None,
        force_download: bool = False,
        force_filename: Optional[str] = None,
        proxies: Optional[Dict] = None,
        etag_timeout: float = 10,
        resume_download: bool = False,
        token: Union[bool, str, None] = None,
        local_files_only: bool = False,
        legacy_cache_layout: bool = False,
    ):
        """Download a given file if it's not already present in the local cache.
    
        The new cache file layout looks like this:
        - The cache directory contains one subfolder per repo_id (namespaced by repo type)
        - inside each repo folder:
            - refs is a list of the latest known revision => commit_hash pairs
            - blobs contains the actual file blobs (identified by their git-sha or sha256, depending on
              whether they're LFS files or not)
            - snapshots contains one subfolder per commit, each "commit" contains the subset of the files
              that have been resolved at that particular commit. Each filename is a symlink to the blob
              at that particular commit.
    
        ```
        [  96]  .
        └── [ 160]  models--julien-c--EsperBERTo-small
            ├── [ 160]  blobs
            │   ├── [321M]  403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
            │   ├── [ 398]  7cb18dc9bafbfcf74629a4b760af1b160957a83e
            │   └── [1.4K]  d7edf6bd2a681fb0175f7735299831ee1b22b812
            ├── [  96]  refs
            │   └── [  40]  main
            └── [ 128]  snapshots
                ├── [ 128]  2439f60ef33a0d46d85da5001d52aeda5b00ce9f
                │   ├── [  52]  README.md -> ../../blobs/d7edf6bd2a681fb0175f7735299831ee1b22b812
                │   └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
                └── [ 128]  bbc77c8132af1cc5cf678da3f1ddf2de43606d48
                    ├── [  52]  README.md -> ../../blobs/7cb18dc9bafbfcf74629a4b760af1b160957a83e
                    └── [  76]  pytorch_model.bin -> ../../blobs/403450e234d65943a7dcf7e05a771ce3c92faa84dd07db4ac20f592037a1e4bd
        ```
    
        Args:
            repo_id (`str`):
                A user or an organization name and a repo name separated by a `/`.
            filename (`str`):
                The name of the file in the repo.
            subfolder (`str`, *optional*):
                An optional value corresponding to a folder inside the model repo.
            repo_type (`str`, *optional*):
                Set to `"dataset"` or `"space"` if uploading to a dataset or space,
                `None` or `"model"` if uploading to a model. Default is `None`.
            revision (`str`, *optional*):
                An optional Git revision id which can be a branch name, a tag, or a
                commit hash.
            library_name (`str`, *optional*):
                The name of the library to which the object corresponds.
            library_version (`str`, *optional*):
                The version of the library.
            cache_dir (`str`, `Path`, *optional*):
                Path to the folder where cached files are stored.
            user_agent (`dict`, `str`, *optional*):
                The user-agent info in the form of a dictionary or a string.
            force_download (`bool`, *optional*, defaults to `False`):
                Whether the file should be downloaded even if it already exists in
                the local cache.
            proxies (`dict`, *optional*):
                Dictionary mapping protocol to the URL of the proxy passed to
                `requests.request`.
            etag_timeout (`float`, *optional*, defaults to `10`):
                When fetching ETag, how many seconds to wait for the server to send
                data before giving up which is passed to `requests.request`.
            resume_download (`bool`, *optional*, defaults to `False`):
                If `True`, resume a previously interrupted download.
            token (`str`, `bool`, *optional*):
                A token to be used for the download.
                    - If `True`, the token is read from the HuggingFace config
                      folder.
                    - If a string, it's used as the authentication token.
            local_files_only (`bool`, *optional*, defaults to `False`):
                If `True`, avoid downloading the file and return the path to the
                local cached file if it exists.
            legacy_cache_layout (`bool`, *optional*, defaults to `False`):
                If `True`, uses the legacy file cache layout i.e. just call [`hf_hub_url`]
                then `cached_download`. This is deprecated as the new cache layout is
                more powerful.
    
        Returns:
            Local path (string) of file or if networking is off, last version of
            file cached on disk.
    
        <Tip>
    
        Raises the following errors:
    
            - [`EnvironmentError`](https://docs.python.org/3/library/exceptions.html#EnvironmentError)
              if `token=True` and the token cannot be found.
            - [`OSError`](https://docs.python.org/3/library/exceptions.html#OSError)
              if ETag cannot be determined.
            - [`ValueError`](https://docs.python.org/3/library/exceptions.html#ValueError)
              if some parameter value is invalid
            - [`~utils.RepositoryNotFoundError`]
              If the repository to download from cannot be found. This may be because it doesn't exist,
              or because it is set to `private` and you do not have access.
            - [`~utils.RevisionNotFoundError`]
              If the revision to download from cannot be found.
            - [`~utils.EntryNotFoundError`]
              If the file to download cannot be found.
            - [`~utils.LocalEntryNotFoundError`]
              If network is disabled or unavailable and file is not found in cache.
    
        </Tip>
        """
        if force_filename is not None:
            warnings.warn(
                "The `force_filename` parameter is deprecated as a new caching system, "
                "which keeps the filenames as they are on the Hub, is now in place.",
                FutureWarning,
            )
            legacy_cache_layout = True
    
        if legacy_cache_layout:
            url = hf_hub_url(
                repo_id,
                filename,
                subfolder=subfolder,
                repo_type=repo_type,
                revision=revision,
            )
    
            return cached_download(
                url,
                library_name=library_name,
                library_version=library_version,
                cache_dir=cache_dir,
                user_agent=user_agent,
                force_download=force_download,
                force_filename=force_filename,
                proxies=proxies,
                etag_timeout=etag_timeout,
                resume_download=resume_download,
                token=token,
                local_files_only=local_files_only,
                legacy_cache_layout=legacy_cache_layout,
            )
    
        if cache_dir is None:
            cache_dir = HUGGINGFACE_HUB_CACHE
        if revision is None:
            revision = DEFAULT_REVISION
        if isinstance(cache_dir, Path):
            cache_dir = str(cache_dir)
    
        if subfolder == "":
            subfolder = None
        if subfolder is not None:
            # This is used to create a URL, and not a local path, hence the forward slash.
            filename = f"{subfolder}/{filename}"
    
        if repo_type is None:
            repo_type = "model"
        if repo_type not in REPO_TYPES:
            raise ValueError(
                f"Invalid repo type: {repo_type}. Accepted repo types are:"
                f" {str(REPO_TYPES)}"
            )
    
        storage_folder = os.path.join(
            cache_dir, repo_folder_name(repo_id=repo_id, repo_type=repo_type)
        )
        os.makedirs(storage_folder, exist_ok=True)
    
        # cross platform transcription of filename, to be used as a local file path.
        relative_filename = os.path.join(*filename.split("/"))
    
        # if user provides a commit_hash and they already have the file on disk,
        # shortcut everything.
        if REGEX_COMMIT_HASH.match(revision):
            pointer_path = os.path.join(
                storage_folder, "snapshots", revision, relative_filename
            )
            if os.path.exists(pointer_path):
                return pointer_path
    
        url = hf_hub_url(repo_id, filename, repo_type=repo_type, revision=revision)
    
        headers = build_hf_headers(
            token=token,
            library_name=library_name,
            library_version=library_version,
            user_agent=user_agent,
        )
    
        url_to_download = url
        etag = None
        commit_hash = None
        if not local_files_only:
            try:
                try:
                    metadata = get_hf_file_metadata(
                        url=url,
                        token=token,
                        proxies=proxies,
                        timeout=etag_timeout,
                    )
                except EntryNotFoundError as http_error:
                    # Cache the non-existence of the file and raise
                    commit_hash = http_error.response.headers.get(
                        HUGGINGFACE_HEADER_X_REPO_COMMIT
                    )
                    if commit_hash is not None and not legacy_cache_layout:
                        no_exist_file_path = (
                            Path(storage_folder)
                            / ".no_exist"
                            / commit_hash
                            / relative_filename
                        )
                        no_exist_file_path.parent.mkdir(parents=True, exist_ok=True)
                        no_exist_file_path.touch()
                        _cache_commit_hash_for_specific_revision(
                            storage_folder, revision, commit_hash
                        )
                    raise
    
                # Commit hash must exist
                commit_hash = metadata.commit_hash
                if commit_hash is None:
                    raise OSError(
                        "Distant resource does not seem to be on huggingface.co (missing"
                        " commit header)."
                    )
    
                # Etag must exist
                etag = metadata.etag
                # We favor a custom header indicating the etag of the linked resource, and
                # we fallback to the regular etag header.
                # If we don't have any of those, raise an error.
                if etag is None:
                    raise OSError(
                        "Distant resource does not have an ETag, we won't be able to"
                        " reliably ensure reproducibility."
                    )
    
                # In case of a redirect, save an extra redirect on the request.get call,
                # and ensure we download the exact atomic version even if it changed
                # between the HEAD and the GET (unlikely, but hey).
                # Useful for lfs blobs that are stored on a CDN.
                if metadata.location != url:
                    url_to_download = metadata.location
                    # Remove authorization header when downloading a LFS blob
                    headers.pop("authorization", None)
            except (requests.exceptions.SSLError, requests.exceptions.ProxyError):
                # Actually raise for those subclasses of ConnectionError
                raise
            except (
                requests.exceptions.ConnectionError,
                requests.exceptions.Timeout,
                OfflineModeIsEnabled,
            ):
                # Otherwise, our Internet connection is down.
                # etag is None
                pass
    
        # etag is None == we don't have a connection or we passed local_files_only.
        # try to get the last downloaded one from the specified revision.
        # If the specified revision is a commit hash, look inside "snapshots".
        # If the specified revision is a branch or tag, look inside "refs".
        if etag is None:
            # In those cases, we cannot force download.
            if force_download:
                raise ValueError(
                    "We have no connection or you passed local_files_only, so"
                    " force_download is not an accepted option."
                )
    
            # Try to get "commit_hash" from "revision"
            commit_hash = None
            if REGEX_COMMIT_HASH.match(revision):
                commit_hash = revision
            else:
                ref_path = os.path.join(storage_folder, "refs", revision)
                if os.path.isfile(ref_path):
                    with open(ref_path) as f:
                        commit_hash = f.read()
    
            # Return pointer file if exists
            if commit_hash is not None:
                pointer_path = os.path.join(
                    storage_folder, "snapshots", commit_hash, relative_filename
                )
                if os.path.exists(pointer_path):
                    return pointer_path
    
            # If we couldn't find an appropriate file on disk,
            # raise an error.
            # If files cannot be found and local_files_only=True,
            # the models might've been found if local_files_only=False
            # Notify the user about that
            if local_files_only:
                raise LocalEntryNotFoundError(
>                   "Cannot find the requested files in the disk cache and"
                    " outgoing traffic has been disabled. To enable hf.co look-ups"
                    " and downloads online, set 'local_files_only' to False."
                )
E               huggingface_hub.utils._errors.LocalEntryNotFoundError: Cannot find the requested files in the disk cache and outgoing traffic has been disabled. To enable hf.co look-ups and downloads online, set 'local_files_only' to False.

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/huggingface_hub/file_download.py:1206: LocalEntryNotFoundError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_controlnet.StableDiffusionControlNetPipelineSlowTests testMethod=test_seg>

    def test_seg(self):
>       controlnet = ControlNetModel.from_pretrained("lllyasviel/sd-controlnet-seg")

tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py:333: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/models/modeling_utils.py:387: in from_pretrained
    **kwargs,
ppdiffusers/configuration_utils.py:343: in load_config
    from_hf_hub=from_hf_hub,
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

pretrained_model_name_or_path = 'lllyasviel/sd-controlnet-seg'

    def bos_hf_download(
        pretrained_model_name_or_path,
        *,
        filename,
        subfolder,
        cache_dir,
        force_download=False,
        revision=None,
        from_hf_hub=False,
        proxies=None,
        resume_download=False,
        local_files_only=None,
        use_auth_token=None,
        user_agent=None,
    ):
        if from_hf_hub:
            try:
                model_file = hf_hub_download(
                    pretrained_model_name_or_path,
                    filename=filename,
                    cache_dir=cache_dir,
                    force_download=force_download,
                    proxies=proxies,
                    resume_download=resume_download,
                    local_files_only=local_files_only,
                    use_auth_token=use_auth_token,
                    user_agent=user_agent,
                    subfolder=subfolder,
                    revision=revision,
                )
                return model_file
    
            except RepositoryNotFoundError:
                raise EnvironmentError(
                    f"{pretrained_model_name_or_path} is not a local folder and is not a valid model identifier "
                    "listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a "
                    "token having permission to this repo with `use_auth_token` or log in with `huggingface-cli "
                    "login`."
                )
            except RevisionNotFoundError:
                raise EnvironmentError(
                    f"{revision} is not a valid git identifier (branch name, tag name or commit id) that exists for "
                    "this model name. Check the model page at "
                    f"'https://huggingface.co/{pretrained_model_name_or_path}' for available revisions."
                )
            except EntryNotFoundError:
>               raise EnvironmentError(f"{pretrained_model_name_or_path} does not appear to have a file named {filename}.")
E               OSError: lllyasviel/sd-controlnet-seg does not appear to have a file named config.json.

ppdiffusers/utils/download_utils.py:435: OSError
_ StableDiffusionImageVariationPipelineSlowTests.test_stable_diffusion_img_variation_intermediate_state _

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_image_variation.StableDiffusionImageVariationPipelineSlowTests testMethod=test_stable_diffusion_img_variation_intermediate_state>

    def test_stable_diffusion_img_variation_intermediate_state(self):
        number_of_steps = 0
    
        def callback_fn(step: int, timestep: int, latents: paddle.Tensor
            ) ->None:
            callback_fn.has_been_called = True
            nonlocal number_of_steps
            number_of_steps += 1
            if step == 1:
                latents = latents.detach().cpu().numpy()
                assert latents.shape == (1, 4, 64, 64)
                latents_slice = latents[0, -3:, -3:, -1]
                expected_slice = np.array([-0.1621, 0.2837, -0.7979, -
                    0.1221, -1.3057, 0.7681, -2.1191, 0.0464, 1.6309])
                assert np.abs(latents_slice.flatten() - expected_slice).max(
                    ) < 0.05
            elif step == 2:
                latents = latents.detach().cpu().numpy()
                assert latents.shape == (1, 4, 64, 64)
                latents_slice = latents[0, -3:, -3:, -1]
                expected_slice = np.array([0.6299, 1.75, 1.1992, -2.1582, -
                    1.8994, 0.7334, -0.709, 1.0137, 1.5273])
                assert np.abs(latents_slice.flatten() - expected_slice).max(
                    ) < 0.05
        callback_fn.has_been_called = False
        pipe = StableDiffusionImageVariationPipeline.from_pretrained(
            'fusing/sd-image-variations-diffusers', safety_checker=None,
            paddle_dtype=paddle.float16)
        pipe.set_progress_bar_config(disable=None)
        pipe.enable_attention_slicing()
        inputs = self.get_inputs(dtype='float16')
>       pipe(**inputs, callback=callback_fn, callback_steps=1)

tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py:209: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/decorator.py:232: in fun
    return caller(func, *(extras + args), **kw)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py:396: in _decorate_function
    return func(*args, **kwargs)
ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py:344: in __call__
    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=image_embeddings).sample
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/unet_2d_condition.py:611: in forward
    cross_attention_kwargs=cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/unet_2d_blocks.py:886: in forward
    cross_attention_kwargs=cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/transformer_2d.py:269: in forward
    class_labels=class_labels,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/attention.py:319: in forward
    **cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/cross_attention.py:216: in forward
    **cross_attention_kwargs,
ppdiffusers/models/cross_attention.py:561: in __call__
    key = attn.to_k(encoder_hidden_states)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/nn/layer/common.py:175: in forward
    x=input, weight=self.weight, bias=self.bias, name=self.name
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = Tensor(shape=[2, 1, 512], dtype=float16, place=Place(gpu:0), stop_gradient=True,
       [[[ 0.        ,  0.        ,  ....        ]],

        [[-0.89257812, -1.42187500,  1.30175781, ...,  0.61767578,
           2.86914062,  0.95800781]]])
weight = Parameter containing:
Tensor(shape=[768, 320], dtype=float16, place=Place(gpu:0), stop_gradient=False,
       [[ 0.044...0,  0.01560974],
        [-0.11663818,  0.07733154, -0.07916260, ...,  0.00149918,
         -0.01997375, -0.04635620]])
bias = None, name = None

    def linear(x, weight, bias=None, name=None):
        r"""
    
        Fully-connected linear transformation operator. For each input :math:`X` ,
        the equation is:
    
        .. math::
    
            Out = XW + b
    
        where :math:`W` is the weight and :math:`b` is the bias.
    
        If the weight is a 2-D tensor of shape :math:`[in\_features, out\_features]` ,
        input should be a multi-dimensional tensor of shape
        :math:`[batch\_size, *, in\_features]` , where :math:`*` means any number of
        additional dimensions. The linear operator multiplies input tensor with
        weight and produces an output tensor of shape :math:`[batch\_size, *, out\_features]` ,
        If :math:`bias` is not None, the bias should be a 1-D tensor of shape
        :math:`[out\_features]` and will be added to the output.
    
        Parameters:
            x (Tensor): Input tensor. The data type should be float16, float32 or float64.
            weight (Tensor): Weight tensor. The data type should be float16, float32 or float64.
            bias (Tensor, optional): Bias tensor. The data type should be float16, float32 or float64.
                                     If it is set to None, no bias will be added to the output units.
            name (str, optional): Normally there is no need for user to set this parameter.
                                  For detailed information, please refer to :ref:`api_guide_Name` .
    
        Returns:
            Tensor, the shape is :math:`[batch\_size, *, out\_features]` and the
            data type is the same with input :math:`x` .
    
        Examples:
            .. code-block:: python
    
              import paddle
    
              x = paddle.randn((3, 2), dtype="float32")
              # x: [[-0.32342386 -1.200079  ]
              #     [ 0.7979031  -0.90978354]
              #     [ 0.40597573  1.8095392 ]]
              weight = paddle.full(shape=[2, 4], fill_value="0.5", dtype="float32", name="weight")
              # weight: [[0.5 0.5 0.5 0.5]
              #          [0.5 0.5 0.5 0.5]]
              bias = paddle.ones(shape=[4], dtype="float32", name="bias")
              # bias: [1. 1. 1. 1.]
              y = paddle.nn.functional.linear(x, weight, bias)
              # y: [[0.23824859 0.23824859 0.23824859 0.23824859]
              #     [0.9440598  0.9440598  0.9440598  0.9440598 ]
              #     [2.1077576  2.1077576  2.1077576  2.1077576 ]]
        """
        if in_dygraph_mode():
            # TODO(jiabin): using addmm for fast forward route
>           return _C_ops.linear(x, weight, bias)
E           ValueError: (InvalidArgument) Input(Y) has error dim.Y'dims[0] must be equal to 512But received Y'dims[0] is 768
E             [Hint: Expected y_dims[y_ndim - 2] == K, but received y_dims[y_ndim - 2]:768 != K:512.] (at /home/workplace/paddle_fmha_cutlass_for_yujun/Paddle/paddle/phi/kernels/impl/matmul_kernel_impl.h:314)
E             [operator < linear > error]

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/nn/functional/common.py:1867: ValueError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:09:35,972] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:09:35,973] [    INFO][0m - Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
[32m[2023-03-15 14:09:35,975] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/image_encoder/config.json[0m
[32m[2023-03-15 14:09:35,976] [    INFO][0m - Model config CLIPVisionConfig {
  "_name_or_path": "lambdalabs/sd-image-variations-diffusers/image_encoder",
  "architectures": [
    "CLIPVisionModelWithProjection"
  ],
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.25.0.dev0"
}
[0m
You have disabled the safety checker for <class 'ppdiffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_image_variation.StableDiffusionImageVariationPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. PaddleNLP team, diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
_ StableDiffusionImageVariationPipelineSlowTests.test_stable_diffusion_img_variation_pipeline_default _

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_image_variation.StableDiffusionImageVariationPipelineSlowTests testMethod=test_stable_diffusion_img_variation_pipeline_default>

    def test_stable_diffusion_img_variation_pipeline_default(self):
        sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(
            'fusing/sd-image-variations-diffusers', safety_checker=None)
        sd_pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
>       image = sd_pipe(**inputs).images

tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py:162: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/decorator.py:232: in fun
    return caller(func, *(extras + args), **kw)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py:396: in _decorate_function
    return func(*args, **kwargs)
ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py:344: in __call__
    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=image_embeddings).sample
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/unet_2d_condition.py:611: in forward
    cross_attention_kwargs=cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/unet_2d_blocks.py:886: in forward
    cross_attention_kwargs=cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/transformer_2d.py:269: in forward
    class_labels=class_labels,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/attention.py:319: in forward
    **cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/cross_attention.py:216: in forward
    **cross_attention_kwargs,
ppdiffusers/models/cross_attention.py:298: in __call__
    key = attn.to_k(encoder_hidden_states)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/nn/layer/common.py:175: in forward
    x=input, weight=self.weight, bias=self.bias, name=self.name
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = Tensor(shape=[2, 1, 512], dtype=float32, place=Place(gpu:0), stop_gradient=True,
       [[[ 0.        ,  0.        ,  ....        ]],

        [[-1.48928797,  0.45089719,  0.56765676, ...,  1.25701153,
          -0.68457711, -0.45567286]]])
weight = Parameter containing:
Tensor(shape=[768, 320], dtype=float32, place=Place(gpu:0), stop_gradient=False,
       [[ 0.044...6,  0.01560681],
        [-0.11663229,  0.07735243, -0.07919054, ...,  0.00149961,
         -0.01997135, -0.04636448]])
bias = None, name = None

    def linear(x, weight, bias=None, name=None):
        r"""
    
        Fully-connected linear transformation operator. For each input :math:`X` ,
        the equation is:
    
        .. math::
    
            Out = XW + b
    
        where :math:`W` is the weight and :math:`b` is the bias.
    
        If the weight is a 2-D tensor of shape :math:`[in\_features, out\_features]` ,
        input should be a multi-dimensional tensor of shape
        :math:`[batch\_size, *, in\_features]` , where :math:`*` means any number of
        additional dimensions. The linear operator multiplies input tensor with
        weight and produces an output tensor of shape :math:`[batch\_size, *, out\_features]` ,
        If :math:`bias` is not None, the bias should be a 1-D tensor of shape
        :math:`[out\_features]` and will be added to the output.
    
        Parameters:
            x (Tensor): Input tensor. The data type should be float16, float32 or float64.
            weight (Tensor): Weight tensor. The data type should be float16, float32 or float64.
            bias (Tensor, optional): Bias tensor. The data type should be float16, float32 or float64.
                                     If it is set to None, no bias will be added to the output units.
            name (str, optional): Normally there is no need for user to set this parameter.
                                  For detailed information, please refer to :ref:`api_guide_Name` .
    
        Returns:
            Tensor, the shape is :math:`[batch\_size, *, out\_features]` and the
            data type is the same with input :math:`x` .
    
        Examples:
            .. code-block:: python
    
              import paddle
    
              x = paddle.randn((3, 2), dtype="float32")
              # x: [[-0.32342386 -1.200079  ]
              #     [ 0.7979031  -0.90978354]
              #     [ 0.40597573  1.8095392 ]]
              weight = paddle.full(shape=[2, 4], fill_value="0.5", dtype="float32", name="weight")
              # weight: [[0.5 0.5 0.5 0.5]
              #          [0.5 0.5 0.5 0.5]]
              bias = paddle.ones(shape=[4], dtype="float32", name="bias")
              # bias: [1. 1. 1. 1.]
              y = paddle.nn.functional.linear(x, weight, bias)
              # y: [[0.23824859 0.23824859 0.23824859 0.23824859]
              #     [0.9440598  0.9440598  0.9440598  0.9440598 ]
              #     [2.1077576  2.1077576  2.1077576  2.1077576 ]]
        """
        if in_dygraph_mode():
            # TODO(jiabin): using addmm for fast forward route
>           return _C_ops.linear(x, weight, bias)
E           ValueError: (InvalidArgument) Input(Y) has error dim.Y'dims[0] must be equal to 512But received Y'dims[0] is 768
E             [Hint: Expected y_dims[y_ndim - 2] == K, but received y_dims[y_ndim - 2]:768 != K:512.] (at /home/workplace/paddle_fmha_cutlass_for_yujun/Paddle/paddle/phi/kernels/impl/matmul_kernel_impl.h:314)
E             [operator < linear > error]

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/nn/functional/common.py:1867: ValueError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:09:55,937] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:09:55,937] [    INFO][0m - Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
[32m[2023-03-15 14:09:55,940] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/image_encoder/config.json[0m
[32m[2023-03-15 14:09:55,941] [    INFO][0m - Model config CLIPVisionConfig {
  "_name_or_path": "lambdalabs/sd-image-variations-diffusers/image_encoder",
  "architectures": [
    "CLIPVisionModelWithProjection"
  ],
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.25.0.dev0"
}
[0m
You have disabled the safety checker for <class 'ppdiffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_image_variation.StableDiffusionImageVariationPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. PaddleNLP team, diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
___ StableDiffusionImageVariationPipelineNightlyTests.test_img_variation_dpm ___

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_image_variation.StableDiffusionImageVariationPipelineNightlyTests testMethod=test_img_variation_dpm>

    def test_img_variation_dpm(self):
        sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(
            'fusing/sd-image-variations-diffusers')
        sd_pipe.scheduler = DPMSolverMultistepScheduler.from_config(sd_pipe
            .scheduler.config)
        sd_pipe
        sd_pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
        inputs['num_inference_steps'] = 25
>       image = sd_pipe(**inputs).images[0]

tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py:258: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/decorator.py:232: in fun
    return caller(func, *(extras + args), **kw)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py:396: in _decorate_function
    return func(*args, **kwargs)
ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py:344: in __call__
    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=image_embeddings).sample
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/unet_2d_condition.py:611: in forward
    cross_attention_kwargs=cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/unet_2d_blocks.py:886: in forward
    cross_attention_kwargs=cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/transformer_2d.py:269: in forward
    class_labels=class_labels,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/attention.py:319: in forward
    **cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/cross_attention.py:216: in forward
    **cross_attention_kwargs,
ppdiffusers/models/cross_attention.py:298: in __call__
    key = attn.to_k(encoder_hidden_states)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/nn/layer/common.py:175: in forward
    x=input, weight=self.weight, bias=self.bias, name=self.name
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = Tensor(shape=[2, 1, 512], dtype=float32, place=Place(gpu:0), stop_gradient=True,
       [[[ 0.        ,  0.        ,  ....        ]],

        [[-0.14677177, -0.88242143, -0.00949924, ...,  0.50084162,
          -0.29174596, -0.20858495]]])
weight = Parameter containing:
Tensor(shape=[768, 320], dtype=float32, place=Place(gpu:0), stop_gradient=False,
       [[ 0.044...6,  0.01560681],
        [-0.11663229,  0.07735243, -0.07919054, ...,  0.00149961,
         -0.01997135, -0.04636448]])
bias = None, name = None

    def linear(x, weight, bias=None, name=None):
        r"""
    
        Fully-connected linear transformation operator. For each input :math:`X` ,
        the equation is:
    
        .. math::
    
            Out = XW + b
    
        where :math:`W` is the weight and :math:`b` is the bias.
    
        If the weight is a 2-D tensor of shape :math:`[in\_features, out\_features]` ,
        input should be a multi-dimensional tensor of shape
        :math:`[batch\_size, *, in\_features]` , where :math:`*` means any number of
        additional dimensions. The linear operator multiplies input tensor with
        weight and produces an output tensor of shape :math:`[batch\_size, *, out\_features]` ,
        If :math:`bias` is not None, the bias should be a 1-D tensor of shape
        :math:`[out\_features]` and will be added to the output.
    
        Parameters:
            x (Tensor): Input tensor. The data type should be float16, float32 or float64.
            weight (Tensor): Weight tensor. The data type should be float16, float32 or float64.
            bias (Tensor, optional): Bias tensor. The data type should be float16, float32 or float64.
                                     If it is set to None, no bias will be added to the output units.
            name (str, optional): Normally there is no need for user to set this parameter.
                                  For detailed information, please refer to :ref:`api_guide_Name` .
    
        Returns:
            Tensor, the shape is :math:`[batch\_size, *, out\_features]` and the
            data type is the same with input :math:`x` .
    
        Examples:
            .. code-block:: python
    
              import paddle
    
              x = paddle.randn((3, 2), dtype="float32")
              # x: [[-0.32342386 -1.200079  ]
              #     [ 0.7979031  -0.90978354]
              #     [ 0.40597573  1.8095392 ]]
              weight = paddle.full(shape=[2, 4], fill_value="0.5", dtype="float32", name="weight")
              # weight: [[0.5 0.5 0.5 0.5]
              #          [0.5 0.5 0.5 0.5]]
              bias = paddle.ones(shape=[4], dtype="float32", name="bias")
              # bias: [1. 1. 1. 1.]
              y = paddle.nn.functional.linear(x, weight, bias)
              # y: [[0.23824859 0.23824859 0.23824859 0.23824859]
              #     [0.9440598  0.9440598  0.9440598  0.9440598 ]
              #     [2.1077576  2.1077576  2.1077576  2.1077576 ]]
        """
        if in_dygraph_mode():
            # TODO(jiabin): using addmm for fast forward route
>           return _C_ops.linear(x, weight, bias)
E           ValueError: (InvalidArgument) Input(Y) has error dim.Y'dims[0] must be equal to 512But received Y'dims[0] is 768
E             [Hint: Expected y_dims[y_ndim - 2] == K, but received y_dims[y_ndim - 2]:768 != K:512.] (at /home/workplace/paddle_fmha_cutlass_for_yujun/Paddle/paddle/phi/kernels/impl/matmul_kernel_impl.h:314)
E             [operator < linear > error]

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/nn/functional/common.py:1867: ValueError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:10:06,251] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/safety_checker/config.json[0m
[32m[2023-03-15 14:10:06,252] [    INFO][0m - Model config CLIPVisionConfig {
  "attention_dropout": 0.0,
  "begin_suppress_tokens": null,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 768,
  "return_dict": true,
  "suppress_tokens": null,
  "tf_legacy_loss": false,
  "torch_dtype": null,
  "torchscript": false,
  "transformers_version": "4.25.0.dev0",
  "use_bfloat16": false
}
[0m
[32m[2023-03-15 14:10:17,335] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:10:17,335] [    INFO][0m - Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
[32m[2023-03-15 14:10:17,338] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/image_encoder/config.json[0m
[32m[2023-03-15 14:10:17,339] [    INFO][0m - Model config CLIPVisionConfig {
  "_name_or_path": "lambdalabs/sd-image-variations-diffusers/image_encoder",
  "architectures": [
    "CLIPVisionModelWithProjection"
  ],
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.25.0.dev0"
}
[0m
__ StableDiffusionImageVariationPipelineNightlyTests.test_img_variation_pndm ___

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_image_variation.StableDiffusionImageVariationPipelineNightlyTests testMethod=test_img_variation_pndm>

    def test_img_variation_pndm(self):
        sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(
            'fusing/sd-image-variations-diffusers')
        sd_pipe
        sd_pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
>       image = sd_pipe(**inputs).images[0]

tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py:242: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/decorator.py:232: in fun
    return caller(func, *(extras + args), **kw)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py:396: in _decorate_function
    return func(*args, **kwargs)
ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py:344: in __call__
    noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=image_embeddings).sample
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/unet_2d_condition.py:611: in forward
    cross_attention_kwargs=cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/unet_2d_blocks.py:886: in forward
    cross_attention_kwargs=cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/transformer_2d.py:269: in forward
    class_labels=class_labels,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/attention.py:319: in forward
    **cross_attention_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
ppdiffusers/models/cross_attention.py:216: in forward
    **cross_attention_kwargs,
ppdiffusers/models/cross_attention.py:298: in __call__
    key = attn.to_k(encoder_hidden_states)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1020: in __call__
    return self.forward(*inputs, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/nn/layer/common.py:175: in forward
    x=input, weight=self.weight, bias=self.bias, name=self.name
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

x = Tensor(shape=[2, 1, 512], dtype=float32, place=Place(gpu:0), stop_gradient=True,
       [[[ 0.        ,  0.        ,  ....        ]],

        [[ 2.45466399,  0.79690826,  0.69208694, ..., -0.41810393,
           2.12874889, -1.04742908]]])
weight = Parameter containing:
Tensor(shape=[768, 320], dtype=float32, place=Place(gpu:0), stop_gradient=False,
       [[ 0.044...6,  0.01560681],
        [-0.11663229,  0.07735243, -0.07919054, ...,  0.00149961,
         -0.01997135, -0.04636448]])
bias = None, name = None

    def linear(x, weight, bias=None, name=None):
        r"""
    
        Fully-connected linear transformation operator. For each input :math:`X` ,
        the equation is:
    
        .. math::
    
            Out = XW + b
    
        where :math:`W` is the weight and :math:`b` is the bias.
    
        If the weight is a 2-D tensor of shape :math:`[in\_features, out\_features]` ,
        input should be a multi-dimensional tensor of shape
        :math:`[batch\_size, *, in\_features]` , where :math:`*` means any number of
        additional dimensions. The linear operator multiplies input tensor with
        weight and produces an output tensor of shape :math:`[batch\_size, *, out\_features]` ,
        If :math:`bias` is not None, the bias should be a 1-D tensor of shape
        :math:`[out\_features]` and will be added to the output.
    
        Parameters:
            x (Tensor): Input tensor. The data type should be float16, float32 or float64.
            weight (Tensor): Weight tensor. The data type should be float16, float32 or float64.
            bias (Tensor, optional): Bias tensor. The data type should be float16, float32 or float64.
                                     If it is set to None, no bias will be added to the output units.
            name (str, optional): Normally there is no need for user to set this parameter.
                                  For detailed information, please refer to :ref:`api_guide_Name` .
    
        Returns:
            Tensor, the shape is :math:`[batch\_size, *, out\_features]` and the
            data type is the same with input :math:`x` .
    
        Examples:
            .. code-block:: python
    
              import paddle
    
              x = paddle.randn((3, 2), dtype="float32")
              # x: [[-0.32342386 -1.200079  ]
              #     [ 0.7979031  -0.90978354]
              #     [ 0.40597573  1.8095392 ]]
              weight = paddle.full(shape=[2, 4], fill_value="0.5", dtype="float32", name="weight")
              # weight: [[0.5 0.5 0.5 0.5]
              #          [0.5 0.5 0.5 0.5]]
              bias = paddle.ones(shape=[4], dtype="float32", name="bias")
              # bias: [1. 1. 1. 1.]
              y = paddle.nn.functional.linear(x, weight, bias)
              # y: [[0.23824859 0.23824859 0.23824859 0.23824859]
              #     [0.9440598  0.9440598  0.9440598  0.9440598 ]
              #     [2.1077576  2.1077576  2.1077576  2.1077576 ]]
        """
        if in_dygraph_mode():
            # TODO(jiabin): using addmm for fast forward route
>           return _C_ops.linear(x, weight, bias)
E           ValueError: (InvalidArgument) Input(Y) has error dim.Y'dims[0] must be equal to 512But received Y'dims[0] is 768
E             [Hint: Expected y_dims[y_ndim - 2] == K, but received y_dims[y_ndim - 2]:768 != K:512.] (at /home/workplace/paddle_fmha_cutlass_for_yujun/Paddle/paddle/phi/kernels/impl/matmul_kernel_impl.h:314)
E             [operator < linear > error]

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/nn/functional/common.py:1867: ValueError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:10:27,252] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/safety_checker/config.json[0m
[32m[2023-03-15 14:10:27,253] [    INFO][0m - Model config CLIPVisionConfig {
  "attention_dropout": 0.0,
  "begin_suppress_tokens": null,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 768,
  "return_dict": true,
  "suppress_tokens": null,
  "tf_legacy_loss": false,
  "torch_dtype": null,
  "torchscript": false,
  "transformers_version": "4.25.0.dev0",
  "use_bfloat16": false
}
[0m
[32m[2023-03-15 14:10:38,035] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:10:38,036] [    INFO][0m - Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
[32m[2023-03-15 14:10:38,039] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--fusing--sd-image-variations-diffusers/snapshots/4ff37f5862b207ff400a343efcdb349de2007e8b/image_encoder/config.json[0m
[32m[2023-03-15 14:10:38,040] [    INFO][0m - Model config CLIPVisionConfig {
  "_name_or_path": "lambdalabs/sd-image-variations-diffusers/image_encoder",
  "architectures": [
    "CLIPVisionModelWithProjection"
  ],
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.25.0.dev0"
}
[0m
_________ StableDiffusionImg2ImgPipelineNightlyTests.test_img2img_pndm _________

self = <urllib3.response.HTTPResponse object at 0x7f6debf01590>

    @contextmanager
    def _error_catcher(self):
        """
        Catch low-level python exceptions, instead re-raising urllib3
        variants, so that low-level exceptions are not leaked in the
        high-level api.
    
        On exit, release the connection back to the pool.
        """
        clean_exit = False
    
        try:
            try:
>               yield

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6debf01590>, amt = 10240
decode_content = True, cache_content = False

    def read(self, amt=None, decode_content=None, cache_content=False):
        """
        Similar to :meth:`http.client.HTTPResponse.read`, but with two additional
        parameters: ``decode_content`` and ``cache_content``.
    
        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param cache_content:
            If True, will save the returned data such that the same result is
            returned despite of the state of the underlying file object. This
            is useful if you want the ``.data`` property to continue working
            after having ``.read()`` the file object. (Overridden if ``amt`` is
            set.)
        """
        self._init_decoder()
        if decode_content is None:
            decode_content = self.decode_content
    
        if self._fp is None:
            return
    
        flush_decoder = False
        fp_closed = getattr(self._fp, "closed", False)
    
        with self._error_catcher():
>           data = self._fp_read(amt) if not fp_closed else b""

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6debf01590>, amt = 10240

    def _fp_read(self, amt):
        """
        Read a response with the thought that reading the number of bytes
        larger than can fit in a 32-bit int at a time via SSL in some
        known cases leads to an overflow error that has to be prevented
        if `amt` or `self.length_remaining` indicate that a problem may
        happen.
    
        The known cases:
          * 3.8 <= CPython < 3.9.7 because of a bug
            https://github.com/urllib3/urllib3/issues/2513#issuecomment-1152559900.
          * urllib3 injected with pyOpenSSL-backed SSL-support.
          * CPython < 3.10 only when `amt` does not fit 32-bit int.
        """
        assert self._fp
        c_int_max = 2 ** 31 - 1
        if (
            (
                (amt and amt > c_int_max)
                or (self.length_remaining and self.length_remaining > c_int_max)
            )
            and not util.IS_SECURETRANSPORT
            and (util.IS_PYOPENSSL or sys.version_info < (3, 10))
        ):
            buffer = io.BytesIO()
            # Besides `max_chunk_amt` being a maximum chunk size, it
            # affects memory overhead of reading a response by this
            # method in CPython.
            # `c_int_max` equal to 2 GiB - 1 byte is the actual maximum
            # chunk size that does not lead to an overflow error, but
            # 256 MiB is a compromise.
            max_chunk_amt = 2 ** 28
            while amt is None or amt != 0:
                if amt is not None:
                    chunk_amt = min(amt, max_chunk_amt)
                    amt -= chunk_amt
                else:
                    chunk_amt = max_chunk_amt
                data = self._fp.read(chunk_amt)
                if not data:
                    break
                buffer.write(data)
                del data  # to reduce peak memory usage by `max_chunk_amt`.
            return buffer.getvalue()
        else:
            # StringIO doesn't like amt=None
>           return self._fp.read(amt) if amt is not None else self._fp.read()

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:533: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <http.client.HTTPResponse object at 0x7f6debf01990>, amt = 10240

    def read(self, amt=None):
        if self.fp is None:
            return b""
    
        if self._method == "HEAD":
            self._close_conn()
            return b""
    
        if amt is not None:
            # Amount is given, implement using readinto
            b = bytearray(amt)
>           n = self.readinto(b)

/root/anaconda3/envs/benchmark/lib/python3.7/http/client.py:465: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <http.client.HTTPResponse object at 0x7f6debf01990>
b = bytearray(b'\x85m\x03?\xdd\xd7\x02?0\x1c\xcb>\x8f\xb8\xfd>\xb0K\x08?\xf0\xf0\xcb>\xf5`\x00?\xc1e\x0c?\x80\x88\xcc>\xaf...0\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00')

    def readinto(self, b):
        """Read up to len(b) bytes into bytearray b and return the number
        of bytes read.
        """
    
        if self.fp is None:
            return 0
    
        if self._method == "HEAD":
            self._close_conn()
            return 0
    
        if self.chunked:
            return self._readinto_chunked(b)
    
        if self.length is not None:
            if len(b) > self.length:
                # clip the read to the "end of response"
                b = memoryview(b)[0:self.length]
    
        # we do not use _safe_read() here because this may be a .will_close
        # connection, and the user is reading more bytes than will be provided
        # (for example, reading in 1k chunks)
>       n = self.fp.readinto(b)

/root/anaconda3/envs/benchmark/lib/python3.7/http/client.py:509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <socket.SocketIO object at 0x7f6debf019d0>
b = <memory at 0x7f6e700bf460>

    def readinto(self, b):
        """Read up to len(b) bytes into the writable buffer *b* and return
        the number of bytes read.  If the socket is non-blocking and no bytes
        are available, None is returned.
    
        If *b* is non-empty, a 0 return value indicates that the connection
        was shutdown at the other end.
        """
        self._checkClosed()
        self._checkReadable()
        if self._timeout_occurred:
            raise OSError("cannot read from timed out object")
        while True:
            try:
>               return self._sock.recv_into(b)

/root/anaconda3/envs/benchmark/lib/python3.7/socket.py:589: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>
buffer = <memory at 0x7f6e700bf460>, nbytes = 8192, flags = 0

    def recv_into(self, buffer, nbytes=None, flags=0):
        self._checkClosed()
        if buffer and (nbytes is None):
            nbytes = len(buffer)
        elif nbytes is None:
            nbytes = 1024
        if self._sslobj is not None:
            if flags != 0:
                raise ValueError(
                  "non-zero flags not allowed in calls to recv_into() on %s" %
                  self.__class__)
>           return self.read(nbytes, buffer)

/root/anaconda3/envs/benchmark/lib/python3.7/ssl.py:1071: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>
len = 8192, buffer = <memory at 0x7f6e700bf460>

    def read(self, len=1024, buffer=None):
        """Read up to LEN bytes and return them.
        Return zero-length string on EOF."""
    
        self._checkClosed()
        if self._sslobj is None:
            raise ValueError("Read on closed or unwrapped SSL socket.")
        try:
            if buffer is not None:
>               return self._sslobj.read(len, buffer)
E               ConnectionResetError: [Errno 104] Connection reset by peer

/root/anaconda3/envs/benchmark/lib/python3.7/ssl.py:929: ConnectionResetError

During handling of the above exception, another exception occurred:

    def generate():
        # Special case for urllib3.
        if hasattr(self.raw, "stream"):
            try:
>               yield from self.raw.stream(chunk_size, decode_content=True)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/models.py:816: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6debf01590>, amt = 10240
decode_content = True

    def stream(self, amt=2 ** 16, decode_content=None):
        """
        A generator wrapper for the read() method. A call will block until
        ``amt`` bytes have been read from the connection or until the
        connection is closed.
    
        :param amt:
            How much of the content to read. The generator will return up to
            much data per iteration, but may return less. This is particularly
            likely when using compressed data. However, the empty string will
            never be returned.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
        """
        if self.chunked and self.supports_chunked_reads():
            for line in self.read_chunked(amt, decode_content=decode_content):
                yield line
        else:
            while not is_fp_closed(self._fp):
>               data = self.read(amt=amt, decode_content=decode_content)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:628: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6debf01590>, amt = 10240
decode_content = True, cache_content = False

    def read(self, amt=None, decode_content=None, cache_content=False):
        """
        Similar to :meth:`http.client.HTTPResponse.read`, but with two additional
        parameters: ``decode_content`` and ``cache_content``.
    
        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param cache_content:
            If True, will save the returned data such that the same result is
            returned despite of the state of the underlying file object. This
            is useful if you want the ``.data`` property to continue working
            after having ``.read()`` the file object. (Overridden if ``amt`` is
            set.)
        """
        self._init_decoder()
        if decode_content is None:
            decode_content = self.decode_content
    
        if self._fp is None:
            return
    
        flush_decoder = False
        fp_closed = getattr(self._fp, "closed", False)
    
        with self._error_catcher():
            data = self._fp_read(amt) if not fp_closed else b""
            if amt is None:
                flush_decoder = True
            else:
                cache_content = False
                if (
                    amt != 0 and not data
                ):  # Platform-specific: Buggy versions of Python.
                    # Close the connection when no data is returned
                    #
                    # This is redundant to what httplib/http.client _should_
                    # already do.  However, versions of python released before
                    # December 15, 2012 (http://bugs.python.org/issue16298) do
                    # not properly close the connection in all cases. There is
                    # no harm in redundantly calling close.
                    self._fp.close()
                    flush_decoder = True
                    if self.enforce_content_length and self.length_remaining not in (
                        0,
                        None,
                    ):
                        # This is an edge case that httplib failed to cover due
                        # to concerns of backward compatibility. We're
                        # addressing it here to make sure IncompleteRead is
                        # raised during streaming, so all calls with incorrect
                        # Content-Length are caught.
>                       raise IncompleteRead(self._fp_bytes_read, self.length_remaining)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:593: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f6e01c7ad10>
type = <class 'ConnectionResetError'>
value = ConnectionResetError(104, 'Connection reset by peer')
traceback = <traceback object at 0x7f6debf0ef00>

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                raise RuntimeError("generator didn't stop")
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = type()
            try:
>               self.gen.throw(type, value, traceback)

/root/anaconda3/envs/benchmark/lib/python3.7/contextlib.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6debf01590>

    @contextmanager
    def _error_catcher(self):
        """
        Catch low-level python exceptions, instead re-raising urllib3
        variants, so that low-level exceptions are not leaked in the
        high-level api.
    
        On exit, release the connection back to the pool.
        """
        clean_exit = False
    
        try:
            try:
                yield
    
            except SocketTimeout:
                # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
                # there is yet no clean way to get at it from this context.
                raise ReadTimeoutError(self._pool, None, "Read timed out.")
    
            except BaseSSLError as e:
                # FIXME: Is there a better way to differentiate between SSLErrors?
                if "read operation timed out" not in str(e):
                    # SSL errors related to framing/MAC get wrapped and reraised here
                    raise SSLError(e)
    
                raise ReadTimeoutError(self._pool, None, "Read timed out.")
    
            except (HTTPException, SocketError) as e:
                # This includes IncompleteRead.
>               raise ProtocolError("Connection broken: %r" % e, e)
E               urllib3.exceptions.ProtocolError: ("Connection broken: ConnectionResetError(104, 'Connection reset by peer')", ConnectionResetError(104, 'Connection reset by peer'))

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:461: ProtocolError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_img2img.StableDiffusionImg2ImgPipelineNightlyTests testMethod=test_img2img_pndm>

    def test_img2img_pndm(self):
        sd_pipe = StableDiffusionImg2ImgPipeline.from_pretrained(
            'runwayml/stable-diffusion-v1-5')
        sd_pipe
        sd_pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
        image = sd_pipe(**inputs).images[0]
        expected_image = load_numpy(
>           'https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_img2img/stable_diffusion_1_5_pndm.npy'
            )

tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py:323: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/utils/testing_utils.py:197: in load_numpy
    response = requests.get(arry)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/api.py:73: in get
    return request("get", url, params=params, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/api.py:59: in request
    return session.request(method=method, url=url, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:587: in request
    resp = self.send(prep, **send_kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:723: in send
    history = [resp for resp in gen]
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:723: in <listcomp>
    history = [resp for resp in gen]
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:274: in resolve_redirects
    **adapter_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:745: in send
    r.content
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/models.py:899: in content
    self._content = b"".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def generate():
        # Special case for urllib3.
        if hasattr(self.raw, "stream"):
            try:
                yield from self.raw.stream(chunk_size, decode_content=True)
            except ProtocolError as e:
>               raise ChunkedEncodingError(e)
E               requests.exceptions.ChunkedEncodingError: ("Connection broken: ConnectionResetError(104, 'Connection reset by peer')", ConnectionResetError(104, 'Connection reset by peer'))

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/models.py:818: ChunkedEncodingError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:14:55,136] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-v1-5/snapshots/39593d5650112b4cc580433f6b0435385882d819/text_encoder/config.json[0m
[32m[2023-03-15 14:14:55,137] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.22.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:14:56,786] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-v1-5/snapshots/39593d5650112b4cc580433f6b0435385882d819/safety_checker/config.json[0m
[32m[2023-03-15 14:14:56,787] [    INFO][0m - Model config CLIPVisionConfig {
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 768,
  "return_dict": true,
  "tf_legacy_loss": false,
  "torch_dtype": null,
  "torchscript": false,
  "transformers_version": "4.22.0.dev0",
  "use_bfloat16": false
}
[0m
[32m[2023-03-15 14:15:07,332] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-v1-5/snapshots/39593d5650112b4cc580433f6b0435385882d819/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-v1-5/snapshots/39593d5650112b4cc580433f6b0435385882d819/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:15:07,333] [    INFO][0m - size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.[0m
[32m[2023-03-15 14:15:07,333] [    INFO][0m - crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.[0m
[32m[2023-03-15 14:15:07,333] [    INFO][0m - Image processor CLIPFeatureExtractor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPFeatureExtractor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
__ StableDiffusionInpaintPipelineSlowTests.test_stable_diffusion_inpaint_fp16 __

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_inpaint.StableDiffusionInpaintPipelineSlowTests testMethod=test_stable_diffusion_inpaint_fp16>

    def test_stable_diffusion_inpaint_fp16(self):
        pipe = StableDiffusionInpaintPipeline.from_pretrained(
            'runwayml/stable-diffusion-inpainting', paddle_dtype=paddle.float16,
            safety_checker=None)
        pipe.set_progress_bar_config(disable=None)
        pipe.enable_attention_slicing()
        inputs = self.get_inputs(dtype='float16')
        image = pipe(**inputs).images
        image_slice = image[0, 253:256, 253:256, -1].flatten()
        assert image.shape == (1, 512, 512, 3)
        expected_slice = np.array([0.06103516, 0.11010742, 0.10571289, 0.07910156, 0.08569336,
                                    0.08398438, 0.05273438, 0.03222656, 0.01660156])
>       assert np.abs(expected_slice - image_slice).max() < 0.05
E       AssertionError: assert nan < 0.05
E        +  where nan = <built-in method max of numpy.ndarray object at 0x7f7093b84f90>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f7093b84f90> = array([nan, nan, nan, nan, nan, nan, nan, nan, nan]).max
E        +      where array([nan, nan, nan, nan, nan, nan, nan, nan, nan]) = <ufunc 'absolute'>((array([0.06103516, 0.11010742, 0.10571289, 0.07910156, 0.08569336,\n       0.08398438, 0.05273438, 0.03222656, 0.01660156]) - array([nan, nan, nan, nan, nan, nan, nan, nan, nan], dtype=float32)))
E        +        where <ufunc 'absolute'> = np.abs

tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py:178: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:23:20,377] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-inpainting/snapshots/caac1048f28756b68042add4670bec6f4ae314f8/text_encoder/config.json[0m
[32m[2023-03-15 14:23:20,378] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.22.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:23:31,208] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-inpainting/snapshots/caac1048f28756b68042add4670bec6f4ae314f8/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-inpainting/snapshots/caac1048f28756b68042add4670bec6f4ae314f8/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:23:31,209] [    INFO][0m - size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.[0m
[32m[2023-03-15 14:23:31,209] [    INFO][0m - crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.[0m
[32m[2023-03-15 14:23:31,209] [    INFO][0m - Image processor CLIPFeatureExtractor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPFeatureExtractor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
You have disabled the safety checker for <class 'ppdiffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_inpaint.StableDiffusionInpaintPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. PaddleNLP team, diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
_________ StableDiffusionInpaintPipelineNightlyTests.test_inpaint_dpm __________

self = <urllib3.response.HTTPResponse object at 0x7f7093976e10>

    @contextmanager
    def _error_catcher(self):
        """
        Catch low-level python exceptions, instead re-raising urllib3
        variants, so that low-level exceptions are not leaked in the
        high-level api.
    
        On exit, release the connection back to the pool.
        """
        clean_exit = False
    
        try:
            try:
>               yield

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f7093976e10>, amt = 10240
decode_content = True, cache_content = False

    def read(self, amt=None, decode_content=None, cache_content=False):
        """
        Similar to :meth:`http.client.HTTPResponse.read`, but with two additional
        parameters: ``decode_content`` and ``cache_content``.
    
        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param cache_content:
            If True, will save the returned data such that the same result is
            returned despite of the state of the underlying file object. This
            is useful if you want the ``.data`` property to continue working
            after having ``.read()`` the file object. (Overridden if ``amt`` is
            set.)
        """
        self._init_decoder()
        if decode_content is None:
            decode_content = self.decode_content
    
        if self._fp is None:
            return
    
        flush_decoder = False
        fp_closed = getattr(self._fp, "closed", False)
    
        with self._error_catcher():
>           data = self._fp_read(amt) if not fp_closed else b""

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f7093976e10>, amt = 10240

    def _fp_read(self, amt):
        """
        Read a response with the thought that reading the number of bytes
        larger than can fit in a 32-bit int at a time via SSL in some
        known cases leads to an overflow error that has to be prevented
        if `amt` or `self.length_remaining` indicate that a problem may
        happen.
    
        The known cases:
          * 3.8 <= CPython < 3.9.7 because of a bug
            https://github.com/urllib3/urllib3/issues/2513#issuecomment-1152559900.
          * urllib3 injected with pyOpenSSL-backed SSL-support.
          * CPython < 3.10 only when `amt` does not fit 32-bit int.
        """
        assert self._fp
        c_int_max = 2 ** 31 - 1
        if (
            (
                (amt and amt > c_int_max)
                or (self.length_remaining and self.length_remaining > c_int_max)
            )
            and not util.IS_SECURETRANSPORT
            and (util.IS_PYOPENSSL or sys.version_info < (3, 10))
        ):
            buffer = io.BytesIO()
            # Besides `max_chunk_amt` being a maximum chunk size, it
            # affects memory overhead of reading a response by this
            # method in CPython.
            # `c_int_max` equal to 2 GiB - 1 byte is the actual maximum
            # chunk size that does not lead to an overflow error, but
            # 256 MiB is a compromise.
            max_chunk_amt = 2 ** 28
            while amt is None or amt != 0:
                if amt is not None:
                    chunk_amt = min(amt, max_chunk_amt)
                    amt -= chunk_amt
                else:
                    chunk_amt = max_chunk_amt
                data = self._fp.read(chunk_amt)
                if not data:
                    break
                buffer.write(data)
                del data  # to reduce peak memory usage by `max_chunk_amt`.
            return buffer.getvalue()
        else:
            # StringIO doesn't like amt=None
>           return self._fp.read(amt) if amt is not None else self._fp.read()

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:533: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <http.client.HTTPResponse object at 0x7f7093cd9a10>, amt = 10240

    def read(self, amt=None):
        if self.fp is None:
            return b""
    
        if self._method == "HEAD":
            self._close_conn()
            return b""
    
        if amt is not None:
            # Amount is given, implement using readinto
            b = bytearray(amt)
>           n = self.readinto(b)

/root/anaconda3/envs/benchmark/lib/python3.7/http/client.py:465: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <http.client.HTTPResponse object at 0x7f7093cd9a10>
b = bytearray(b'T\x05\x93>W*\xa2>\xb0H\xaf>A\xec\x9a>\xbc\xfe\xaa>\x96D\xb4>\xe0}\xab>@\xea\xb7>\x9b\x81\xc2>\x88B\xb2>J_\...0\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00')

    def readinto(self, b):
        """Read up to len(b) bytes into bytearray b and return the number
        of bytes read.
        """
    
        if self.fp is None:
            return 0
    
        if self._method == "HEAD":
            self._close_conn()
            return 0
    
        if self.chunked:
            return self._readinto_chunked(b)
    
        if self.length is not None:
            if len(b) > self.length:
                # clip the read to the "end of response"
                b = memoryview(b)[0:self.length]
    
        # we do not use _safe_read() here because this may be a .will_close
        # connection, and the user is reading more bytes than will be provided
        # (for example, reading in 1k chunks)
>       n = self.fp.readinto(b)

/root/anaconda3/envs/benchmark/lib/python3.7/http/client.py:509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <socket.SocketIO object at 0x7f6e05598310>
b = <memory at 0x7f7093c5d460>

    def readinto(self, b):
        """Read up to len(b) bytes into the writable buffer *b* and return
        the number of bytes read.  If the socket is non-blocking and no bytes
        are available, None is returned.
    
        If *b* is non-empty, a 0 return value indicates that the connection
        was shutdown at the other end.
        """
        self._checkClosed()
        self._checkReadable()
        if self._timeout_occurred:
            raise OSError("cannot read from timed out object")
        while True:
            try:
>               return self._sock.recv_into(b)

/root/anaconda3/envs/benchmark/lib/python3.7/socket.py:589: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>
buffer = <memory at 0x7f7093c5d460>, nbytes = 8192, flags = 0

    def recv_into(self, buffer, nbytes=None, flags=0):
        self._checkClosed()
        if buffer and (nbytes is None):
            nbytes = len(buffer)
        elif nbytes is None:
            nbytes = 1024
        if self._sslobj is not None:
            if flags != 0:
                raise ValueError(
                  "non-zero flags not allowed in calls to recv_into() on %s" %
                  self.__class__)
>           return self.read(nbytes, buffer)

/root/anaconda3/envs/benchmark/lib/python3.7/ssl.py:1071: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>
len = 8192, buffer = <memory at 0x7f7093c5d460>

    def read(self, len=1024, buffer=None):
        """Read up to LEN bytes and return them.
        Return zero-length string on EOF."""
    
        self._checkClosed()
        if self._sslobj is None:
            raise ValueError("Read on closed or unwrapped SSL socket.")
        try:
            if buffer is not None:
>               return self._sslobj.read(len, buffer)
E               ConnectionResetError: [Errno 104] Connection reset by peer

/root/anaconda3/envs/benchmark/lib/python3.7/ssl.py:929: ConnectionResetError

During handling of the above exception, another exception occurred:

    def generate():
        # Special case for urllib3.
        if hasattr(self.raw, "stream"):
            try:
>               yield from self.raw.stream(chunk_size, decode_content=True)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/models.py:816: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f7093976e10>, amt = 10240
decode_content = True

    def stream(self, amt=2 ** 16, decode_content=None):
        """
        A generator wrapper for the read() method. A call will block until
        ``amt`` bytes have been read from the connection or until the
        connection is closed.
    
        :param amt:
            How much of the content to read. The generator will return up to
            much data per iteration, but may return less. This is particularly
            likely when using compressed data. However, the empty string will
            never be returned.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
        """
        if self.chunked and self.supports_chunked_reads():
            for line in self.read_chunked(amt, decode_content=decode_content):
                yield line
        else:
            while not is_fp_closed(self._fp):
>               data = self.read(amt=amt, decode_content=decode_content)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:628: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f7093976e10>, amt = 10240
decode_content = True, cache_content = False

    def read(self, amt=None, decode_content=None, cache_content=False):
        """
        Similar to :meth:`http.client.HTTPResponse.read`, but with two additional
        parameters: ``decode_content`` and ``cache_content``.
    
        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param cache_content:
            If True, will save the returned data such that the same result is
            returned despite of the state of the underlying file object. This
            is useful if you want the ``.data`` property to continue working
            after having ``.read()`` the file object. (Overridden if ``amt`` is
            set.)
        """
        self._init_decoder()
        if decode_content is None:
            decode_content = self.decode_content
    
        if self._fp is None:
            return
    
        flush_decoder = False
        fp_closed = getattr(self._fp, "closed", False)
    
        with self._error_catcher():
            data = self._fp_read(amt) if not fp_closed else b""
            if amt is None:
                flush_decoder = True
            else:
                cache_content = False
                if (
                    amt != 0 and not data
                ):  # Platform-specific: Buggy versions of Python.
                    # Close the connection when no data is returned
                    #
                    # This is redundant to what httplib/http.client _should_
                    # already do.  However, versions of python released before
                    # December 15, 2012 (http://bugs.python.org/issue16298) do
                    # not properly close the connection in all cases. There is
                    # no harm in redundantly calling close.
                    self._fp.close()
                    flush_decoder = True
                    if self.enforce_content_length and self.length_remaining not in (
                        0,
                        None,
                    ):
                        # This is an edge case that httplib failed to cover due
                        # to concerns of backward compatibility. We're
                        # addressing it here to make sure IncompleteRead is
                        # raised during streaming, so all calls with incorrect
                        # Content-Length are caught.
>                       raise IncompleteRead(self._fp_bytes_read, self.length_remaining)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:593: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f7093cf7450>
type = <class 'ConnectionResetError'>
value = ConnectionResetError(104, 'Connection reset by peer')
traceback = <traceback object at 0x7f7093ced780>

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                raise RuntimeError("generator didn't stop")
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = type()
            try:
>               self.gen.throw(type, value, traceback)

/root/anaconda3/envs/benchmark/lib/python3.7/contextlib.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f7093976e10>

    @contextmanager
    def _error_catcher(self):
        """
        Catch low-level python exceptions, instead re-raising urllib3
        variants, so that low-level exceptions are not leaked in the
        high-level api.
    
        On exit, release the connection back to the pool.
        """
        clean_exit = False
    
        try:
            try:
                yield
    
            except SocketTimeout:
                # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
                # there is yet no clean way to get at it from this context.
                raise ReadTimeoutError(self._pool, None, "Read timed out.")
    
            except BaseSSLError as e:
                # FIXME: Is there a better way to differentiate between SSLErrors?
                if "read operation timed out" not in str(e):
                    # SSL errors related to framing/MAC get wrapped and reraised here
                    raise SSLError(e)
    
                raise ReadTimeoutError(self._pool, None, "Read timed out.")
    
            except (HTTPException, SocketError) as e:
                # This includes IncompleteRead.
>               raise ProtocolError("Connection broken: %r" % e, e)
E               urllib3.exceptions.ProtocolError: ("Connection broken: ConnectionResetError(104, 'Connection reset by peer')", ConnectionResetError(104, 'Connection reset by peer'))

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:461: ProtocolError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_inpaint.StableDiffusionInpaintPipelineNightlyTests testMethod=test_inpaint_dpm>

    def test_inpaint_dpm(self):
        sd_pipe = StableDiffusionInpaintPipeline.from_pretrained(
            'runwayml/stable-diffusion-inpainting')
        sd_pipe.scheduler = DPMSolverMultistepScheduler.from_config(sd_pipe
            .scheduler.config)
        sd_pipe
        sd_pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
        inputs['num_inference_steps'] = 30
        image = sd_pipe(**inputs).images[0]
        expected_image = load_numpy(
>           'https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_inpaint/stable_diffusion_inpaint_dpm_multi.npy'
            )

tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py:287: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/utils/testing_utils.py:197: in load_numpy
    response = requests.get(arry)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/api.py:73: in get
    return request("get", url, params=params, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/api.py:59: in request
    return session.request(method=method, url=url, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:587: in request
    resp = self.send(prep, **send_kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:723: in send
    history = [resp for resp in gen]
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:723: in <listcomp>
    history = [resp for resp in gen]
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:274: in resolve_redirects
    **adapter_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:745: in send
    r.content
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/models.py:899: in content
    self._content = b"".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def generate():
        # Special case for urllib3.
        if hasattr(self.raw, "stream"):
            try:
                yield from self.raw.stream(chunk_size, decode_content=True)
            except ProtocolError as e:
>               raise ChunkedEncodingError(e)
E               requests.exceptions.ChunkedEncodingError: ("Connection broken: ConnectionResetError(104, 'Connection reset by peer')", ConnectionResetError(104, 'Connection reset by peer'))

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/models.py:818: ChunkedEncodingError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:25:08,905] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-inpainting/snapshots/caac1048f28756b68042add4670bec6f4ae314f8/text_encoder/config.json[0m
[32m[2023-03-15 14:25:08,906] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.22.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:25:10,429] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-inpainting/snapshots/caac1048f28756b68042add4670bec6f4ae314f8/safety_checker/config.json[0m
[32m[2023-03-15 14:25:10,430] [    INFO][0m - Model config CLIPVisionConfig {
  "attention_dropout": 0.0,
  "dropout": 0.0,
  "hidden_act": "quick_gelu",
  "hidden_size": 1024,
  "image_size": 224,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "model_type": "clip_vision_model",
  "num_attention_heads": 16,
  "num_channels": 3,
  "num_hidden_layers": 24,
  "paddlenlp_version": null,
  "patch_size": 14,
  "projection_dim": 768,
  "return_dict": true,
  "tf_legacy_loss": false,
  "torch_dtype": null,
  "torchscript": false,
  "transformers_version": "4.22.0.dev0",
  "use_bfloat16": false
}
[0m
[32m[2023-03-15 14:25:22,269] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-inpainting/snapshots/caac1048f28756b68042add4670bec6f4ae314f8/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--runwayml--stable-diffusion-inpainting/snapshots/caac1048f28756b68042add4670bec6f4ae314f8/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:25:22,269] [    INFO][0m - size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.[0m
[32m[2023-03-15 14:25:22,269] [    INFO][0m - crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.[0m
[32m[2023-03-15 14:25:22,270] [    INFO][0m - Image processor CLIPFeatureExtractor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPFeatureExtractor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
_ StableDiffusionInstructPix2PixPipelineSlowTests.test_stable_diffusion_pix2pix_ddim _

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_instruction_pix2pix.StableDiffusionInstructPix2PixPipelineSlowTests testMethod=test_stable_diffusion_pix2pix_ddim>

    def test_stable_diffusion_pix2pix_ddim(self):
        pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
            'timbrooks/instruct-pix2pix', safety_checker=None)
        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
        pipe.set_progress_bar_config(disable=None)
        pipe.enable_attention_slicing()
        inputs = self.get_inputs()
        image = pipe(**inputs).images
        image_slice = image[0, -3:, -3:, -1].flatten()
        assert image.shape == (1, 512, 512, 3)
        expected_slice = np.array([0.3828, 0.3834, 0.3818, 0.3792, 0.3865,
            0.3752, 0.3792, 0.3847, 0.3753])
>       assert np.abs(expected_slice - image_slice).max() < 0.001
E       AssertionError: assert 0.1490580341339111 < 0.001
E        +  where 0.1490580341339111 = <built-in method max of numpy.ndarray object at 0x7f6e07f12bd0>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6e07f12bd0> = array([0.13231174, 0.13516768, 0.13146001, 0.1384025 , 0.12816501,\n       0.14463303, 0.14276854, 0.1274842 , 0.14905803]).max
E        +      where array([0.13231174, 0.13516768, 0.13146001, 0.1384025 , 0.12816501,\n       0.14463303, 0.14276854, 0.1274842 , 0.14905803]) = <ufunc 'absolute'>((array([0.3828, 0.3834, 0.3818, 0.3792, 0.3865, 0.3752, 0.3792, 0.3847,\n       0.3753]) - array([0.51511174, 0.5185677 , 0.51326   , 0.5176025 , 0.514665  ,\n       0.519833  , 0.52196854, 0.5121842 , 0.52435803], dtype=float32)))
E        +        where <ufunc 'absolute'> = np.abs

tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py:222: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:46:19,105] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/text_encoder/config.json[0m
[32m[2023-03-15 14:46:19,106] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:46:36,600] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:46:36,601] [    INFO][0m - Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
_ StableDiffusionInstructPix2PixPipelineSlowTests.test_stable_diffusion_pix2pix_default _

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_instruction_pix2pix.StableDiffusionInstructPix2PixPipelineSlowTests testMethod=test_stable_diffusion_pix2pix_default>

    def test_stable_diffusion_pix2pix_default(self):
        pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
            'timbrooks/instruct-pix2pix', safety_checker=None)
        pipe.set_progress_bar_config(disable=None)
        pipe.enable_attention_slicing()
        inputs = self.get_inputs()
        image = pipe(**inputs).images
        image_slice = image[0, -3:, -3:, -1].flatten()
        assert image.shape == (1, 512, 512, 3)
        expected_slice = np.array([0.5902, 0.6015, 0.6027, 0.5983, 0.6092,
            0.6061, 0.5765, 0.5785, 0.5555])
>       assert np.abs(expected_slice - image_slice).max() < 0.001
E       AssertionError: assert 0.27630558156967167 < 0.001
E        +  where 0.27630558156967167 = <built-in method max of numpy.ndarray object at 0x7f6e0910af30>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6e0910af30> = array([0.26881837, 0.27630558, 0.27142752, 0.27216547, 0.27602202,\n       0.27105001, 0.25252372, 0.24885574, 0.23494157]).max
E        +      where array([0.26881837, 0.27630558, 0.27142752, 0.27216547, 0.27602202,\n       0.27105001, 0.25252372, 0.24885574, 0.23494157]) = <ufunc 'absolute'>((array([0.5902, 0.6015, 0.6027, 0.5983, 0.6092, 0.6061, 0.5765, 0.5785,\n       0.5555]) - array([0.32138163, 0.32519442, 0.33127248, 0.32613453, 0.33317798,\n       0.33505   , 0.32397628, 0.32964426, 0.32055843], dtype=float32)))
E        +        where <ufunc 'absolute'> = np.abs

tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py:193: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:46:43,222] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/text_encoder/config.json[0m
[32m[2023-03-15 14:46:43,223] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:46:51,329] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:46:51,330] [    INFO][0m - Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
_ StableDiffusionInstructPix2PixPipelineSlowTests.test_stable_diffusion_pix2pix_intermediate_state _

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_instruction_pix2pix.StableDiffusionInstructPix2PixPipelineSlowTests testMethod=test_stable_diffusion_pix2pix_intermediate_state>

    def test_stable_diffusion_pix2pix_intermediate_state(self):
        number_of_steps = 0
        def callback_fn(step: int, timestep: int, latents: paddle.Tensor
            ) ->None:
            callback_fn.has_been_called = True
            nonlocal number_of_steps
            number_of_steps += 1
            if step == 1:
                latents = latents.detach().cpu().numpy()
                assert latents.shape == (1, 4, 64, 64)
                latents_slice = latents[0, -3:, -3:, -1]
                expected_slice = np.array([-0.2463, -0.4644, -0.9756,
                    1.5176, 1.4414, 0.7866, 0.9897, 0.8521, 0.7983])
                assert np.abs(latents_slice.flatten() - expected_slice).max(
                    ) < 0.05
            elif step == 2:
                latents = latents.detach().cpu().numpy()
                assert latents.shape == (1, 4, 64, 64)
                latents_slice = latents[0, -3:, -3:, -1]
                expected_slice = np.array([-0.2644, -0.4626, -0.9653,
                    1.5176, 1.4551, 0.7686, 0.9805, 0.8452, 0.8115])
                assert np.abs(latents_slice.flatten() - expected_slice).max(
                    ) < 0.05
        callback_fn.has_been_called = False
        pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
            'timbrooks/instruct-pix2pix', safety_checker=None, paddle_dtype=paddle.float16)
        pipe.set_progress_bar_config(disable=None)
        pipe.enable_attention_slicing()
        inputs = self.get_inputs()
>       pipe(**inputs, callback=callback_fn, callback_steps=1)

tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py:253: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/decorator.py:232: in fun
    return caller(func, *(extras + args), **kw)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/base.py:396: in _decorate_function
    return func(*args, **kwargs)
ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py:358: in __call__
    callback(i, t, latents)
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

step = 1
timestep = Tensor(shape=[1], dtype=float32, place=Place(gpu:0), stop_gradient=True,
       [499.50000000])
latents = array([[[[-0.4263 , -0.1757 ,  0.1887 , ..., -0.465  , -0.1914 ,
          -0.2627 ],
         [-0.82   , -1.122  , -0...      1.556  ],
         [ 0.01465,  0.4211 , -0.02144, ...,  1.495  , -0.2045 ,
           1.227  ]]]], dtype=float16)

    def callback_fn(step: int, timestep: int, latents: paddle.Tensor
        ) ->None:
        callback_fn.has_been_called = True
        nonlocal number_of_steps
        number_of_steps += 1
        if step == 1:
            latents = latents.detach().cpu().numpy()
            assert latents.shape == (1, 4, 64, 64)
            latents_slice = latents[0, -3:, -3:, -1]
            expected_slice = np.array([-0.2463, -0.4644, -0.9756,
                1.5176, 1.4414, 0.7866, 0.9897, 0.8521, 0.7983])
>           assert np.abs(latents_slice.flatten() - expected_slice).max(
E           AssertionError: assert 0.7035640625 < 0.05
E            +  where 0.7035640625 = <built-in method max of numpy.ndarray object at 0x7f6df1746e10>()
E            +    where <built-in method max of numpy.ndarray object at 0x7f6df1746e10> = array([0.46414922, 0.43501406, 0.41111875, 0.30759531, 0.52246719,\n       0.59035313, 0.16850312, 0.70356406, 0.4282625 ]).max
E            +      where array([0.46414922, 0.43501406, 0.41111875, 0.30759531, 0.52246719,\n       0.59035313, 0.16850312, 0.70356406, 0.4282625 ]) = <ufunc 'absolute'>((array([-0.7104, -0.8994, -1.387 ,  1.825 ,  1.964 ,  1.377 ,  1.158 ,\n        1.556 ,  1.227 ], dtype=float16) - array([-0.2463, -0.4644, -0.9756,  1.5176,  1.4414,  0.7866,  0.9897,\n        0.8521,  0.7983])))
E            +        where <ufunc 'absolute'> = np.abs
E            +        and   array([-0.7104, -0.8994, -1.387 ,  1.825 ,  1.964 ,  1.377 ,  1.158 ,\n        1.556 ,  1.227 ], dtype=float16) = <built-in method flatten of numpy.ndarray object at 0x7f6df17462d0>()
E            +          where <built-in method flatten of numpy.ndarray object at 0x7f6df17462d0> = array([[-0.7104, -0.8994, -1.387 ],\n       [ 1.825 ,  1.964 ,  1.377 ],\n       [ 1.158 ,  1.556 ,  1.227 ]], dtype=float16).flatten

tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py:237: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:47:02,208] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/text_encoder/config.json[0m
[32m[2023-03-15 14:47:02,209] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:47:11,821] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:47:11,822] [    INFO][0m - Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
_ StableDiffusionInstructPix2PixPipelineSlowTests.test_stable_diffusion_pix2pix_k_lms _

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_instruction_pix2pix.StableDiffusionInstructPix2PixPipelineSlowTests testMethod=test_stable_diffusion_pix2pix_k_lms>

    def test_stable_diffusion_pix2pix_k_lms(self):
        pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(
            'timbrooks/instruct-pix2pix', safety_checker=None)
        pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config
            )
        pipe.set_progress_bar_config(disable=None)
        pipe.enable_attention_slicing()
        inputs = self.get_inputs()
        image = pipe(**inputs).images
        image_slice = image[0, -3:, -3:, -1].flatten()
        assert image.shape == (1, 512, 512, 3)
        expected_slice = np.array([0.6578, 0.6817, 0.6972, 0.6761, 0.6856,
            0.6916, 0.6428, 0.6516, 0.6301])
>       assert np.abs(expected_slice - image_slice).max() < 0.001
E       AssertionError: assert 0.29746887059211735 < 0.001
E        +  where 0.29746887059211735 = <built-in method max of numpy.ndarray object at 0x7f6e0b30ecf0>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6e0b30ecf0> = array([0.26845516, 0.28870661, 0.29746887, 0.25649719, 0.26173567,\n       0.26086176, 0.21602919, 0.21986326, 0.21113734]).max
E        +      where array([0.26845516, 0.28870661, 0.29746887, 0.25649719, 0.26173567,\n       0.26086176, 0.21602919, 0.21986326, 0.21113734]) = <ufunc 'absolute'>((array([0.6578, 0.6817, 0.6972, 0.6761, 0.6856, 0.6916, 0.6428, 0.6516,\n       0.6301]) - array([0.38934484, 0.3929934 , 0.39973113, 0.4196028 , 0.42386433,\n       0.43073824, 0.4267708 , 0.43173674, 0.41896266], dtype=float32)))
E        +        where <ufunc 'absolute'> = np.abs

tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py:208: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:47:25,975] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/text_encoder/config.json[0m
[32m[2023-03-15 14:47:25,976] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:47:33,488] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:47:33,489] [    INFO][0m - Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
_ StableDiffusionInstructPix2PixPipelineSlowTests.test_stable_diffusion_pix2pix_pipeline_multiple_of_8 _

self = <tests.pipelines.stable_diffusion.test_stable_diffusion_instruction_pix2pix.StableDiffusionInstructPix2PixPipelineSlowTests testMethod=test_stable_diffusion_pix2pix_pipeline_multiple_of_8>

    def test_stable_diffusion_pix2pix_pipeline_multiple_of_8(self):
        inputs = self.get_inputs()
        inputs['image'] = inputs['image'].resize((504, 504))
        model_id = 'timbrooks/instruct-pix2pix'
        pipe = StableDiffusionInstructPix2PixPipeline.from_pretrained(model_id,
            safety_checker=None)
        pipe.set_progress_bar_config(disable=None)
        pipe.enable_attention_slicing()
        output = pipe(**inputs)
        image = output.images[0]
        image_slice = image[255:258, 383:386, -1]
        assert image.shape == (504, 504, 3)
        expected_slice = np.array([0.2726, 0.2529, 0.2664, 0.2655, 0.2641,
            0.2642, 0.2591, 0.2649, 0.259])
>       assert np.abs(image_slice.flatten() - expected_slice).max() < 0.005
E       AssertionError: assert 0.08922699580192567 < 0.005
E        +  where 0.08922699580192567 = <built-in method max of numpy.ndarray object at 0x7f6e0b2d1c30>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6e0b2d1c30> = array([0.089227  , 0.04831436, 0.0235336 , 0.08304136, 0.04399462,\n       0.00662288, 0.06229801, 0.0463855 , 0.01030627]).max
E        +      where array([0.089227  , 0.04831436, 0.0235336 , 0.08304136, 0.04399462,\n       0.00662288, 0.06229801, 0.0463855 , 0.01030627]) = <ufunc 'absolute'>((array([0.183373  , 0.20458564, 0.2428664 , 0.18245864, 0.22010538,\n       0.25757712, 0.19680199, 0.2185145 , 0.24869373], dtype=float32) - array([0.2726, 0.2529, 0.2664, 0.2655, 0.2641, 0.2642, 0.2591, 0.2649,\n       0.259 ])))
E        +        where <ufunc 'absolute'> = np.abs
E        +        and   array([0.183373  , 0.20458564, 0.2428664 , 0.18245864, 0.22010538,\n       0.25757712, 0.19680199, 0.2185145 , 0.24869373], dtype=float32) = <built-in method flatten of numpy.ndarray object at 0x7f6e0b2d16f0>()
E        +          where <built-in method flatten of numpy.ndarray object at 0x7f6e0b2d16f0> = array([[0.183373  , 0.20458564, 0.2428664 ],\n       [0.18245864, 0.22010538, 0.25757712],\n       [0.19680199, 0.2185145 , 0.24869373]], dtype=float32).flatten

tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py:273: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:48:35,701] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/text_encoder/config.json[0m
[32m[2023-03-15 14:48:35,707] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 768,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:48:43,110] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--timbrooks--instruct-pix2pix/snapshots/93224554bd65f19b6f0c99cbcce3a4ac59bb6382/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:48:43,110] [    INFO][0m - Image processor CLIPImageProcessor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPImageProcessor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
_________ StableDiffusion2PipelineFastTests.test_stable_diffusion_ddim _________

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion.StableDiffusion2PipelineFastTests testMethod=test_stable_diffusion_ddim>

    def test_stable_diffusion_ddim(self):
        components = self.get_dummy_components()
        sd_pipe = StableDiffusionPipeline(**components)
        sd_pipe.set_progress_bar_config(disable=None)
        inputs = self.get_dummy_inputs()
        image = sd_pipe(**inputs).images
        image_slice = image[0, -3:, -3:, -1]
        assert image.shape == (1, 64, 64, 3)
        expected_slice = np.array([0.5649, 0.6022, 0.4804, 0.527, 0.5585, 0.4643, 0.5159, 0.4963, 0.4793])
>       assert np.abs(image_slice.flatten() - expected_slice).max() < 0.01
E       AssertionError: assert 0.41847285480499263 < 0.01
E        +  where 0.41847285480499263 = <built-in method max of numpy.ndarray object at 0x7f6ded3b1630>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6ded3b1630> = array([0.38666945, 0.41847285, 0.14012063, 0.26553958, 0.33021084,\n       0.01973476, 0.30050544, 0.33982607, 0.12451194]).max
E        +      where array([0.38666945, 0.41847285, 0.14012063, 0.26553958, 0.33021084,\n       0.01973476, 0.30050544, 0.33982607, 0.12451194]) = <ufunc 'absolute'>((array([0.17823055, 0.18372715, 0.34027937, 0.26146042, 0.22828916,\n       0.44456524, 0.21539456, 0.15647393, 0.35478806], dtype=float32) - array([0.5649, 0.6022, 0.4804, 0.527 , 0.5585, 0.4643, 0.5159, 0.4963,\n       0.4793])))
E        +        where <ufunc 'absolute'> = np.abs
E        +        and   array([0.17823055, 0.18372715, 0.34027937, 0.26146042, 0.22828916,\n       0.44456524, 0.21539456, 0.15647393, 0.35478806], dtype=float32) = <built-in method flatten of numpy.ndarray object at 0x7f6e16023d50>()
E        +          where <built-in method flatten of numpy.ndarray object at 0x7f6e16023d50> = array([[0.17823055, 0.18372715, 0.34027937],\n       [0.26146042, 0.22828916, 0.44456524],\n       [0.21539456, 0.15647393, 0.35478806]], dtype=float32).flatten

tests/pipelines/stable_diffusion_2/test_stable_diffusion.py:123: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:53:58,835] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/vocab.json[0m
[32m[2023-03-15 14:53:58,835] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/merges.txt[0m
[32m[2023-03-15 14:53:58,835] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json and saved to /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip[0m
[33m[2023-03-15 14:53:58,866] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json> not exist[0m
[32m[2023-03-15 14:53:58,867] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/special_tokens_map.json[0m
[32m[2023-03-15 14:53:58,867] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/tokenizer_config.json[0m
You have disabled the safety checker for <class 'ppdiffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. PaddleNLP team, diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
_______ StableDiffusion2PipelineFastTests.test_stable_diffusion_k_euler ________

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion.StableDiffusion2PipelineFastTests testMethod=test_stable_diffusion_k_euler>

    def test_stable_diffusion_k_euler(self):
        components = self.get_dummy_components()
        components["scheduler"] = EulerDiscreteScheduler.from_config(components["scheduler"].config)
        sd_pipe = StableDiffusionPipeline(**components)
        sd_pipe.set_progress_bar_config(disable=None)
        inputs = self.get_dummy_inputs()
        image = sd_pipe(**inputs).images
        image_slice = image[0, -3:, -3:, -1]
        assert image.shape == (1, 64, 64, 3)
        expected_slice = np.array([0.4717, 0.5376, 0.4568, 0.5225, 0.5734, 0.4797, 0.5467, 0.5074, 0.5043])
>       assert np.abs(image_slice.flatten() - expected_slice).max() < 0.01
E       AssertionError: assert 0.37830259332656857 < 0.01
E        +  where 0.37830259332656857 = <built-in method max of numpy.ndarray object at 0x7f6df1bc8450>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6df1bc8450> = array([0.2717581 , 0.37830259, 0.11212178, 0.25858482, 0.34364451,\n       0.03004351, 0.31254631, 0.3463861 , 0.16686237]).max
E        +      where array([0.2717581 , 0.37830259, 0.11212178, 0.25858482, 0.34364451,\n       0.03004351, 0.31254631, 0.3463861 , 0.16686237]) = <ufunc 'absolute'>((array([0.1999419 , 0.1592974 , 0.34467822, 0.26391518, 0.22975549,\n       0.4496565 , 0.23415369, 0.1610139 , 0.33743763], dtype=float32) - array([0.4717, 0.5376, 0.4568, 0.5225, 0.5734, 0.4797, 0.5467, 0.5074,\n       0.5043])))
E        +        where <ufunc 'absolute'> = np.abs
E        +        and   array([0.1999419 , 0.1592974 , 0.34467822, 0.26391518, 0.22975549,\n       0.4496565 , 0.23415369, 0.1610139 , 0.33743763], dtype=float32) = <built-in method flatten of numpy.ndarray object at 0x7f6e03c53510>()
E        +          where <built-in method flatten of numpy.ndarray object at 0x7f6e03c53510> = array([[0.1999419 , 0.1592974 , 0.34467822],\n       [0.26391518, 0.22975549, 0.4496565 ],\n       [0.23415369, 0.1610139 , 0.33743763]], dtype=float32).flatten

tests/pipelines/stable_diffusion_2/test_stable_diffusion.py:171: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:53:59,361] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/vocab.json[0m
[32m[2023-03-15 14:53:59,361] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/merges.txt[0m
[32m[2023-03-15 14:53:59,361] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json and saved to /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip[0m
[33m[2023-03-15 14:53:59,394] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json> not exist[0m
[32m[2023-03-15 14:53:59,394] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/special_tokens_map.json[0m
[32m[2023-03-15 14:53:59,394] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/tokenizer_config.json[0m
You have disabled the safety checker for <class 'ppdiffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. PaddleNLP team, diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
__ StableDiffusion2PipelineFastTests.test_stable_diffusion_k_euler_ancestral ___

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion.StableDiffusion2PipelineFastTests testMethod=test_stable_diffusion_k_euler_ancestral>

    def test_stable_diffusion_k_euler_ancestral(self):
        components = self.get_dummy_components()
        components["scheduler"] = EulerAncestralDiscreteScheduler.from_config(components["scheduler"].config)
        sd_pipe = StableDiffusionPipeline(**components)
        sd_pipe.set_progress_bar_config(disable=None)
        inputs = self.get_dummy_inputs()
        image = sd_pipe(**inputs).images
        image_slice = image[0, -3:, -3:, -1]
        assert image.shape == (1, 64, 64, 3)
        expected_slice = np.array([0.4715, 0.5376, 0.4569, 0.5224, 0.5734, 0.4797, 0.5465, 0.5074, 0.5046])
>       assert np.abs(image_slice.flatten() - expected_slice).max() < 0.01
E       AssertionError: assert 0.3787946892738342 < 0.01
E        +  where 0.3787946892738342 = <built-in method max of numpy.ndarray object at 0x7f6df105ced0>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6df105ced0> = array([0.27133065, 0.37879469, 0.11258173, 0.25811324, 0.34436081,\n       0.03025246, 0.31169811, 0.34645149, 0.16719909]).max
E        +      where array([0.27133065, 0.37879469, 0.11258173, 0.25811324, 0.34436081,\n       0.03025246, 0.31169811, 0.34645149, 0.16719909]) = <ufunc 'absolute'>((array([0.20016935, 0.15880531, 0.34431827, 0.26428676, 0.22903919,\n       0.44944754, 0.23480189, 0.16094851, 0.3374009 ], dtype=float32) - array([0.4715, 0.5376, 0.4569, 0.5224, 0.5734, 0.4797, 0.5465, 0.5074,\n       0.5046])))
E        +        where <ufunc 'absolute'> = np.abs
E        +        and   array([0.20016935, 0.15880531, 0.34431827, 0.26428676, 0.22903919,\n       0.44944754, 0.23480189, 0.16094851, 0.3374009 ], dtype=float32) = <built-in method flatten of numpy.ndarray object at 0x7f6e160235d0>()
E        +          where <built-in method flatten of numpy.ndarray object at 0x7f6e160235d0> = array([[0.20016935, 0.15880531, 0.34431827],\n       [0.26428676, 0.22903919, 0.44944754],\n       [0.23480189, 0.16094851, 0.3374009 ]], dtype=float32).flatten

tests/pipelines/stable_diffusion_2/test_stable_diffusion.py:159: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:53:59,878] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/vocab.json[0m
[32m[2023-03-15 14:53:59,878] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/merges.txt[0m
[32m[2023-03-15 14:53:59,878] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json and saved to /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip[0m
[33m[2023-03-15 14:53:59,916] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json> not exist[0m
[32m[2023-03-15 14:53:59,917] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/special_tokens_map.json[0m
[32m[2023-03-15 14:53:59,917] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/tokenizer_config.json[0m
You have disabled the safety checker for <class 'ppdiffusers.pipelines.stable_diffusion.pipeline_stable_diffusion.StableDiffusionPipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. PaddleNLP team, diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .
_ StableDiffusion2PipelineNightlyTests.test_stable_diffusion_2_0_default_ddim __

self = <urllib3.response.HTTPResponse object at 0x7f6dfdc954d0>

    @contextmanager
    def _error_catcher(self):
        """
        Catch low-level python exceptions, instead re-raising urllib3
        variants, so that low-level exceptions are not leaked in the
        high-level api.
    
        On exit, release the connection back to the pool.
        """
        clean_exit = False
    
        try:
            try:
>               yield

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:444: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6dfdc954d0>, amt = 10240
decode_content = True, cache_content = False

    def read(self, amt=None, decode_content=None, cache_content=False):
        """
        Similar to :meth:`http.client.HTTPResponse.read`, but with two additional
        parameters: ``decode_content`` and ``cache_content``.
    
        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param cache_content:
            If True, will save the returned data such that the same result is
            returned despite of the state of the underlying file object. This
            is useful if you want the ``.data`` property to continue working
            after having ``.read()`` the file object. (Overridden if ``amt`` is
            set.)
        """
        self._init_decoder()
        if decode_content is None:
            decode_content = self.decode_content
    
        if self._fp is None:
            return
    
        flush_decoder = False
        fp_closed = getattr(self._fp, "closed", False)
    
        with self._error_catcher():
>           data = self._fp_read(amt) if not fp_closed else b""

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:567: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6dfdc954d0>, amt = 10240

    def _fp_read(self, amt):
        """
        Read a response with the thought that reading the number of bytes
        larger than can fit in a 32-bit int at a time via SSL in some
        known cases leads to an overflow error that has to be prevented
        if `amt` or `self.length_remaining` indicate that a problem may
        happen.
    
        The known cases:
          * 3.8 <= CPython < 3.9.7 because of a bug
            https://github.com/urllib3/urllib3/issues/2513#issuecomment-1152559900.
          * urllib3 injected with pyOpenSSL-backed SSL-support.
          * CPython < 3.10 only when `amt` does not fit 32-bit int.
        """
        assert self._fp
        c_int_max = 2 ** 31 - 1
        if (
            (
                (amt and amt > c_int_max)
                or (self.length_remaining and self.length_remaining > c_int_max)
            )
            and not util.IS_SECURETRANSPORT
            and (util.IS_PYOPENSSL or sys.version_info < (3, 10))
        ):
            buffer = io.BytesIO()
            # Besides `max_chunk_amt` being a maximum chunk size, it
            # affects memory overhead of reading a response by this
            # method in CPython.
            # `c_int_max` equal to 2 GiB - 1 byte is the actual maximum
            # chunk size that does not lead to an overflow error, but
            # 256 MiB is a compromise.
            max_chunk_amt = 2 ** 28
            while amt is None or amt != 0:
                if amt is not None:
                    chunk_amt = min(amt, max_chunk_amt)
                    amt -= chunk_amt
                else:
                    chunk_amt = max_chunk_amt
                data = self._fp.read(chunk_amt)
                if not data:
                    break
                buffer.write(data)
                del data  # to reduce peak memory usage by `max_chunk_amt`.
            return buffer.getvalue()
        else:
            # StringIO doesn't like amt=None
>           return self._fp.read(amt) if amt is not None else self._fp.read()

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:533: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <http.client.HTTPResponse object at 0x7f6dfdc955d0>, amt = 10240

    def read(self, amt=None):
        if self.fp is None:
            return b""
    
        if self._method == "HEAD":
            self._close_conn()
            return b""
    
        if amt is not None:
            # Amount is given, implement using readinto
            b = bytearray(amt)
>           n = self.readinto(b)

/root/anaconda3/envs/benchmark/lib/python3.7/http/client.py:465: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <http.client.HTTPResponse object at 0x7f6dfdc955d0>
b = bytearray(b'\xd2\x87\x8f>\xb8\n\xfd>Z,\xc1>b%\x8f>\x9f\x0f\xf8>\x18\xe8\xbc>|`\x85>\xdfs\xfb>\x01\x87\xc5>\x06|\x88>\x...0\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00')

    def readinto(self, b):
        """Read up to len(b) bytes into bytearray b and return the number
        of bytes read.
        """
    
        if self.fp is None:
            return 0
    
        if self._method == "HEAD":
            self._close_conn()
            return 0
    
        if self.chunked:
            return self._readinto_chunked(b)
    
        if self.length is not None:
            if len(b) > self.length:
                # clip the read to the "end of response"
                b = memoryview(b)[0:self.length]
    
        # we do not use _safe_read() here because this may be a .will_close
        # connection, and the user is reading more bytes than will be provided
        # (for example, reading in 1k chunks)
>       n = self.fp.readinto(b)

/root/anaconda3/envs/benchmark/lib/python3.7/http/client.py:509: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <socket.SocketIO object at 0x7f6dfdc95450>
b = <memory at 0x7f7093c9fd50>

    def readinto(self, b):
        """Read up to len(b) bytes into the writable buffer *b* and return
        the number of bytes read.  If the socket is non-blocking and no bytes
        are available, None is returned.
    
        If *b* is non-empty, a 0 return value indicates that the connection
        was shutdown at the other end.
        """
        self._checkClosed()
        self._checkReadable()
        if self._timeout_occurred:
            raise OSError("cannot read from timed out object")
        while True:
            try:
>               return self._sock.recv_into(b)

/root/anaconda3/envs/benchmark/lib/python3.7/socket.py:589: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>
buffer = <memory at 0x7f7093c9fd50>, nbytes = 8192, flags = 0

    def recv_into(self, buffer, nbytes=None, flags=0):
        self._checkClosed()
        if buffer and (nbytes is None):
            nbytes = len(buffer)
        elif nbytes is None:
            nbytes = 1024
        if self._sslobj is not None:
            if flags != 0:
                raise ValueError(
                  "non-zero flags not allowed in calls to recv_into() on %s" %
                  self.__class__)
>           return self.read(nbytes, buffer)

/root/anaconda3/envs/benchmark/lib/python3.7/ssl.py:1071: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <ssl.SSLSocket [closed] fd=-1, family=AddressFamily.AF_INET, type=SocketKind.SOCK_STREAM, proto=6>
len = 8192, buffer = <memory at 0x7f7093c9fd50>

    def read(self, len=1024, buffer=None):
        """Read up to LEN bytes and return them.
        Return zero-length string on EOF."""
    
        self._checkClosed()
        if self._sslobj is None:
            raise ValueError("Read on closed or unwrapped SSL socket.")
        try:
            if buffer is not None:
>               return self._sslobj.read(len, buffer)
E               ConnectionResetError: [Errno 104] Connection reset by peer

/root/anaconda3/envs/benchmark/lib/python3.7/ssl.py:929: ConnectionResetError

During handling of the above exception, another exception occurred:

    def generate():
        # Special case for urllib3.
        if hasattr(self.raw, "stream"):
            try:
>               yield from self.raw.stream(chunk_size, decode_content=True)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/models.py:816: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6dfdc954d0>, amt = 10240
decode_content = True

    def stream(self, amt=2 ** 16, decode_content=None):
        """
        A generator wrapper for the read() method. A call will block until
        ``amt`` bytes have been read from the connection or until the
        connection is closed.
    
        :param amt:
            How much of the content to read. The generator will return up to
            much data per iteration, but may return less. This is particularly
            likely when using compressed data. However, the empty string will
            never be returned.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
        """
        if self.chunked and self.supports_chunked_reads():
            for line in self.read_chunked(amt, decode_content=decode_content):
                yield line
        else:
            while not is_fp_closed(self._fp):
>               data = self.read(amt=amt, decode_content=decode_content)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:628: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6dfdc954d0>, amt = 10240
decode_content = True, cache_content = False

    def read(self, amt=None, decode_content=None, cache_content=False):
        """
        Similar to :meth:`http.client.HTTPResponse.read`, but with two additional
        parameters: ``decode_content`` and ``cache_content``.
    
        :param amt:
            How much of the content to read. If specified, caching is skipped
            because it doesn't make sense to cache partial content as the full
            response.
    
        :param decode_content:
            If True, will attempt to decode the body based on the
            'content-encoding' header.
    
        :param cache_content:
            If True, will save the returned data such that the same result is
            returned despite of the state of the underlying file object. This
            is useful if you want the ``.data`` property to continue working
            after having ``.read()`` the file object. (Overridden if ``amt`` is
            set.)
        """
        self._init_decoder()
        if decode_content is None:
            decode_content = self.decode_content
    
        if self._fp is None:
            return
    
        flush_decoder = False
        fp_closed = getattr(self._fp, "closed", False)
    
        with self._error_catcher():
            data = self._fp_read(amt) if not fp_closed else b""
            if amt is None:
                flush_decoder = True
            else:
                cache_content = False
                if (
                    amt != 0 and not data
                ):  # Platform-specific: Buggy versions of Python.
                    # Close the connection when no data is returned
                    #
                    # This is redundant to what httplib/http.client _should_
                    # already do.  However, versions of python released before
                    # December 15, 2012 (http://bugs.python.org/issue16298) do
                    # not properly close the connection in all cases. There is
                    # no harm in redundantly calling close.
                    self._fp.close()
                    flush_decoder = True
                    if self.enforce_content_length and self.length_remaining not in (
                        0,
                        None,
                    ):
                        # This is an edge case that httplib failed to cover due
                        # to concerns of backward compatibility. We're
                        # addressing it here to make sure IncompleteRead is
                        # raised during streaming, so all calls with incorrect
                        # Content-Length are caught.
>                       raise IncompleteRead(self._fp_bytes_read, self.length_remaining)

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:593: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <contextlib._GeneratorContextManager object at 0x7f6dfdc921d0>
type = <class 'ConnectionResetError'>
value = ConnectionResetError(104, 'Connection reset by peer')
traceback = <traceback object at 0x7f6dfdc824b0>

    def __exit__(self, type, value, traceback):
        if type is None:
            try:
                next(self.gen)
            except StopIteration:
                return False
            else:
                raise RuntimeError("generator didn't stop")
        else:
            if value is None:
                # Need to force instantiation so we can reliably
                # tell if we get the same exception back
                value = type()
            try:
>               self.gen.throw(type, value, traceback)

/root/anaconda3/envs/benchmark/lib/python3.7/contextlib.py:130: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = <urllib3.response.HTTPResponse object at 0x7f6dfdc954d0>

    @contextmanager
    def _error_catcher(self):
        """
        Catch low-level python exceptions, instead re-raising urllib3
        variants, so that low-level exceptions are not leaked in the
        high-level api.
    
        On exit, release the connection back to the pool.
        """
        clean_exit = False
    
        try:
            try:
                yield
    
            except SocketTimeout:
                # FIXME: Ideally we'd like to include the url in the ReadTimeoutError but
                # there is yet no clean way to get at it from this context.
                raise ReadTimeoutError(self._pool, None, "Read timed out.")
    
            except BaseSSLError as e:
                # FIXME: Is there a better way to differentiate between SSLErrors?
                if "read operation timed out" not in str(e):
                    # SSL errors related to framing/MAC get wrapped and reraised here
                    raise SSLError(e)
    
                raise ReadTimeoutError(self._pool, None, "Read timed out.")
    
            except (HTTPException, SocketError) as e:
                # This includes IncompleteRead.
>               raise ProtocolError("Connection broken: %r" % e, e)
E               urllib3.exceptions.ProtocolError: ("Connection broken: ConnectionResetError(104, 'Connection reset by peer')", ConnectionResetError(104, 'Connection reset by peer'))

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/urllib3/response.py:461: ProtocolError

During handling of the above exception, another exception occurred:

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion.StableDiffusion2PipelineNightlyTests testMethod=test_stable_diffusion_2_0_default_ddim>

    def test_stable_diffusion_2_0_default_ddim(self):
        sd_pipe = StableDiffusionPipeline.from_pretrained("stabilityai/stable-diffusion-2-base")
        sd_pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
        image = sd_pipe(**inputs).images[0]
        expected_image = load_numpy(
>           "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_2_text2img/stable_diffusion_2_0_base_ddim.npy"
        )

tests/pipelines/stable_diffusion_2/test_stable_diffusion.py:337: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/utils/testing_utils.py:197: in load_numpy
    response = requests.get(arry)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/api.py:73: in get
    return request("get", url, params=params, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/api.py:59: in request
    return session.request(method=method, url=url, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:587: in request
    resp = self.send(prep, **send_kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:723: in send
    history = [resp for resp in gen]
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:723: in <listcomp>
    history = [resp for resp in gen]
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:274: in resolve_redirects
    **adapter_kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/sessions.py:745: in send
    r.content
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/models.py:899: in content
    self._content = b"".join(self.iter_content(CONTENT_CHUNK_SIZE)) or b""
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

    def generate():
        # Special case for urllib3.
        if hasattr(self.raw, "stream"):
            try:
                yield from self.raw.stream(chunk_size, decode_content=True)
            except ProtocolError as e:
>               raise ChunkedEncodingError(e)
E               requests.exceptions.ChunkedEncodingError: ("Connection broken: ConnectionResetError(104, 'Connection reset by peer')", ConnectionResetError(104, 'Connection reset by peer'))

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/requests/models.py:818: ChunkedEncodingError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 14:54:57,437] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-base/snapshots/d28fc8045793886e512c5389771d3b3d560f9575/text_encoder/config.json[0m
[32m[2023-03-15 14:54:57,438] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "/home/suraj_huggingface_co/.cache/huggingface/diffusers/models--fusing--stable-diffusion-v2/snapshots/3282d2bdc378f4afd43edbbb90803779a5249116/text_encoder",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_size": 1024,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 16,
  "num_hidden_layers": 23,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.25.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 14:55:08,190] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-base/snapshots/d28fc8045793886e512c5389771d3b3d560f9575/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-base/snapshots/d28fc8045793886e512c5389771d3b3d560f9575/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 14:55:08,191] [    INFO][0m - size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.[0m
[32m[2023-03-15 14:55:08,191] [    INFO][0m - crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.[0m
[32m[2023-03-15 14:55:08,191] [    INFO][0m - Image processor CLIPFeatureExtractor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPFeatureExtractor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
_ StableDiffusionAttendAndExcitePipelineIntegrationTests.test_attend_and_excite_fp16 _

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_attend_and_excite.StableDiffusionAttendAndExcitePipelineIntegrationTests testMethod=test_attend_and_excite_fp16>

    def test_attend_and_excite_fp16(self):
        generator = paddle.Generator().manual_seed(seed=51)
        pipe = StableDiffusionAttendAndExcitePipeline.from_pretrained(
            "CompVis/stable-diffusion-v1-4", safety_checker=None, paddle_dtype=paddle.float16
        )
    
        prompt = "a painting of an elephant with glasses"
        token_indices = [5, 7]
        image = pipe(
            prompt=prompt,
            token_indices=token_indices,
            guidance_scale=7.5,
            generator=generator,
            num_inference_steps=5,
            max_iter_to_alter=5,
            output_type="numpy",
        ).images[0]
        expected_image = load_numpy(
            "https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/attend-and-excite/elephant_glasses.npy"
        )
>       assert np.abs((expected_image - image).max()) < 0.5
E       AssertionError: assert 1.0 < 0.5
E        +  where 1.0 = <ufunc 'absolute'>(1.0)
E        +    where <ufunc 'absolute'> = np.abs
E        +    and   1.0 = <built-in method max of numpy.ndarray object at 0x7f6e09a54cf0>()
E        +      where <built-in method max of numpy.ndarray object at 0x7f6e09a54cf0> = (array([[[0.93603516, 0.94433594, 0.86816406],\n        [0.9482422 , 0.9638672 , 0.90722656],\n        [0.96484375, 0.97265625, 0.92578125],\n        ...,\n        [0.6689453 , 0.7475586 , 0.8017578 ],\n        [0.66503906, 0.7470703 , 0.80859375],\n        [0.6582031 , 0.72558594, 0.7734375 ]],\n\n       [[0.984375  , 1.        , 0.9423828 ],\n        [0.9785156 , 0.9951172 , 0.9511719 ],\n        [0.9897461 , 1.        , 0.95214844],\n        ...,\n        [0.67333984, 0.7661133 , 0.82714844],\n        [0.66308594, 0.75683594, 0.82910156],\n        [0.65234375, 0.7402344 , 0.8173828 ]],\n\n       [[1.        , 1.        , 0.9814453 ],\n        [0.99121094, 1.        , 0.98339844],\n        [0.9848633 , 1.        , 0.9589844 ],\n        ...,\n        [0.6665039 , 0.76171875, 0.8261719 ],\n        [0.6591797 , 0.7558594 , 0.82910156],\n        [0.6542969 , 0.7504883 , 0.8251953 ]],\n\n       ...,\n\n       [[0.39697266, 0.41308594, 0.40112305],\n        [0.38134766, 0.38989258, 0.39111328],\n        [0.39892578, 0.41064453, 0.41259766],\n        ...,\n        [0.2397461 , 0.2006836 , 0.1706543 ],\n        [0.21362305, 0.17602539, 0.15551758],\n        [0.29345703, 0.25878906, 0.22045898]],\n\n       [[0.38916016, 0.39916992, 0.3930664 ],\n        [0.37304688, 0.37719727, 0.3803711 ],\n        [0.4020996 , 0.41259766, 0.41723633],\n        ...,\n        [0.26123047, 0.21875   , 0.19482422],\n        [0.24926758, 0.21582031, 0.19995117],\n        [0.3076172 , 0.28027344, 0.23583984]],\n\n       [[0.41186523, 0.41577148, 0.40722656],\n        [0.40673828, 0.41137695, 0.40185547],\n        [0.41723633, 0.42578125, 0.41723633],\n        ...,\n        [0.3071289 , 0.28051758, 0.26513672],\n        [0.31201172, 0.28320312, 0.25952148],\n        [0.328125  , 0.28857422, 0.265625  ]]], dtype=float32) - array([[[0.5180664 , 0.5751953 , 0.5605469 ],\n        [0.5463867 , 0.5961914 , 0.59765625],\n        [0.52783203, 0.5917969 , 0.5961914 ],\n        ...,\n        [0.57421875, 0.39257812, 0.36083984],\n        [0.5644531 , 0.40234375, 0.37109375],\n        [0.5463867 , 0.42822266, 0.3918457 ]],\n\n       [[0.52001953, 0.5878906 , 0.5917969 ],\n        [0.5126953 , 0.5839844 , 0.59716797],\n        [0.5131836 , 0.59716797, 0.60791016],\n        ...,\n        [0.5908203 , 0.375     , 0.34277344],\n        [0.60253906, 0.40771484, 0.36450195],\n        [0.56640625, 0.4038086 , 0.3544922 ]],\n\n       [[0.51464844, 0.59277344, 0.6069336 ],\n        [0.49780273, 0.58251953, 0.6035156 ],\n        [0.51464844, 0.6010742 , 0.6196289 ],\n        ...,\n        [0.62597656, 0.39794922, 0.3618164 ],\n        [0.6201172 , 0.4086914 , 0.36621094],\n        [0.58154297, 0.39672852, 0.3486328 ]],\n\n       ...,\n\n       [[0.36450195, 0.26098633, 0.23657227],\n        [0.39331055, 0.2529297 , 0.2607422 ],\n        [0.3894043 , 0.24511719, 0.25708008],\n        ...,\n        [0.3720703 , 0.2602539 , 0.2319336 ],\n        [0.3552246 , 0.24902344, 0.21826172],\n        [0.38500977, 0.29516602, 0.25073242]],\n\n       [[0.36816406, 0.26391602, 0.25170898],\n        [0.38134766, 0.24389648, 0.24829102],\n        [0.40698242, 0.26416016, 0.26904297],\n        ...,\n        [0.39501953, 0.26708984, 0.23999023],\n        [0.35766602, 0.2578125 , 0.22241211],\n        [0.3828125 , 0.29736328, 0.25097656]],\n\n       [[0.3881836 , 0.30664062, 0.2854004 ],\n        [0.4086914 , 0.2944336 , 0.26904297],\n        [0.42578125, 0.2915039 , 0.27124023],\n        ...,\n        [0.40600586, 0.29516602, 0.2541504 ],\n        [0.37646484, 0.28076172, 0.24658203],\n        [0.39379883, 0.30859375, 0.27612305]]], dtype=float32)).max

tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py:158: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:11:29,996] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/text_encoder/config.json[0m
[32m[2023-03-15 15:11:29,997] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "openai/clip-vit-large-patch14",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "quick_gelu",
  "hidden_size": 768,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.21.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 15:11:39,956] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--CompVis--stable-diffusion-v1-4/snapshots/249dd2d739844dea6a0bc7fc27b3c1d014720b28/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 15:11:39,957] [    INFO][0m - size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.[0m
[32m[2023-03-15 15:11:39,957] [    INFO][0m - crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.[0m
[32m[2023-03-15 15:11:39,957] [    INFO][0m - Image processor CLIPFeatureExtractor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPFeatureExtractor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
You have disabled the safety checker for <class 'ppdiffusers.pipelines.stable_diffusion.pipeline_stable_diffusion_attend_and_excite.StableDiffusionAttendAndExcitePipeline'> by passing `safety_checker=None`. Ensure that you abide to the conditions of the Stable Diffusion license and do not expose unfiltered results in services or applications open to the public. PaddleNLP team, diffusers team and Hugging Face strongly recommend to keep the safety filter enabled in all public facing circumstances, disabling it only for use-cases that involve analyzing network behavior or auditing its results. For more information, please have a look at https://github.com/huggingface/diffusers/pull/254 .

  0%|          | 0/5 [00:00<?, ?it/s]
 20%|██        | 1/5 [00:00<00:01,  2.18it/s]
 40%|████      | 2/5 [00:00<00:00,  3.21it/s]
 60%|██████    | 3/5 [00:00<00:00,  3.85it/s]
 80%|████████  | 4/5 [00:01<00:00,  4.26it/s]
100%|██████████| 5/5 [00:01<00:00,  4.86it/s]
100%|██████████| 5/5 [00:01<00:00,  4.11it/s]
_ StableDiffusionDepth2ImgPipelineFastTests.test_stable_diffusion_depth2img_multiple_init_images _

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_depth.StableDiffusionDepth2ImgPipelineFastTests testMethod=test_stable_diffusion_depth2img_multiple_init_images>

    def test_stable_diffusion_depth2img_multiple_init_images(self):
        components = self.get_dummy_components()
        pipe = StableDiffusionDepth2ImgPipeline(**components)
        pipe.set_progress_bar_config(disable=None)
        inputs = self.get_dummy_inputs()
        inputs["prompt"] = [inputs["prompt"]] * 2
        inputs["image"] = 2 * [inputs["image"]]
        image = pipe(**inputs).images
        image_slice = image[-1, -3:, -3:, -1]
        assert image.shape == (2, 32, 32, 3)
        expected_slice = np.array([0.6267, 0.5232, 0.6001, 0.6738, 0.5029, 0.6429, 0.5364, 0.4159, 0.4674])
>       assert np.abs(image_slice.flatten() - expected_slice).max() < 0.001
E       AssertionError: assert 0.49892630109786984 < 0.001
E        +  where 0.49892630109786984 = <built-in method max of numpy.ndarray object at 0x7f6df10bf210>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6df10bf210> = array([0.00109442, 0.13312292, 0.4989263 , 0.28993557, 0.28704083,\n       0.4513279 , 0.01432017, 0.01306028, 0.00771274]).max
E        +      where array([0.00109442, 0.13312292, 0.4989263 , 0.28993557, 0.28704083,\n       0.4513279 , 0.01432017, 0.01306028, 0.00771274]) = <ufunc 'absolute'>((array([0.6256056 , 0.39007708, 0.1011737 , 0.38386443, 0.21585917,\n       0.1915721 , 0.5220798 , 0.40283972, 0.47511274], dtype=float32) - array([0.6267, 0.5232, 0.6001, 0.6738, 0.5029, 0.6429, 0.5364, 0.4159,\n       0.4674])))
E        +        where <ufunc 'absolute'> = np.abs
E        +        and   array([0.6256056 , 0.39007708, 0.1011737 , 0.38386443, 0.21585917,\n       0.1915721 , 0.5220798 , 0.40283972, 0.47511274], dtype=float32) = <built-in method flatten of numpy.ndarray object at 0x7f6df1c0a990>()
E        +          where <built-in method flatten of numpy.ndarray object at 0x7f6df1c0a990> = array([[0.6256056 , 0.39007708, 0.1011737 ],\n       [0.38386443, 0.21585917, 0.1915721 ],\n       [0.5220798 , 0.40283972, 0.47511274]], dtype=float32).flatten

tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py:255: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:12:13,824] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/vocab.json[0m
[32m[2023-03-15 15:12:13,824] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/merges.txt[0m
[32m[2023-03-15 15:12:13,824] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json and saved to /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip[0m
[33m[2023-03-15 15:12:13,854] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json> not exist[0m
[32m[2023-03-15 15:12:13,855] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/special_tokens_map.json[0m
[32m[2023-03-15 15:12:13,855] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/tokenizer_config.json[0m
[32m[2023-03-15 15:12:13,857] [    INFO][0m - Initializing the config with a `BiT` backbone.[0m
[32m[2023-03-15 15:12:14,161] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--hf-internal-testing--tiny-random-DPTForDepthEstimation/snapshots/11b7735d64d95b6599811631b012d2dec6eaa2c1/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--hf-internal-testing--tiny-random-DPTForDepthEstimation/snapshots/11b7735d64d95b6599811631b012d2dec6eaa2c1/preprocessor_config.json[0m
[32m[2023-03-15 15:12:14,161] [    INFO][0m - Image processor DPTImageProcessor {
  "crop_size": 32,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "ensure_multiple_of": 1,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "DPTImageProcessor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "keep_aspect_ratio": false,
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "height": 32,
    "width": 32
  }
}
[0m
________ StableDiffusionImg2ImgPipelineNightlyTests.test_depth2img_ddim ________

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_depth.StableDiffusionImg2ImgPipelineNightlyTests testMethod=test_depth2img_ddim>

    def test_depth2img_ddim(self):
        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained("stabilityai/stable-diffusion-2-depth")
        pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)
        pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
        image = pipe(**inputs).images[0]
        expected_image = load_numpy(
            "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_depth2img/stable_diffusion_2_0_ddim.npy"
        )
        max_diff = np.abs(expected_image - image).max()
>       assert max_diff < 0.001
E       assert 0.77503633 < 0.001

tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py:434: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:14:33,050] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/text_encoder/config.json[0m
[32m[2023-03-15 15:14:33,051] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "/home/suraj_huggingface_co/.cache/huggingface/diffusers/models--fusing--sd-depth-test/snapshots/200d326f1f39e1df3ffc78f7f0323c54fd68bdb6/text_encoder",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_size": 1024,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 16,
  "num_hidden_layers": 23,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 15:14:44,934] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 15:14:44,935] [    INFO][0m - Image processor DPTImageProcessor {
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "ensure_multiple_of": 1,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "DPTImageProcessor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "keep_aspect_ratio": false,
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "height": 384,
    "width": 384
  }
}
[0m
[32m[2023-03-15 15:14:44,935] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/depth_estimator/config.json[0m
[32m[2023-03-15 15:14:44,936] [    INFO][0m - Initializing the config with a `BiT` backbone.[0m
[32m[2023-03-15 15:14:44,938] [    INFO][0m - Model config DPTConfig {
  "_commit_hash": "65776c96786d3dd82c1cdb4847ea0c2cb46a943c",
  "_name_or_path": "Intel/dpt-hybrid-midas",
  "architectures": [
    "DPTForDepthEstimation"
  ],
  "attention_probs_dropout_prob": 0.0,
  "auxiliary_loss_weight": 0.4,
  "backbone_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "classifier_dropout": null,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "depths": [
      3,
      4,
      9
    ],
    "diversity_penalty": 0.0,
    "do_sample": false,
    "drop_path_rate": 0.0,
    "dtype": null,
    "early_stopping": false,
    "embedding_dynamic_padding": true,
    "embedding_size": 64,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "global_padding": "SAME",
    "hidden_act": "relu",
    "hidden_sizes": [
      256,
      512,
      1024,
      2048
    ],
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_type": "bottleneck",
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "bit",
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_channels": 3,
    "num_groups": 32,
    "num_return_sequences": 1,
    "out_features": [
      "stage1",
      "stage2",
      "stage3"
    ],
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "output_stride": 32,
    "pad_token_id": null,
    "paddlenlp_version": null,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "stage_names": [
      "stem",
      "stage1",
      "stage2",
      "stage3"
    ],
    "suppress_tokens": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.26.0.dev0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": false,
    "width_factor": 1
  },
  "backbone_featmap_shape": [
    1,
    1024,
    24,
    24
  ],
  "backbone_out_indices": [
    2,
    5,
    8,
    11
  ],
  "fusion_hidden_size": 256,
  "head_in_index": -1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34",
    "35": "LABEL_35",
    "36": "LABEL_36",
    "37": "LABEL_37",
    "38": "LABEL_38",
    "39": "LABEL_39",
    "40": "LABEL_40",
    "41": "LABEL_41",
    "42": "LABEL_42",
    "43": "LABEL_43",
    "44": "LABEL_44",
    "45": "LABEL_45",
    "46": "LABEL_46",
    "47": "LABEL_47",
    "48": "LABEL_48",
    "49": "LABEL_49",
    "50": "LABEL_50",
    "51": "LABEL_51",
    "52": "LABEL_52",
    "53": "LABEL_53",
    "54": "LABEL_54",
    "55": "LABEL_55",
    "56": "LABEL_56",
    "57": "LABEL_57",
    "58": "LABEL_58",
    "59": "LABEL_59",
    "60": "LABEL_60",
    "61": "LABEL_61",
    "62": "LABEL_62",
    "63": "LABEL_63",
    "64": "LABEL_64",
    "65": "LABEL_65",
    "66": "LABEL_66",
    "67": "LABEL_67",
    "68": "LABEL_68",
    "69": "LABEL_69",
    "70": "LABEL_70",
    "71": "LABEL_71",
    "72": "LABEL_72",
    "73": "LABEL_73",
    "74": "LABEL_74",
    "75": "LABEL_75",
    "76": "LABEL_76",
    "77": "LABEL_77",
    "78": "LABEL_78",
    "79": "LABEL_79",
    "80": "LABEL_80",
    "81": "LABEL_81",
    "82": "LABEL_82",
    "83": "LABEL_83",
    "84": "LABEL_84",
    "85": "LABEL_85",
    "86": "LABEL_86",
    "87": "LABEL_87",
    "88": "LABEL_88",
    "89": "LABEL_89",
    "90": "LABEL_90",
    "91": "LABEL_91",
    "92": "LABEL_92",
    "93": "LABEL_93",
    "94": "LABEL_94",
    "95": "LABEL_95",
    "96": "LABEL_96",
    "97": "LABEL_97",
    "98": "LABEL_98",
    "99": "LABEL_99",
    "100": "LABEL_100",
    "101": "LABEL_101",
    "102": "LABEL_102",
    "103": "LABEL_103",
    "104": "LABEL_104",
    "105": "LABEL_105",
    "106": "LABEL_106",
    "107": "LABEL_107",
    "108": "LABEL_108",
    "109": "LABEL_109",
    "110": "LABEL_110",
    "111": "LABEL_111",
    "112": "LABEL_112",
    "113": "LABEL_113",
    "114": "LABEL_114",
    "115": "LABEL_115",
    "116": "LABEL_116",
    "117": "LABEL_117",
    "118": "LABEL_118",
    "119": "LABEL_119",
    "120": "LABEL_120",
    "121": "LABEL_121",
    "122": "LABEL_122",
    "123": "LABEL_123",
    "124": "LABEL_124",
    "125": "LABEL_125",
    "126": "LABEL_126",
    "127": "LABEL_127",
    "128": "LABEL_128",
    "129": "LABEL_129",
    "130": "LABEL_130",
    "131": "LABEL_131",
    "132": "LABEL_132",
    "133": "LABEL_133",
    "134": "LABEL_134",
    "135": "LABEL_135",
    "136": "LABEL_136",
    "137": "LABEL_137",
    "138": "LABEL_138",
    "139": "LABEL_139",
    "140": "LABEL_140",
    "141": "LABEL_141",
    "142": "LABEL_142",
    "143": "LABEL_143",
    "144": "LABEL_144",
    "145": "LABEL_145",
    "146": "LABEL_146",
    "147": "LABEL_147",
    "148": "LABEL_148",
    "149": "LABEL_149"
  },
  "image_size": 384,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_hybrid": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_100": 100,
    "LABEL_101": 101,
    "LABEL_102": 102,
    "LABEL_103": 103,
    "LABEL_104": 104,
    "LABEL_105": 105,
    "LABEL_106": 106,
    "LABEL_107": 107,
    "LABEL_108": 108,
    "LABEL_109": 109,
    "LABEL_11": 11,
    "LABEL_110": 110,
    "LABEL_111": 111,
    "LABEL_112": 112,
    "LABEL_113": 113,
    "LABEL_114": 114,
    "LABEL_115": 115,
    "LABEL_116": 116,
    "LABEL_117": 117,
    "LABEL_118": 118,
    "LABEL_119": 119,
    "LABEL_12": 12,
    "LABEL_120": 120,
    "LABEL_121": 121,
    "LABEL_122": 122,
    "LABEL_123": 123,
    "LABEL_124": 124,
    "LABEL_125": 125,
    "LABEL_126": 126,
    "LABEL_127": 127,
    "LABEL_128": 128,
    "LABEL_129": 129,
    "LABEL_13": 13,
    "LABEL_130": 130,
    "LABEL_131": 131,
    "LABEL_132": 132,
    "LABEL_133": 133,
    "LABEL_134": 134,
    "LABEL_135": 135,
    "LABEL_136": 136,
    "LABEL_137": 137,
    "LABEL_138": 138,
    "LABEL_139": 139,
    "LABEL_14": 14,
    "LABEL_140": 140,
    "LABEL_141": 141,
    "LABEL_142": 142,
    "LABEL_143": 143,
    "LABEL_144": 144,
    "LABEL_145": 145,
    "LABEL_146": 146,
    "LABEL_147": 147,
    "LABEL_148": 148,
    "LABEL_149": 149,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_32": 32,
    "LABEL_33": 33,
    "LABEL_34": 34,
    "LABEL_35": 35,
    "LABEL_36": 36,
    "LABEL_37": 37,
    "LABEL_38": 38,
    "LABEL_39": 39,
    "LABEL_4": 4,
    "LABEL_40": 40,
    "LABEL_41": 41,
    "LABEL_42": 42,
    "LABEL_43": 43,
    "LABEL_44": 44,
    "LABEL_45": 45,
    "LABEL_46": 46,
    "LABEL_47": 47,
    "LABEL_48": 48,
    "LABEL_49": 49,
    "LABEL_5": 5,
    "LABEL_50": 50,
    "LABEL_51": 51,
    "LABEL_52": 52,
    "LABEL_53": 53,
    "LABEL_54": 54,
    "LABEL_55": 55,
    "LABEL_56": 56,
    "LABEL_57": 57,
    "LABEL_58": 58,
    "LABEL_59": 59,
    "LABEL_6": 6,
    "LABEL_60": 60,
    "LABEL_61": 61,
    "LABEL_62": 62,
    "LABEL_63": 63,
    "LABEL_64": 64,
    "LABEL_65": 65,
    "LABEL_66": 66,
    "LABEL_67": 67,
    "LABEL_68": 68,
    "LABEL_69": 69,
    "LABEL_7": 7,
    "LABEL_70": 70,
    "LABEL_71": 71,
    "LABEL_72": 72,
    "LABEL_73": 73,
    "LABEL_74": 74,
    "LABEL_75": 75,
    "LABEL_76": 76,
    "LABEL_77": 77,
    "LABEL_78": 78,
    "LABEL_79": 79,
    "LABEL_8": 8,
    "LABEL_80": 80,
    "LABEL_81": 81,
    "LABEL_82": 82,
    "LABEL_83": 83,
    "LABEL_84": 84,
    "LABEL_85": 85,
    "LABEL_86": 86,
    "LABEL_87": 87,
    "LABEL_88": 88,
    "LABEL_89": 89,
    "LABEL_9": 9,
    "LABEL_90": 90,
    "LABEL_91": 91,
    "LABEL_92": 92,
    "LABEL_93": 93,
    "LABEL_94": 94,
    "LABEL_95": 95,
    "LABEL_96": 96,
    "LABEL_97": 97,
    "LABEL_98": 98,
    "LABEL_99": 99
  },
  "layer_norm_eps": 1e-12,
  "model_type": "dpt",
  "neck_hidden_sizes": [
    256,
    512,
    768,
    768
  ],
  "neck_ignore_stages": [
    0,
    1
  ],
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "paddlenlp_version": null,
  "patch_size": 16,
  "qkv_bias": true,
  "readout_type": "project",
  "reassemble_factors": [
    1,
    1,
    1,
    0.5
  ],
  "return_dict": true,
  "semantic_classifier_dropout": 0.1,
  "semantic_loss_ignore_index": 255,
  "torch_dtype": "float32",
  "transformers_version": null,
  "use_auxiliary_head": true,
  "use_batch_norm_in_fusion_residual": false
}
[0m
________ StableDiffusionImg2ImgPipelineNightlyTests.test_depth2img_pndm ________

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_depth.StableDiffusionImg2ImgPipelineNightlyTests testMethod=test_depth2img_pndm>

    def test_depth2img_pndm(self):
        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained("stabilityai/stable-diffusion-2-depth")
        pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
        image = pipe(**inputs).images[0]
        expected_image = load_numpy(
            "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_depth2img/stable_diffusion_2_0_pndm.npy"
        )
        max_diff = np.abs(expected_image - image).max()
>       assert max_diff < 0.001
E       assert 1.0 < 0.001

tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py:421: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:15:31,375] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/text_encoder/config.json[0m
[32m[2023-03-15 15:15:31,377] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "/home/suraj_huggingface_co/.cache/huggingface/diffusers/models--fusing--sd-depth-test/snapshots/200d326f1f39e1df3ffc78f7f0323c54fd68bdb6/text_encoder",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_size": 1024,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 16,
  "num_hidden_layers": 23,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 15:15:43,614] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 15:15:43,614] [    INFO][0m - Image processor DPTImageProcessor {
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "ensure_multiple_of": 1,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "DPTImageProcessor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "keep_aspect_ratio": false,
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "height": 384,
    "width": 384
  }
}
[0m
[32m[2023-03-15 15:15:43,615] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/depth_estimator/config.json[0m
[32m[2023-03-15 15:15:43,615] [    INFO][0m - Initializing the config with a `BiT` backbone.[0m
[32m[2023-03-15 15:15:43,617] [    INFO][0m - Model config DPTConfig {
  "_commit_hash": "65776c96786d3dd82c1cdb4847ea0c2cb46a943c",
  "_name_or_path": "Intel/dpt-hybrid-midas",
  "architectures": [
    "DPTForDepthEstimation"
  ],
  "attention_probs_dropout_prob": 0.0,
  "auxiliary_loss_weight": 0.4,
  "backbone_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "classifier_dropout": null,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "depths": [
      3,
      4,
      9
    ],
    "diversity_penalty": 0.0,
    "do_sample": false,
    "drop_path_rate": 0.0,
    "dtype": null,
    "early_stopping": false,
    "embedding_dynamic_padding": true,
    "embedding_size": 64,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "global_padding": "SAME",
    "hidden_act": "relu",
    "hidden_sizes": [
      256,
      512,
      1024,
      2048
    ],
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_type": "bottleneck",
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "bit",
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_channels": 3,
    "num_groups": 32,
    "num_return_sequences": 1,
    "out_features": [
      "stage1",
      "stage2",
      "stage3"
    ],
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "output_stride": 32,
    "pad_token_id": null,
    "paddlenlp_version": null,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "stage_names": [
      "stem",
      "stage1",
      "stage2",
      "stage3"
    ],
    "suppress_tokens": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.26.0.dev0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": false,
    "width_factor": 1
  },
  "backbone_featmap_shape": [
    1,
    1024,
    24,
    24
  ],
  "backbone_out_indices": [
    2,
    5,
    8,
    11
  ],
  "fusion_hidden_size": 256,
  "head_in_index": -1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34",
    "35": "LABEL_35",
    "36": "LABEL_36",
    "37": "LABEL_37",
    "38": "LABEL_38",
    "39": "LABEL_39",
    "40": "LABEL_40",
    "41": "LABEL_41",
    "42": "LABEL_42",
    "43": "LABEL_43",
    "44": "LABEL_44",
    "45": "LABEL_45",
    "46": "LABEL_46",
    "47": "LABEL_47",
    "48": "LABEL_48",
    "49": "LABEL_49",
    "50": "LABEL_50",
    "51": "LABEL_51",
    "52": "LABEL_52",
    "53": "LABEL_53",
    "54": "LABEL_54",
    "55": "LABEL_55",
    "56": "LABEL_56",
    "57": "LABEL_57",
    "58": "LABEL_58",
    "59": "LABEL_59",
    "60": "LABEL_60",
    "61": "LABEL_61",
    "62": "LABEL_62",
    "63": "LABEL_63",
    "64": "LABEL_64",
    "65": "LABEL_65",
    "66": "LABEL_66",
    "67": "LABEL_67",
    "68": "LABEL_68",
    "69": "LABEL_69",
    "70": "LABEL_70",
    "71": "LABEL_71",
    "72": "LABEL_72",
    "73": "LABEL_73",
    "74": "LABEL_74",
    "75": "LABEL_75",
    "76": "LABEL_76",
    "77": "LABEL_77",
    "78": "LABEL_78",
    "79": "LABEL_79",
    "80": "LABEL_80",
    "81": "LABEL_81",
    "82": "LABEL_82",
    "83": "LABEL_83",
    "84": "LABEL_84",
    "85": "LABEL_85",
    "86": "LABEL_86",
    "87": "LABEL_87",
    "88": "LABEL_88",
    "89": "LABEL_89",
    "90": "LABEL_90",
    "91": "LABEL_91",
    "92": "LABEL_92",
    "93": "LABEL_93",
    "94": "LABEL_94",
    "95": "LABEL_95",
    "96": "LABEL_96",
    "97": "LABEL_97",
    "98": "LABEL_98",
    "99": "LABEL_99",
    "100": "LABEL_100",
    "101": "LABEL_101",
    "102": "LABEL_102",
    "103": "LABEL_103",
    "104": "LABEL_104",
    "105": "LABEL_105",
    "106": "LABEL_106",
    "107": "LABEL_107",
    "108": "LABEL_108",
    "109": "LABEL_109",
    "110": "LABEL_110",
    "111": "LABEL_111",
    "112": "LABEL_112",
    "113": "LABEL_113",
    "114": "LABEL_114",
    "115": "LABEL_115",
    "116": "LABEL_116",
    "117": "LABEL_117",
    "118": "LABEL_118",
    "119": "LABEL_119",
    "120": "LABEL_120",
    "121": "LABEL_121",
    "122": "LABEL_122",
    "123": "LABEL_123",
    "124": "LABEL_124",
    "125": "LABEL_125",
    "126": "LABEL_126",
    "127": "LABEL_127",
    "128": "LABEL_128",
    "129": "LABEL_129",
    "130": "LABEL_130",
    "131": "LABEL_131",
    "132": "LABEL_132",
    "133": "LABEL_133",
    "134": "LABEL_134",
    "135": "LABEL_135",
    "136": "LABEL_136",
    "137": "LABEL_137",
    "138": "LABEL_138",
    "139": "LABEL_139",
    "140": "LABEL_140",
    "141": "LABEL_141",
    "142": "LABEL_142",
    "143": "LABEL_143",
    "144": "LABEL_144",
    "145": "LABEL_145",
    "146": "LABEL_146",
    "147": "LABEL_147",
    "148": "LABEL_148",
    "149": "LABEL_149"
  },
  "image_size": 384,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_hybrid": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_100": 100,
    "LABEL_101": 101,
    "LABEL_102": 102,
    "LABEL_103": 103,
    "LABEL_104": 104,
    "LABEL_105": 105,
    "LABEL_106": 106,
    "LABEL_107": 107,
    "LABEL_108": 108,
    "LABEL_109": 109,
    "LABEL_11": 11,
    "LABEL_110": 110,
    "LABEL_111": 111,
    "LABEL_112": 112,
    "LABEL_113": 113,
    "LABEL_114": 114,
    "LABEL_115": 115,
    "LABEL_116": 116,
    "LABEL_117": 117,
    "LABEL_118": 118,
    "LABEL_119": 119,
    "LABEL_12": 12,
    "LABEL_120": 120,
    "LABEL_121": 121,
    "LABEL_122": 122,
    "LABEL_123": 123,
    "LABEL_124": 124,
    "LABEL_125": 125,
    "LABEL_126": 126,
    "LABEL_127": 127,
    "LABEL_128": 128,
    "LABEL_129": 129,
    "LABEL_13": 13,
    "LABEL_130": 130,
    "LABEL_131": 131,
    "LABEL_132": 132,
    "LABEL_133": 133,
    "LABEL_134": 134,
    "LABEL_135": 135,
    "LABEL_136": 136,
    "LABEL_137": 137,
    "LABEL_138": 138,
    "LABEL_139": 139,
    "LABEL_14": 14,
    "LABEL_140": 140,
    "LABEL_141": 141,
    "LABEL_142": 142,
    "LABEL_143": 143,
    "LABEL_144": 144,
    "LABEL_145": 145,
    "LABEL_146": 146,
    "LABEL_147": 147,
    "LABEL_148": 148,
    "LABEL_149": 149,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_32": 32,
    "LABEL_33": 33,
    "LABEL_34": 34,
    "LABEL_35": 35,
    "LABEL_36": 36,
    "LABEL_37": 37,
    "LABEL_38": 38,
    "LABEL_39": 39,
    "LABEL_4": 4,
    "LABEL_40": 40,
    "LABEL_41": 41,
    "LABEL_42": 42,
    "LABEL_43": 43,
    "LABEL_44": 44,
    "LABEL_45": 45,
    "LABEL_46": 46,
    "LABEL_47": 47,
    "LABEL_48": 48,
    "LABEL_49": 49,
    "LABEL_5": 5,
    "LABEL_50": 50,
    "LABEL_51": 51,
    "LABEL_52": 52,
    "LABEL_53": 53,
    "LABEL_54": 54,
    "LABEL_55": 55,
    "LABEL_56": 56,
    "LABEL_57": 57,
    "LABEL_58": 58,
    "LABEL_59": 59,
    "LABEL_6": 6,
    "LABEL_60": 60,
    "LABEL_61": 61,
    "LABEL_62": 62,
    "LABEL_63": 63,
    "LABEL_64": 64,
    "LABEL_65": 65,
    "LABEL_66": 66,
    "LABEL_67": 67,
    "LABEL_68": 68,
    "LABEL_69": 69,
    "LABEL_7": 7,
    "LABEL_70": 70,
    "LABEL_71": 71,
    "LABEL_72": 72,
    "LABEL_73": 73,
    "LABEL_74": 74,
    "LABEL_75": 75,
    "LABEL_76": 76,
    "LABEL_77": 77,
    "LABEL_78": 78,
    "LABEL_79": 79,
    "LABEL_8": 8,
    "LABEL_80": 80,
    "LABEL_81": 81,
    "LABEL_82": 82,
    "LABEL_83": 83,
    "LABEL_84": 84,
    "LABEL_85": 85,
    "LABEL_86": 86,
    "LABEL_87": 87,
    "LABEL_88": 88,
    "LABEL_89": 89,
    "LABEL_9": 9,
    "LABEL_90": 90,
    "LABEL_91": 91,
    "LABEL_92": 92,
    "LABEL_93": 93,
    "LABEL_94": 94,
    "LABEL_95": 95,
    "LABEL_96": 96,
    "LABEL_97": 97,
    "LABEL_98": 98,
    "LABEL_99": 99
  },
  "layer_norm_eps": 1e-12,
  "model_type": "dpt",
  "neck_hidden_sizes": [
    256,
    512,
    768,
    768
  ],
  "neck_ignore_stages": [
    0,
    1
  ],
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "paddlenlp_version": null,
  "patch_size": 16,
  "qkv_bias": true,
  "readout_type": "project",
  "reassemble_factors": [
    1,
    1,
    1,
    0.5
  ],
  "return_dict": true,
  "semantic_classifier_dropout": 0.1,
  "semantic_loss_ignore_index": 255,
  "torch_dtype": "float32",
  "transformers_version": null,
  "use_auxiliary_head": true,
  "use_batch_norm_in_fusion_residual": false
}
[0m
_________ StableDiffusionImg2ImgPipelineNightlyTests.test_img2img_dpm __________

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_depth.StableDiffusionImg2ImgPipelineNightlyTests testMethod=test_img2img_dpm>

    def test_img2img_dpm(self):
        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained("stabilityai/stable-diffusion-2-depth")
        pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)
        pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
        inputs["num_inference_steps"] = 30
        image = pipe(**inputs).images[0]
        expected_image = load_numpy(
            "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_depth2img/stable_diffusion_2_0_dpm_multi.npy"
        )
        max_diff = np.abs(expected_image - image).max()
>       assert max_diff < 0.001
E       assert 1.0 < 0.001

tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py:461: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:16:08,207] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/text_encoder/config.json[0m
[32m[2023-03-15 15:16:08,208] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "/home/suraj_huggingface_co/.cache/huggingface/diffusers/models--fusing--sd-depth-test/snapshots/200d326f1f39e1df3ffc78f7f0323c54fd68bdb6/text_encoder",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_size": 1024,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 16,
  "num_hidden_layers": 23,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 15:16:20,925] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 15:16:20,926] [    INFO][0m - Image processor DPTImageProcessor {
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "ensure_multiple_of": 1,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "DPTImageProcessor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "keep_aspect_ratio": false,
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "height": 384,
    "width": 384
  }
}
[0m
[32m[2023-03-15 15:16:20,926] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/depth_estimator/config.json[0m
[32m[2023-03-15 15:16:20,927] [    INFO][0m - Initializing the config with a `BiT` backbone.[0m
[32m[2023-03-15 15:16:20,930] [    INFO][0m - Model config DPTConfig {
  "_commit_hash": "65776c96786d3dd82c1cdb4847ea0c2cb46a943c",
  "_name_or_path": "Intel/dpt-hybrid-midas",
  "architectures": [
    "DPTForDepthEstimation"
  ],
  "attention_probs_dropout_prob": 0.0,
  "auxiliary_loss_weight": 0.4,
  "backbone_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "classifier_dropout": null,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "depths": [
      3,
      4,
      9
    ],
    "diversity_penalty": 0.0,
    "do_sample": false,
    "drop_path_rate": 0.0,
    "dtype": null,
    "early_stopping": false,
    "embedding_dynamic_padding": true,
    "embedding_size": 64,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "global_padding": "SAME",
    "hidden_act": "relu",
    "hidden_sizes": [
      256,
      512,
      1024,
      2048
    ],
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_type": "bottleneck",
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "bit",
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_channels": 3,
    "num_groups": 32,
    "num_return_sequences": 1,
    "out_features": [
      "stage1",
      "stage2",
      "stage3"
    ],
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "output_stride": 32,
    "pad_token_id": null,
    "paddlenlp_version": null,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "stage_names": [
      "stem",
      "stage1",
      "stage2",
      "stage3"
    ],
    "suppress_tokens": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.26.0.dev0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": false,
    "width_factor": 1
  },
  "backbone_featmap_shape": [
    1,
    1024,
    24,
    24
  ],
  "backbone_out_indices": [
    2,
    5,
    8,
    11
  ],
  "fusion_hidden_size": 256,
  "head_in_index": -1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34",
    "35": "LABEL_35",
    "36": "LABEL_36",
    "37": "LABEL_37",
    "38": "LABEL_38",
    "39": "LABEL_39",
    "40": "LABEL_40",
    "41": "LABEL_41",
    "42": "LABEL_42",
    "43": "LABEL_43",
    "44": "LABEL_44",
    "45": "LABEL_45",
    "46": "LABEL_46",
    "47": "LABEL_47",
    "48": "LABEL_48",
    "49": "LABEL_49",
    "50": "LABEL_50",
    "51": "LABEL_51",
    "52": "LABEL_52",
    "53": "LABEL_53",
    "54": "LABEL_54",
    "55": "LABEL_55",
    "56": "LABEL_56",
    "57": "LABEL_57",
    "58": "LABEL_58",
    "59": "LABEL_59",
    "60": "LABEL_60",
    "61": "LABEL_61",
    "62": "LABEL_62",
    "63": "LABEL_63",
    "64": "LABEL_64",
    "65": "LABEL_65",
    "66": "LABEL_66",
    "67": "LABEL_67",
    "68": "LABEL_68",
    "69": "LABEL_69",
    "70": "LABEL_70",
    "71": "LABEL_71",
    "72": "LABEL_72",
    "73": "LABEL_73",
    "74": "LABEL_74",
    "75": "LABEL_75",
    "76": "LABEL_76",
    "77": "LABEL_77",
    "78": "LABEL_78",
    "79": "LABEL_79",
    "80": "LABEL_80",
    "81": "LABEL_81",
    "82": "LABEL_82",
    "83": "LABEL_83",
    "84": "LABEL_84",
    "85": "LABEL_85",
    "86": "LABEL_86",
    "87": "LABEL_87",
    "88": "LABEL_88",
    "89": "LABEL_89",
    "90": "LABEL_90",
    "91": "LABEL_91",
    "92": "LABEL_92",
    "93": "LABEL_93",
    "94": "LABEL_94",
    "95": "LABEL_95",
    "96": "LABEL_96",
    "97": "LABEL_97",
    "98": "LABEL_98",
    "99": "LABEL_99",
    "100": "LABEL_100",
    "101": "LABEL_101",
    "102": "LABEL_102",
    "103": "LABEL_103",
    "104": "LABEL_104",
    "105": "LABEL_105",
    "106": "LABEL_106",
    "107": "LABEL_107",
    "108": "LABEL_108",
    "109": "LABEL_109",
    "110": "LABEL_110",
    "111": "LABEL_111",
    "112": "LABEL_112",
    "113": "LABEL_113",
    "114": "LABEL_114",
    "115": "LABEL_115",
    "116": "LABEL_116",
    "117": "LABEL_117",
    "118": "LABEL_118",
    "119": "LABEL_119",
    "120": "LABEL_120",
    "121": "LABEL_121",
    "122": "LABEL_122",
    "123": "LABEL_123",
    "124": "LABEL_124",
    "125": "LABEL_125",
    "126": "LABEL_126",
    "127": "LABEL_127",
    "128": "LABEL_128",
    "129": "LABEL_129",
    "130": "LABEL_130",
    "131": "LABEL_131",
    "132": "LABEL_132",
    "133": "LABEL_133",
    "134": "LABEL_134",
    "135": "LABEL_135",
    "136": "LABEL_136",
    "137": "LABEL_137",
    "138": "LABEL_138",
    "139": "LABEL_139",
    "140": "LABEL_140",
    "141": "LABEL_141",
    "142": "LABEL_142",
    "143": "LABEL_143",
    "144": "LABEL_144",
    "145": "LABEL_145",
    "146": "LABEL_146",
    "147": "LABEL_147",
    "148": "LABEL_148",
    "149": "LABEL_149"
  },
  "image_size": 384,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_hybrid": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_100": 100,
    "LABEL_101": 101,
    "LABEL_102": 102,
    "LABEL_103": 103,
    "LABEL_104": 104,
    "LABEL_105": 105,
    "LABEL_106": 106,
    "LABEL_107": 107,
    "LABEL_108": 108,
    "LABEL_109": 109,
    "LABEL_11": 11,
    "LABEL_110": 110,
    "LABEL_111": 111,
    "LABEL_112": 112,
    "LABEL_113": 113,
    "LABEL_114": 114,
    "LABEL_115": 115,
    "LABEL_116": 116,
    "LABEL_117": 117,
    "LABEL_118": 118,
    "LABEL_119": 119,
    "LABEL_12": 12,
    "LABEL_120": 120,
    "LABEL_121": 121,
    "LABEL_122": 122,
    "LABEL_123": 123,
    "LABEL_124": 124,
    "LABEL_125": 125,
    "LABEL_126": 126,
    "LABEL_127": 127,
    "LABEL_128": 128,
    "LABEL_129": 129,
    "LABEL_13": 13,
    "LABEL_130": 130,
    "LABEL_131": 131,
    "LABEL_132": 132,
    "LABEL_133": 133,
    "LABEL_134": 134,
    "LABEL_135": 135,
    "LABEL_136": 136,
    "LABEL_137": 137,
    "LABEL_138": 138,
    "LABEL_139": 139,
    "LABEL_14": 14,
    "LABEL_140": 140,
    "LABEL_141": 141,
    "LABEL_142": 142,
    "LABEL_143": 143,
    "LABEL_144": 144,
    "LABEL_145": 145,
    "LABEL_146": 146,
    "LABEL_147": 147,
    "LABEL_148": 148,
    "LABEL_149": 149,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_32": 32,
    "LABEL_33": 33,
    "LABEL_34": 34,
    "LABEL_35": 35,
    "LABEL_36": 36,
    "LABEL_37": 37,
    "LABEL_38": 38,
    "LABEL_39": 39,
    "LABEL_4": 4,
    "LABEL_40": 40,
    "LABEL_41": 41,
    "LABEL_42": 42,
    "LABEL_43": 43,
    "LABEL_44": 44,
    "LABEL_45": 45,
    "LABEL_46": 46,
    "LABEL_47": 47,
    "LABEL_48": 48,
    "LABEL_49": 49,
    "LABEL_5": 5,
    "LABEL_50": 50,
    "LABEL_51": 51,
    "LABEL_52": 52,
    "LABEL_53": 53,
    "LABEL_54": 54,
    "LABEL_55": 55,
    "LABEL_56": 56,
    "LABEL_57": 57,
    "LABEL_58": 58,
    "LABEL_59": 59,
    "LABEL_6": 6,
    "LABEL_60": 60,
    "LABEL_61": 61,
    "LABEL_62": 62,
    "LABEL_63": 63,
    "LABEL_64": 64,
    "LABEL_65": 65,
    "LABEL_66": 66,
    "LABEL_67": 67,
    "LABEL_68": 68,
    "LABEL_69": 69,
    "LABEL_7": 7,
    "LABEL_70": 70,
    "LABEL_71": 71,
    "LABEL_72": 72,
    "LABEL_73": 73,
    "LABEL_74": 74,
    "LABEL_75": 75,
    "LABEL_76": 76,
    "LABEL_77": 77,
    "LABEL_78": 78,
    "LABEL_79": 79,
    "LABEL_8": 8,
    "LABEL_80": 80,
    "LABEL_81": 81,
    "LABEL_82": 82,
    "LABEL_83": 83,
    "LABEL_84": 84,
    "LABEL_85": 85,
    "LABEL_86": 86,
    "LABEL_87": 87,
    "LABEL_88": 88,
    "LABEL_89": 89,
    "LABEL_9": 9,
    "LABEL_90": 90,
    "LABEL_91": 91,
    "LABEL_92": 92,
    "LABEL_93": 93,
    "LABEL_94": 94,
    "LABEL_95": 95,
    "LABEL_96": 96,
    "LABEL_97": 97,
    "LABEL_98": 98,
    "LABEL_99": 99
  },
  "layer_norm_eps": 1e-12,
  "model_type": "dpt",
  "neck_hidden_sizes": [
    256,
    512,
    768,
    768
  ],
  "neck_ignore_stages": [
    0,
    1
  ],
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "paddlenlp_version": null,
  "patch_size": 16,
  "qkv_bias": true,
  "readout_type": "project",
  "reassemble_factors": [
    1,
    1,
    1,
    0.5
  ],
  "return_dict": true,
  "semantic_classifier_dropout": 0.1,
  "semantic_loss_ignore_index": 255,
  "torch_dtype": "float32",
  "transformers_version": null,
  "use_auxiliary_head": true,
  "use_batch_norm_in_fusion_residual": false
}
[0m
_________ StableDiffusionImg2ImgPipelineNightlyTests.test_img2img_lms __________

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_depth.StableDiffusionImg2ImgPipelineNightlyTests testMethod=test_img2img_lms>

    def test_img2img_lms(self):
        pipe = StableDiffusionDepth2ImgPipeline.from_pretrained("stabilityai/stable-diffusion-2-depth")
        pipe.scheduler = LMSDiscreteScheduler.from_config(pipe.scheduler.config)
        pipe.set_progress_bar_config(disable=None)
        inputs = self.get_inputs()
        image = pipe(**inputs).images[0]
        expected_image = load_numpy(
            "https://huggingface.co/datasets/diffusers/test-arrays/resolve/main/stable_diffusion_depth2img/stable_diffusion_2_0_lms.npy"
        )
        max_diff = np.abs(expected_image - image).max()
>       assert max_diff < 0.001
E       assert 0.9579176 < 0.001

tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py:447: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:16:50,230] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/text_encoder/config.json[0m
[32m[2023-03-15 15:16:50,231] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "/home/suraj_huggingface_co/.cache/huggingface/diffusers/models--fusing--sd-depth-test/snapshots/200d326f1f39e1df3ffc78f7f0323c54fd68bdb6/text_encoder",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_size": 1024,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 16,
  "num_hidden_layers": 23,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.26.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 15:17:02,627] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 15:17:02,628] [    INFO][0m - Image processor DPTImageProcessor {
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "ensure_multiple_of": 1,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "DPTImageProcessor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "keep_aspect_ratio": false,
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "height": 384,
    "width": 384
  }
}
[0m
[32m[2023-03-15 15:17:02,628] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-depth/snapshots/d41a0687231847e8bd55f43fb1f576afaeefef19/depth_estimator/config.json[0m
[32m[2023-03-15 15:17:02,629] [    INFO][0m - Initializing the config with a `BiT` backbone.[0m
[32m[2023-03-15 15:17:02,631] [    INFO][0m - Model config DPTConfig {
  "_commit_hash": "65776c96786d3dd82c1cdb4847ea0c2cb46a943c",
  "_name_or_path": "Intel/dpt-hybrid-midas",
  "architectures": [
    "DPTForDepthEstimation"
  ],
  "attention_probs_dropout_prob": 0.0,
  "auxiliary_loss_weight": 0.4,
  "backbone_config": {
    "_name_or_path": "",
    "add_cross_attention": false,
    "architectures": null,
    "bad_words_ids": null,
    "begin_suppress_tokens": null,
    "bos_token_id": null,
    "chunk_size_feed_forward": 0,
    "classifier_dropout": null,
    "cross_attention_hidden_size": null,
    "decoder_start_token_id": null,
    "depths": [
      3,
      4,
      9
    ],
    "diversity_penalty": 0.0,
    "do_sample": false,
    "drop_path_rate": 0.0,
    "dtype": null,
    "early_stopping": false,
    "embedding_dynamic_padding": true,
    "embedding_size": 64,
    "encoder_no_repeat_ngram_size": 0,
    "eos_token_id": null,
    "exponential_decay_length_penalty": null,
    "finetuning_task": null,
    "forced_bos_token_id": null,
    "forced_eos_token_id": null,
    "global_padding": "SAME",
    "hidden_act": "relu",
    "hidden_sizes": [
      256,
      512,
      1024,
      2048
    ],
    "id2label": {
      "0": "LABEL_0",
      "1": "LABEL_1"
    },
    "is_decoder": false,
    "is_encoder_decoder": false,
    "label2id": {
      "LABEL_0": 0,
      "LABEL_1": 1
    },
    "layer_type": "bottleneck",
    "length_penalty": 1.0,
    "max_length": 20,
    "min_length": 0,
    "model_type": "bit",
    "no_repeat_ngram_size": 0,
    "num_beam_groups": 1,
    "num_beams": 1,
    "num_channels": 3,
    "num_groups": 32,
    "num_return_sequences": 1,
    "out_features": [
      "stage1",
      "stage2",
      "stage3"
    ],
    "output_attentions": false,
    "output_hidden_states": false,
    "output_scores": false,
    "output_stride": 32,
    "pad_token_id": null,
    "paddlenlp_version": null,
    "prefix": null,
    "problem_type": null,
    "pruned_heads": {},
    "remove_invalid_values": false,
    "repetition_penalty": 1.0,
    "return_dict": true,
    "return_dict_in_generate": false,
    "sep_token_id": null,
    "stage_names": [
      "stem",
      "stage1",
      "stage2",
      "stage3"
    ],
    "suppress_tokens": null,
    "task_specific_params": null,
    "temperature": 1.0,
    "tf_legacy_loss": false,
    "tie_encoder_decoder": false,
    "tie_word_embeddings": true,
    "tokenizer_class": null,
    "top_k": 50,
    "top_p": 1.0,
    "torch_dtype": null,
    "torchscript": false,
    "transformers_version": "4.26.0.dev0",
    "typical_p": 1.0,
    "use_bfloat16": false,
    "use_cache": false,
    "width_factor": 1
  },
  "backbone_featmap_shape": [
    1,
    1024,
    24,
    24
  ],
  "backbone_out_indices": [
    2,
    5,
    8,
    11
  ],
  "fusion_hidden_size": 256,
  "head_in_index": -1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2",
    "3": "LABEL_3",
    "4": "LABEL_4",
    "5": "LABEL_5",
    "6": "LABEL_6",
    "7": "LABEL_7",
    "8": "LABEL_8",
    "9": "LABEL_9",
    "10": "LABEL_10",
    "11": "LABEL_11",
    "12": "LABEL_12",
    "13": "LABEL_13",
    "14": "LABEL_14",
    "15": "LABEL_15",
    "16": "LABEL_16",
    "17": "LABEL_17",
    "18": "LABEL_18",
    "19": "LABEL_19",
    "20": "LABEL_20",
    "21": "LABEL_21",
    "22": "LABEL_22",
    "23": "LABEL_23",
    "24": "LABEL_24",
    "25": "LABEL_25",
    "26": "LABEL_26",
    "27": "LABEL_27",
    "28": "LABEL_28",
    "29": "LABEL_29",
    "30": "LABEL_30",
    "31": "LABEL_31",
    "32": "LABEL_32",
    "33": "LABEL_33",
    "34": "LABEL_34",
    "35": "LABEL_35",
    "36": "LABEL_36",
    "37": "LABEL_37",
    "38": "LABEL_38",
    "39": "LABEL_39",
    "40": "LABEL_40",
    "41": "LABEL_41",
    "42": "LABEL_42",
    "43": "LABEL_43",
    "44": "LABEL_44",
    "45": "LABEL_45",
    "46": "LABEL_46",
    "47": "LABEL_47",
    "48": "LABEL_48",
    "49": "LABEL_49",
    "50": "LABEL_50",
    "51": "LABEL_51",
    "52": "LABEL_52",
    "53": "LABEL_53",
    "54": "LABEL_54",
    "55": "LABEL_55",
    "56": "LABEL_56",
    "57": "LABEL_57",
    "58": "LABEL_58",
    "59": "LABEL_59",
    "60": "LABEL_60",
    "61": "LABEL_61",
    "62": "LABEL_62",
    "63": "LABEL_63",
    "64": "LABEL_64",
    "65": "LABEL_65",
    "66": "LABEL_66",
    "67": "LABEL_67",
    "68": "LABEL_68",
    "69": "LABEL_69",
    "70": "LABEL_70",
    "71": "LABEL_71",
    "72": "LABEL_72",
    "73": "LABEL_73",
    "74": "LABEL_74",
    "75": "LABEL_75",
    "76": "LABEL_76",
    "77": "LABEL_77",
    "78": "LABEL_78",
    "79": "LABEL_79",
    "80": "LABEL_80",
    "81": "LABEL_81",
    "82": "LABEL_82",
    "83": "LABEL_83",
    "84": "LABEL_84",
    "85": "LABEL_85",
    "86": "LABEL_86",
    "87": "LABEL_87",
    "88": "LABEL_88",
    "89": "LABEL_89",
    "90": "LABEL_90",
    "91": "LABEL_91",
    "92": "LABEL_92",
    "93": "LABEL_93",
    "94": "LABEL_94",
    "95": "LABEL_95",
    "96": "LABEL_96",
    "97": "LABEL_97",
    "98": "LABEL_98",
    "99": "LABEL_99",
    "100": "LABEL_100",
    "101": "LABEL_101",
    "102": "LABEL_102",
    "103": "LABEL_103",
    "104": "LABEL_104",
    "105": "LABEL_105",
    "106": "LABEL_106",
    "107": "LABEL_107",
    "108": "LABEL_108",
    "109": "LABEL_109",
    "110": "LABEL_110",
    "111": "LABEL_111",
    "112": "LABEL_112",
    "113": "LABEL_113",
    "114": "LABEL_114",
    "115": "LABEL_115",
    "116": "LABEL_116",
    "117": "LABEL_117",
    "118": "LABEL_118",
    "119": "LABEL_119",
    "120": "LABEL_120",
    "121": "LABEL_121",
    "122": "LABEL_122",
    "123": "LABEL_123",
    "124": "LABEL_124",
    "125": "LABEL_125",
    "126": "LABEL_126",
    "127": "LABEL_127",
    "128": "LABEL_128",
    "129": "LABEL_129",
    "130": "LABEL_130",
    "131": "LABEL_131",
    "132": "LABEL_132",
    "133": "LABEL_133",
    "134": "LABEL_134",
    "135": "LABEL_135",
    "136": "LABEL_136",
    "137": "LABEL_137",
    "138": "LABEL_138",
    "139": "LABEL_139",
    "140": "LABEL_140",
    "141": "LABEL_141",
    "142": "LABEL_142",
    "143": "LABEL_143",
    "144": "LABEL_144",
    "145": "LABEL_145",
    "146": "LABEL_146",
    "147": "LABEL_147",
    "148": "LABEL_148",
    "149": "LABEL_149"
  },
  "image_size": 384,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "is_hybrid": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_10": 10,
    "LABEL_100": 100,
    "LABEL_101": 101,
    "LABEL_102": 102,
    "LABEL_103": 103,
    "LABEL_104": 104,
    "LABEL_105": 105,
    "LABEL_106": 106,
    "LABEL_107": 107,
    "LABEL_108": 108,
    "LABEL_109": 109,
    "LABEL_11": 11,
    "LABEL_110": 110,
    "LABEL_111": 111,
    "LABEL_112": 112,
    "LABEL_113": 113,
    "LABEL_114": 114,
    "LABEL_115": 115,
    "LABEL_116": 116,
    "LABEL_117": 117,
    "LABEL_118": 118,
    "LABEL_119": 119,
    "LABEL_12": 12,
    "LABEL_120": 120,
    "LABEL_121": 121,
    "LABEL_122": 122,
    "LABEL_123": 123,
    "LABEL_124": 124,
    "LABEL_125": 125,
    "LABEL_126": 126,
    "LABEL_127": 127,
    "LABEL_128": 128,
    "LABEL_129": 129,
    "LABEL_13": 13,
    "LABEL_130": 130,
    "LABEL_131": 131,
    "LABEL_132": 132,
    "LABEL_133": 133,
    "LABEL_134": 134,
    "LABEL_135": 135,
    "LABEL_136": 136,
    "LABEL_137": 137,
    "LABEL_138": 138,
    "LABEL_139": 139,
    "LABEL_14": 14,
    "LABEL_140": 140,
    "LABEL_141": 141,
    "LABEL_142": 142,
    "LABEL_143": 143,
    "LABEL_144": 144,
    "LABEL_145": 145,
    "LABEL_146": 146,
    "LABEL_147": 147,
    "LABEL_148": 148,
    "LABEL_149": 149,
    "LABEL_15": 15,
    "LABEL_16": 16,
    "LABEL_17": 17,
    "LABEL_18": 18,
    "LABEL_19": 19,
    "LABEL_2": 2,
    "LABEL_20": 20,
    "LABEL_21": 21,
    "LABEL_22": 22,
    "LABEL_23": 23,
    "LABEL_24": 24,
    "LABEL_25": 25,
    "LABEL_26": 26,
    "LABEL_27": 27,
    "LABEL_28": 28,
    "LABEL_29": 29,
    "LABEL_3": 3,
    "LABEL_30": 30,
    "LABEL_31": 31,
    "LABEL_32": 32,
    "LABEL_33": 33,
    "LABEL_34": 34,
    "LABEL_35": 35,
    "LABEL_36": 36,
    "LABEL_37": 37,
    "LABEL_38": 38,
    "LABEL_39": 39,
    "LABEL_4": 4,
    "LABEL_40": 40,
    "LABEL_41": 41,
    "LABEL_42": 42,
    "LABEL_43": 43,
    "LABEL_44": 44,
    "LABEL_45": 45,
    "LABEL_46": 46,
    "LABEL_47": 47,
    "LABEL_48": 48,
    "LABEL_49": 49,
    "LABEL_5": 5,
    "LABEL_50": 50,
    "LABEL_51": 51,
    "LABEL_52": 52,
    "LABEL_53": 53,
    "LABEL_54": 54,
    "LABEL_55": 55,
    "LABEL_56": 56,
    "LABEL_57": 57,
    "LABEL_58": 58,
    "LABEL_59": 59,
    "LABEL_6": 6,
    "LABEL_60": 60,
    "LABEL_61": 61,
    "LABEL_62": 62,
    "LABEL_63": 63,
    "LABEL_64": 64,
    "LABEL_65": 65,
    "LABEL_66": 66,
    "LABEL_67": 67,
    "LABEL_68": 68,
    "LABEL_69": 69,
    "LABEL_7": 7,
    "LABEL_70": 70,
    "LABEL_71": 71,
    "LABEL_72": 72,
    "LABEL_73": 73,
    "LABEL_74": 74,
    "LABEL_75": 75,
    "LABEL_76": 76,
    "LABEL_77": 77,
    "LABEL_78": 78,
    "LABEL_79": 79,
    "LABEL_8": 8,
    "LABEL_80": 80,
    "LABEL_81": 81,
    "LABEL_82": 82,
    "LABEL_83": 83,
    "LABEL_84": 84,
    "LABEL_85": 85,
    "LABEL_86": 86,
    "LABEL_87": 87,
    "LABEL_88": 88,
    "LABEL_89": 89,
    "LABEL_9": 9,
    "LABEL_90": 90,
    "LABEL_91": 91,
    "LABEL_92": 92,
    "LABEL_93": 93,
    "LABEL_94": 94,
    "LABEL_95": 95,
    "LABEL_96": 96,
    "LABEL_97": 97,
    "LABEL_98": 98,
    "LABEL_99": 99
  },
  "layer_norm_eps": 1e-12,
  "model_type": "dpt",
  "neck_hidden_sizes": [
    256,
    512,
    768,
    768
  ],
  "neck_ignore_stages": [
    0,
    1
  ],
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "paddlenlp_version": null,
  "patch_size": 16,
  "qkv_bias": true,
  "readout_type": "project",
  "reassemble_factors": [
    1,
    1,
    1,
    0.5
  ],
  "return_dict": true,
  "semantic_classifier_dropout": 0.1,
  "semantic_loss_ignore_index": 255,
  "torch_dtype": "float32",
  "transformers_version": null,
  "use_auxiliary_head": true,
  "use_batch_norm_in_fusion_residual": false
}
[0m
_ StableDiffusionInpaintPipelineIntegrationTests.test_stable_diffusion_inpaint_pipeline _

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_inpaint.StableDiffusionInpaintPipelineIntegrationTests testMethod=test_stable_diffusion_inpaint_pipeline>

    def test_stable_diffusion_inpaint_pipeline(self):
        init_image = load_image(
            'https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-inpaint/init_image.png'
            )
        mask_image = load_image(
            'https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-inpaint/mask.png'
            )
        # invalid expected_image
        # expected_image = load_numpy(
        #     'https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/sd2-inpaint/yellow_cat_sitting_on_a_park_bench.npy'
        #     )
        model_id = 'stabilityai/stable-diffusion-2-inpainting'
        pipe = StableDiffusionInpaintPipeline.from_pretrained(model_id,
            safety_checker=None)
        pipe.set_progress_bar_config(disable=None)
        pipe.enable_attention_slicing()
        prompt = (
            'Face of a yellow cat, high resolution, sitting on a park bench')
        generator = paddle.Generator().manual_seed(0)
        output = pipe(prompt=prompt, image=init_image, mask_image=
            mask_image, generator=generator, output_type='np')
        image = output.images[0]
        assert image.shape == (512, 512, 3)
        image = image[-3:, -3:, -1]
        expected_image = [[[0.47980508], [0.49545538], [0.501472]], [[0.36860222], [0.5465546], [0.54940426]], [[0.44748512], [0.45160148], [0.48374733]]]
>       assert np.abs(expected_image - image).max() < 0.001
E       AssertionError: assert 0.2338601100440979 < 0.001
E        +  where 0.2338601100440979 = <built-in method max of numpy.ndarray object at 0x7f6e0bd2af30>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6e0bd2af30> = array([[[0.06228056, 0.05652162, 0.0476692 ],\n        [0.18276089, 0.01248619, 0.01042136],\n        [0.11578399, 0.11652765, 0.08009071]],\n\n       [[0.0489223 , 0.05468124, 0.06353366],\n        [0.23386011, 0.06358541, 0.06152058],\n        [0.16371625, 0.16445991, 0.12802297]],\n\n       [[0.0299606 , 0.02420166, 0.01534924],\n        [0.13890699, 0.03136771, 0.03343254],\n        [0.09805932, 0.09880298, 0.06236604]]]).max
E        +      where array([[[0.06228056, 0.05652162, 0.0476692 ],\n        [0.18276089, 0.01248619, 0.01042136],\n        [0.11578399, 0.11652765, 0.08009071]],\n\n       [[0.0489223 , 0.05468124, 0.06353366],\n        [0.23386011, 0.06358541, 0.06152058],\n        [0.16371625, 0.16445991, 0.12802297]],\n\n       [[0.0299606 , 0.02420166, 0.01534924],\n        [0.13890699, 0.03136771, 0.03343254],\n        [0.09805932, 0.09880298, 0.06236604]]]) = <ufunc 'absolute'>(([[[0.47980508], [0.49545538], [0.501472]], [[0.36860222], [0.5465546], [0.54940426]], [[0.44748512], [0.45160148], [0.48374733]]] - array([[0.41752452, 0.42328346, 0.43213588],\n       [0.3126945 , 0.4829692 , 0.48503402],\n       [0.385688  , 0.38494435, 0.4213813 ]], dtype=float32)))
E        +        where <ufunc 'absolute'> = np.abs

tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py:129: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:18:13,926] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-inpainting/snapshots/781cb3e2113c1932245692810716dfd27e355ab6/text_encoder/config.json[0m
[32m[2023-03-15 15:18:13,927] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "./hf-models/stable-diffusion-v2-inpainting/text_encoder",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_size": 1024,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 16,
  "num_hidden_layers": 23,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.25.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 15:18:30,469] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-inpainting/snapshots/781cb3e2113c1932245692810716dfd27e355ab6/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2-inpainting/snapshots/781cb3e2113c1932245692810716dfd27e355ab6/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 15:18:30,470] [    INFO][0m - size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.[0m
[32m[2023-03-15 15:18:30,470] [    INFO][0m - crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.[0m
[32m[2023-03-15 15:18:30,635] [    INFO][0m - Image processor CLIPFeatureExtractor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPFeatureExtractor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
_________ StableDiffusionLatentUpscalePipelineFastTests.test_inference _________

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_latent_upscale.StableDiffusionLatentUpscalePipelineFastTests testMethod=test_inference>

    def test_inference(self):
        components = self.get_dummy_components()
        pipe = self.pipeline_class(**components)
        pipe.set_progress_bar_config(disable=None)
        inputs = self.get_dummy_inputs()
        image = pipe(**inputs).images
        image_slice = image[0, -3:, -3:, -1]
        self.assertEqual(image.shape, (1, 256, 256, 3))
        expected_slice = np.array(
            [0.47222412, 0.41921633, 0.44717434, 0.46874192, 0.42588258, 0.46150726, 0.4677534, 0.45583832, 0.48579055] #TODO check this
        )
        max_diff = np.abs(image_slice.flatten() - expected_slice).max()
>       self.assertLessEqual(max_diff, 0.001)
E       AssertionError: 0.48579055 not less than or equal to 0.001

tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py:139: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:19:55,529] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/vocab.json[0m
[32m[2023-03-15 15:19:55,529] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/merges.txt[0m
[32m[2023-03-15 15:19:55,529] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json and saved to /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip[0m
[33m[2023-03-15 15:19:55,564] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json> not exist[0m
[32m[2023-03-15 15:19:55,564] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/special_tokens_map.json[0m
[32m[2023-03-15 15:19:55,564] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/tokenizer_config.json[0m
____ StableDiffusionUpscalePipelineFastTests.test_stable_diffusion_upscale _____

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_upscale.StableDiffusionUpscalePipelineFastTests testMethod=test_stable_diffusion_upscale>

    def test_stable_diffusion_upscale(self):
        unet = self.dummy_cond_unet_upscale
        low_res_scheduler = DDPMScheduler()
        scheduler = DDIMScheduler(prediction_type='v_prediction')
        vae = self.dummy_vae
        text_encoder = self.dummy_text_encoder
        tokenizer = CLIPTokenizer.from_pretrained(
            'hf-internal-testing/tiny-random-clip')
        image = self.dummy_image.cpu().transpose(perm=[0, 2, 3, 1])[0]
        low_res_image = Image.fromarray(np.uint8(image)).convert('RGB').resize(
            (64, 64))
        sd_pipe = StableDiffusionUpscalePipeline(unet=unet,
            low_res_scheduler=low_res_scheduler, scheduler=scheduler, vae=
            vae, text_encoder=text_encoder, tokenizer=tokenizer,
            max_noise_level=350)
        sd_pipe.set_progress_bar_config(disable=None)
        prompt = 'A painting of a squirrel eating a burger'
        generator = paddle.Generator().manual_seed(0)
        output = sd_pipe([prompt], image=low_res_image, generator=generator,
            guidance_scale=6.0, noise_level=20, num_inference_steps=2,
            output_type='np')
        image = output.images
        generator = paddle.Generator().manual_seed(0)
        image_from_tuple = sd_pipe([prompt], image=low_res_image, generator
            =generator, guidance_scale=6.0, noise_level=20,
            num_inference_steps=2, output_type='np', return_dict=False)[0]
        image_slice = image[0, -3:, -3:, -1]
        image_from_tuple_slice = image_from_tuple[0, -3:, -3:, -1]
        expected_height_width = low_res_image.size[0] * 4
        assert image.shape == (1, expected_height_width,
            expected_height_width, 3)
        expected_slice = np.array([0.2562, 0.3606, 0.4204, 0.4469, 0.4822,
            0.4647, 0.5315, 0.5748, 0.5606])
>       assert np.abs(image_slice.flatten() - expected_slice).max() < 0.01
E       AssertionError: assert 0.5748 < 0.01
E        +  where 0.5748 = <built-in method max of numpy.ndarray object at 0x7f6e0b4e81b0>()
E        +    where <built-in method max of numpy.ndarray object at 0x7f6e0b4e81b0> = array([0.2562    , 0.3606    , 0.04732875, 0.44265633, 0.3545357 ,\n       0.21857457, 0.34607973, 0.5748    , 0.05092676]).max
E        +      where array([0.2562    , 0.3606    , 0.04732875, 0.44265633, 0.3545357 ,\n       0.21857457, 0.34607973, 0.5748    , 0.05092676]) = <ufunc 'absolute'>((array([0.        , 0.        , 0.37307125, 0.00424367, 0.1276643 ,\n       0.68327457, 0.18542027, 0.        , 0.50967324], dtype=float32) - array([0.2562, 0.3606, 0.4204, 0.4469, 0.4822, 0.4647, 0.5315, 0.5748,\n       0.5606])))
E        +        where <ufunc 'absolute'> = np.abs
E        +        and   array([0.        , 0.        , 0.37307125, 0.00424367, 0.1276643 ,\n       0.68327457, 0.18542027, 0.        , 0.50967324], dtype=float32) = <built-in method flatten of numpy.ndarray object at 0x7f6e0b4e8e70>()
E        +          where <built-in method flatten of numpy.ndarray object at 0x7f6e0b4e8e70> = array([[0.        , 0.        , 0.37307125],\n       [0.00424367, 0.1276643 , 0.68327457],\n       [0.18542027, 0.        , 0.50967324]], dtype=float32).flatten

tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py:115: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:21:01,986] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/vocab.json[0m
[32m[2023-03-15 15:21:01,986] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/merges.txt[0m
[32m[2023-03-15 15:21:01,987] [    INFO][0m - Downloading https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json and saved to /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip[0m
[33m[2023-03-15 15:21:02,021] [ WARNING][0m - file<https://bj.bcebos.com/paddlenlp/models/community/hf-internal-testing/tiny-random-clip/added_tokens.json> not exist[0m
[32m[2023-03-15 15:21:02,021] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/special_tokens_map.json[0m
[32m[2023-03-15 15:21:02,022] [    INFO][0m - Already cached /root/mttest/test_caches/models/hf-internal-testing/tiny-random-clip/tokenizer_config.json[0m
_ StableDiffusion2VPredictionPipelineIntegrationTests.test_stable_diffusion_attention_slicing_v_pred _

self = <tests.pipelines.stable_diffusion_2.test_stable_diffusion_v_pred.StableDiffusion2VPredictionPipelineIntegrationTests testMethod=test_stable_diffusion_attention_slicing_v_pred>

    def test_stable_diffusion_attention_slicing_v_pred(self):
        model_id = 'stabilityai/stable-diffusion-2'
        pipe = StableDiffusionPipeline.from_pretrained(model_id,
            paddle_dtype=paddle.float16)
        pipe.set_progress_bar_config(disable=None)
        prompt = 'a photograph of an astronaut riding a horse'
        pipe.enable_attention_slicing()
        generator = paddle.Generator().manual_seed(0)
        output_chunked = pipe([prompt], generator=generator, guidance_scale
            =7.5, num_inference_steps=10, output_type='numpy')
        image_chunked = output_chunked.images
        mem_bytes = paddle.device.cuda.memory_allocated()
        assert mem_bytes < 5.5 * 10 ** 9
        pipe.disable_attention_slicing()
        generator = paddle.Generator().manual_seed(0)
        output = pipe([prompt], generator=generator, guidance_scale=7.5,
            num_inference_steps=10, output_type='numpy')
        image = output.images
        mem_bytes = paddle.device.cuda.memory_allocated()
>       assert mem_bytes > 5.5 * 10 ** 9
E       assert 2579965440 > (5.5 * (10 ** 9))

tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py:254: AssertionError
----------------------------- Captured stderr call -----------------------------
[32m[2023-03-15 15:21:45,805] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2/snapshots/07753ec23aeaf08862a5f6a8fcb0f9a883863b1b/text_encoder/config.json[0m
[32m[2023-03-15 15:21:45,806] [    INFO][0m - Model config CLIPTextConfig {
  "_name_or_path": "hf-models/stable-diffusion-v2-768x768/text_encoder",
  "architectures": [
    "CLIPTextModel"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "dropout": 0.0,
  "eos_token_id": 2,
  "hidden_act": "gelu",
  "hidden_size": 1024,
  "initializer_factor": 1.0,
  "initializer_range": 0.02,
  "intermediate_size": 4096,
  "layer_norm_eps": 1e-05,
  "max_position_embeddings": 77,
  "model_type": "clip_text_model",
  "num_attention_heads": 16,
  "num_hidden_layers": 23,
  "pad_token_id": 1,
  "paddlenlp_version": null,
  "projection_dim": 512,
  "return_dict": true,
  "torch_dtype": "float32",
  "transformers_version": "4.25.0.dev0",
  "vocab_size": 49408
}
[0m
[32m[2023-03-15 15:22:05,186] [    INFO][0m - loading configuration file /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2/snapshots/07753ec23aeaf08862a5f6a8fcb0f9a883863b1b/feature_extractor/preprocessor_config.json from cache at /root/mttest/test_caches/diffusers/models--stabilityai--stable-diffusion-2/snapshots/07753ec23aeaf08862a5f6a8fcb0f9a883863b1b/feature_extractor/preprocessor_config.json[0m
[32m[2023-03-15 15:22:05,187] [    INFO][0m - size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'shortest_edge': 224}.[0m
[32m[2023-03-15 15:22:05,187] [    INFO][0m - crop_size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.[0m
[32m[2023-03-15 15:22:05,188] [    INFO][0m - Image processor CLIPFeatureExtractor {
  "crop_size": {
    "height": 224,
    "width": 224
  },
  "do_center_crop": true,
  "do_convert_rgb": true,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "feature_extractor_type": "CLIPFeatureExtractor",
  "image_mean": [
    0.48145466,
    0.4578275,
    0.40821073
  ],
  "image_processor_type": "CLIPFeatureExtractor",
  "image_std": [
    0.26862954,
    0.26130258,
    0.27577711
  ],
  "resample": 3,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "shortest_edge": 224
  }
}
[0m
_ UnCLIPImageVariationPipelineIntegrationTests.test_unclip_image_variation_karlo _

self = <tests.pipelines.unclip.test_unclip_image_variation.UnCLIPImageVariationPipelineIntegrationTests testMethod=test_unclip_image_variation_karlo>

    def test_unclip_image_variation_karlo(self):
        input_image = load_image(
            'https://huggingface.co/datasets/hf-internal-testing/diffusers-images/resolve/main/unclip/cat.png'
            )
        expected_image = np.array([[0.09096909, 0.13343304, 0.26244187],
       [0.15095001, 0.19459972, 0.3182609 ]])
        # TODO(wugaosheng): test this function
        pipeline = UnCLIPImageVariationPipeline.from_pretrained(
>           'kakaobrain/karlo-v1-alpha-image-variations'
            )

tests/pipelines/unclip/test_unclip_image_variation.py:348: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
ppdiffusers/pipelines/pipeline_utils.py:928: in from_pretrained
    loaded_sub_model = load_method(cached_folder, **loading_kwargs)
ppdiffusers/ppnlp_patch_utils.py:775: in from_pretrained
    return from_pretrained_v3(cls, pretrained_model_name_or_path, from_hf_hub=from_hf_hub, subfolder=subfolder, *args, **kwargs)
ppdiffusers/ppnlp_patch_utils.py:640: in from_pretrained_v3
    **kwargs,
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddlenlp/transformers/clip/configuration.py:283: in from_pretrained
    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)
/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddlenlp/transformers/configuration_utils.py:762: in get_config_dict
    pretrained_model_name_or_path, cache_dir=cache_dir, from_hf_hub=from_hf_hub, **kwargs
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

cls = <class 'paddlenlp.transformers.clip.configuration.CLIPTextConfig'>
pretrained_model_name_or_path = '/root/mttest/test_caches/diffusers/models--kakaobrain--karlo-v1-alpha-image-variations/snapshots/4573828a41f333df688b1111b04add3d9f548331'
kwargs = {'return_unused_kwargs': False}
cache_dir = '/root/mttest/test_caches/diffusers/models--kakaobrain--karlo-v1-alpha-image-variations/snapshots/4573828a41f333df688b1111b04add3d9f548331'
from_hf_hub = 'True', subfolder = None, force_download = False
resolved_config_file = None
configuration_file = '/root/mttest/test_caches/diffusers/models--kakaobrain--karlo-v1-alpha-image-variations/snapshots/4573828a41f333df688b1111b04add3d9f548331/model_config.json'

    @classmethod
    def _get_config_dict(
        cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs
    ) -> Tuple[Dict[str, Any], Dict[str, Any]]:
        cache_dir = kwargs.pop("cache_dir", None)
        from_hf_hub = kwargs.pop("from_hf_hub", False)
        subfolder = kwargs.pop("subfolder", None)
    
        force_download = kwargs.pop("force_download", False)
        pretrained_model_name_or_path = str(pretrained_model_name_or_path)
    
        resolved_config_file = None
    
        # 0. init from pretrained_init_configuration
        if pretrained_model_name_or_path in cls.pretrained_init_configuration:
            # which can be: dict or url
            pretrained_model_name_or_path = cls.pretrained_init_configuration[pretrained_model_name_or_path]
    
            if isinstance(pretrained_model_name_or_path, dict):
                return pretrained_model_name_or_path, kwargs
    
        # 1. get the configuration file from local file, eg: /cache/path/model_config.json
        if os.path.isfile(pretrained_model_name_or_path):
            resolved_config_file = pretrained_model_name_or_path
    
        # 2. get the configuration file from url, eg: https://ip/path/to/model_config.json
        elif is_url(pretrained_model_name_or_path):
            resolved_config_file = get_path_from_url_with_filelock(
                pretrained_model_name_or_path, cache_dir, check_exist=not force_download
            )
        # 3. get the configuration file from local dir with default name, eg: /local/path
        elif os.path.isdir(pretrained_model_name_or_path):
            configuration_file = kwargs.pop("_configuration_file", CONFIG_NAME)
            configuration_file = os.path.join(pretrained_model_name_or_path, configuration_file)
            if os.path.exists(configuration_file):
                resolved_config_file = configuration_file
            else:
                # try to detect old-school config file
                configuration_file = os.path.join(pretrained_model_name_or_path, LEGACY_CONFIG_NAME)
                if os.path.exists(configuration_file):
                    resolved_config_file = configuration_file
                else:
                    raise FileNotFoundError(
>                       "please make sure there is `model_config.json` under the dir, or you can pass the `_configuration_file` "
                        "param into `from_pretarined` method to specific the configuration file name"
                    )  # 4. load it as the community resource file
E                   FileNotFoundError: please make sure there is `model_config.json` under the dir, or you can pass the `_configuration_file` param into `from_pretarined` method to specific the configuration file name

/root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddlenlp/transformers/configuration_utils.py:818: FileNotFoundError
=============================== warnings summary ===============================
../../anaconda3/envs/benchmark/lib/python3.7/site-packages/_distutils_hack/__init__.py:33
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/_distutils_hack/__init__.py:33: UserWarning: Setuptools is replacing distutils.
    warnings.warn("Setuptools is replacing distutils.")

tests/pipelines/altdiffusion/test_alt_diffusion.py: 11 warnings
  /root/mttest/testall/ppdiffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py:100: FutureWarning: The configuration file of this scheduler: DDIMScheduler {
    "_class_name": "DDIMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineFastTests::test_alt_diffusion_pndm
  /root/mttest/testall/ppdiffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py:100: FutureWarning: The configuration file of this scheduler: PNDMScheduler {
    "_class_name": "PNDMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.02,
    "beta_schedule": "linear",
    "beta_start": 0.0001,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "skip_prk_steps": true,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/altdiffusion/test_alt_diffusion.py: 2 warnings
tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py: 2 warnings
tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py: 6 warnings
tests/pipelines/stable_diffusion/test_cycle_diffusion.py: 2 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion.py: 16 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py: 9 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py: 8 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py: 7 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py: 3 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py: 2 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py: 2 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py: 10 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py: 1 warning
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py: 2 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py: 1 warning
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py: 8 warnings
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py: 5 warnings
tests/pipelines/stable_unclip/test_stable_unclip_img2img.py: 12 warnings
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddlenlp/transformers/clip/feature_extraction.py:30: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of PaddleNLP. Please use CLIPImageProcessor instead.
    FutureWarning,

tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineIntegrationTests::test_alt_diffusion
tests/pipelines/altdiffusion/test_alt_diffusion.py::AltDiffusionPipelineIntegrationTests::test_alt_diffusion_fast_ddim
  /root/mttest/testall/ppdiffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py:150: FutureWarning: The configuration file of the unet has set the default `sample_size` to smaller than 64 which seems highly unlikely. If your checkpoint is a fine-tuned version of any of the following: 
  - CompVis/stable-diffusion-v1-4 
  - CompVis/stable-diffusion-v1-3 
  - CompVis/stable-diffusion-v1-2 
  - CompVis/stable-diffusion-v1-1 
  - runwayml/stable-diffusion-v1-5 
  - runwayml/stable-diffusion-inpainting 
   you should change 'sample_size' to 64 in the configuration file. Please make sure to update the config accordingly as leaving `sample_size=32` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `unet/config.json` file
    deprecate("sample_size<64", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py::AltDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_default_case
tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py::AltDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_fp16
  /root/mttest/testall/ppdiffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py:144: FutureWarning: The configuration file of this scheduler: PNDMScheduler {
    "_class_name": "PNDMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.02,
    "beta_schedule": "linear",
    "beta_start": 0.0001,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "skip_prk_steps": true,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py::AltDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_pipeline_multiple_of_8
tests/pipelines/altdiffusion/test_alt_diffusion_img2img.py::AltDiffusionImg2ImgPipelineIntegrationTests::test_stable_diffusion_img2img_pipeline_default
  /root/mttest/testall/ppdiffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py:194: FutureWarning: The configuration file of the unet has set the default `sample_size` to smaller than 64 which seems highly unlikely. If your checkpoint is a fine-tuned version of any of the following: 
  - CompVis/stable-diffusion-v1-4 
  - CompVis/stable-diffusion-v1-3 
  - CompVis/stable-diffusion-v1-2 
  - CompVis/stable-diffusion-v1-1 
  - runwayml/stable-diffusion-v1-5 
  - runwayml/stable-diffusion-inpainting 
   you should change 'sample_size' to 64 in the configuration file. Please make sure to update the config accordingly as leaving `sample_size=32` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `unet/config.json` file
    deprecate("sample_size<64", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/latent_diffusion/test_latent_diffusion.py: 11 warnings
  /root/mttest/testall/ppdiffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py:94: FutureWarning: The configuration file of this scheduler: DDIMScheduler {
    "_class_name": "DDIMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineSlowTests::test_ldm_default_ddim
tests/pipelines/latent_diffusion/test_latent_diffusion.py::LDMTextToImagePipelineNightlyTests::test_ldm_default_ddim
  /root/mttest/testall/ppdiffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py:94: FutureWarning: The configuration file of this scheduler: DDIMScheduler {
    "_class_name": "DDIMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": true,
    "steps_offset": 0,
    "timestep_values": null,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/paint_by_example/test_paint_by_example.py::PaintByExamplePipelineIntegrationTests::test_paint_by_example
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1665: UserWarning: Skip loading for model.vision_model.position_ids. model.vision_model.position_ids is not found in the provided dict.
    warnings.warn(("Skip loading for {}. ".format(key) + str(err)))

tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py: 5 warnings
tests/pipelines/stable_diffusion/test_cycle_diffusion.py: 2 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion.py: 15 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py: 9 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py: 8 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py: 7 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py: 5 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py: 3 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py: 2 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py: 2 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py: 10 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py: 1 warning
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py: 8 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py: 2 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py: 3 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py: 1 warning
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py: 8 warnings
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py: 4 warnings
tests/pipelines/unclip/test_unclip.py: 1 warning
tests/pipelines/versatile_diffusion/test_versatile_diffusion_dual_guided.py: 2 warnings
tests/pipelines/versatile_diffusion/test_versatile_diffusion_mega.py: 2 warnings
tests/pipelines/versatile_diffusion/test_versatile_diffusion_text_to_image.py: 2 warnings
tests/pipelines/vq_diffusion/test_vq_diffusion.py: 1 warning
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1665: UserWarning: Skip loading for text_model.position_ids. text_model.position_ids is not found in the provided dict.
    warnings.warn(("Skip loading for {}. ".format(key) + str(err)))

tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py: 10 warnings
tests/pipelines/stable_diffusion/test_cycle_diffusion.py: 2 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion.py: 32 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py: 19 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py: 15 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py: 11 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py: 6 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py: 6 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py: 4 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py: 21 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py: 3 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py: 18 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py: 5 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py: 4 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py: 2 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py: 20 warnings
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py: 6 warnings
tests/pipelines/unclip/test_unclip.py: 5 warnings
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/safetensors/torch.py:99: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    with safe_open(filename, framework="pt", device=device) as f:

tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py: 10 warnings
tests/pipelines/stable_diffusion/test_cycle_diffusion.py: 2 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion.py: 32 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py: 19 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py: 15 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py: 11 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py: 6 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py: 6 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py: 4 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py: 21 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py: 3 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py: 18 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py: 5 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py: 4 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py: 2 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py: 20 warnings
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py: 6 warnings
tests/pipelines/unclip/test_unclip.py: 5 warnings
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/torch/_utils.py:771: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    return self.fget.__get__(instance, owner)()

tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py: 10 warnings
tests/pipelines/stable_diffusion/test_cycle_diffusion.py: 2 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion.py: 32 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py: 19 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py: 15 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py: 11 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_panorama.py: 6 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py: 6 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py: 4 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py: 21 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py: 3 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py: 18 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py: 5 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py: 4 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py: 2 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py: 20 warnings
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py: 6 warnings
tests/pipelines/unclip/test_unclip.py: 5 warnings
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/torch/storage.py:899: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
    storage = cls(wrap_storage=untyped_storage)

tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py: 4 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion.py: 11 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py: 2 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py: 4 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py: 4 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py: 4 warnings
tests/pipelines/stable_diffusion/test_stable_diffusion_sag.py: 1 warning
tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py: 1 warning
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py: 1 warning
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1665: UserWarning: Skip loading for clip.vision_model.position_ids. clip.vision_model.position_ids is not found in the provided dict.
    warnings.warn(("Skip loading for {}. ".format(key) + str(err)))

tests/pipelines/stable_diffusion/test_cycle_diffusion.py: 11 warnings
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py:169: FutureWarning: The configuration file of this scheduler: DDIMScheduler {
    "_class_name": "DDIMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion.py: 20 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py: 11 warnings
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:99: FutureWarning: The configuration file of this scheduler: DDIMScheduler {
    "_class_name": "DDIMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_height_width_opt
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_long_prompt
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_vae_slicing
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_k_lms
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_long_prompt
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:99: FutureWarning: The configuration file of this scheduler: LMSDiscreteScheduler {
    "_class_name": "LMSDiscreteScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_negative_prompt
tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_num_images_per_prompt
tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_pndm
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:99: FutureWarning: The configuration file of this scheduler: PNDMScheduler {
    "_class_name": "PNDMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.02,
    "beta_schedule": "linear",
    "beta_start": 0.0001,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "skip_prk_steps": true,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineFastTests::test_stable_diffusion_no_safety_checker
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:149: FutureWarning: The configuration file of the unet has set the default `sample_size` to smaller than 64 which seems highly unlikely. If your checkpoint is a fine-tuned version of any of the following: 
  - CompVis/stable-diffusion-v1-4 
  - CompVis/stable-diffusion-v1-3 
  - CompVis/stable-diffusion-v1-2 
  - CompVis/stable-diffusion-v1-1 
  - runwayml/stable-diffusion-v1-5 
  - runwayml/stable-diffusion-inpainting 
   you should change 'sample_size' to 64 in the configuration file. Please make sure to update the config accordingly as leaving `sample_size=32` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `unet/config.json` file
    deprecate("sample_size<64", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py: 18 warnings
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py:77: DeprecationWarning: The 'warn' method is deprecated, use 'warning' instead
    f"You have disabled the safety checker for {self.__class__} by passing `safety_checker=None`. Ensure"

tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineSlowTests::test_stable_diffusion_img_variation_intermediate_state
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineSlowTests::test_stable_diffusion_img_variation_pipeline_default
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineNightlyTests::test_img_variation_dpm
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineNightlyTests::test_img_variation_pndm
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1665: UserWarning: Skip loading for vision_projection. vision_projection receives a shape [1024, 768], but the expected shape is [1024, 512].
    warnings.warn(("Skip loading for {}. ".format(key) + str(err)))

tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineSlowTests::test_stable_diffusion_img_variation_intermediate_state
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineSlowTests::test_stable_diffusion_img_variation_pipeline_default
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineNightlyTests::test_img_variation_dpm
tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineNightlyTests::test_img_variation_pndm
tests/pipelines/versatile_diffusion/test_versatile_diffusion_dual_guided.py::VersatileDiffusionDualGuidedPipelineIntegrationTests::test_inference_dual_guided
tests/pipelines/versatile_diffusion/test_versatile_diffusion_dual_guided.py::VersatileDiffusionDualGuidedPipelineIntegrationTests::test_remove_unused_weights_save_load
tests/pipelines/versatile_diffusion/test_versatile_diffusion_image_variation.py::VersatileDiffusionImageVariationPipelineIntegrationTests::test_inference_image_variations
tests/pipelines/versatile_diffusion/test_versatile_diffusion_mega.py::VersatileDiffusionMegaPipelineIntegrationTests::test_from_save_pretrained
tests/pipelines/versatile_diffusion/test_versatile_diffusion_mega.py::VersatileDiffusionMegaPipelineIntegrationTests::test_inference_dual_guided_then_text_to_image
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/fluid/dygraph/layers.py:1665: UserWarning: Skip loading for vision_model.position_ids. vision_model.position_ids is not found in the provided dict.
    warnings.warn(("Skip loading for {}. ".format(key) + str(err)))

tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py: 14 warnings
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:142: FutureWarning: The configuration file of this scheduler: PNDMScheduler {
    "_class_name": "PNDMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.02,
    "beta_schedule": "linear",
    "beta_start": 0.0001,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "skip_prk_steps": true,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_num_images_per_prompt
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:467: FutureWarning: You have passed 2 text prompts (`prompt`), but only 1 initial images (`image`). Initial images are now duplicating to match the number of text prompts. Note that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update your script to pass as many initial images as text prompts to suppress this warning.
    deprecate("len(prompt) != len(image)", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineFastTests::test_stable_diffusion_img2img_num_images_per_prompt
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py:467: FutureWarning: You have passed 4 text prompts (`prompt`), but only 1 initial images (`image`). Initial images are now duplicating to match the number of text prompts. Note that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update your script to pass as many initial images as text prompts to suppress this warning.
    deprecate("len(prompt) != len(image)", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py: 13 warnings
tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py: 11 warnings
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py:192: FutureWarning: The configuration file of this scheduler: PNDMScheduler {
    "_class_name": "PNDMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.02,
    "beta_schedule": "linear",
    "beta_start": 0.0001,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "skip_prk_steps": true,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineFastTests::test_stable_diffusion_inpaint_legacy
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineFastTests::test_stable_diffusion_inpaint_legacy_negative_prompt
tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint_legacy.py::StableDiffusionInpaintLegacyPipelineFastTests::test_stable_diffusion_inpaint_legacy_num_images_per_prompt
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py:130: FutureWarning: The configuration file of this scheduler: PNDMScheduler {
    "_class_name": "PNDMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.02,
    "beta_schedule": "linear",
    "beta_start": 0.0001,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "skip_prk_steps": true,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_stable_diffusion_pix2pix_num_images_per_prompt
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py:608: FutureWarning: You have passed 2 text prompts (`prompt`), but only 1 initial images (`image`). Initial images are now duplicating to match the number of text prompts. Note that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update your script to pass as many initial images as text prompts to suppress this warning.
    deprecate("len(prompt) != len(image)", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineFastTests::test_stable_diffusion_pix2pix_num_images_per_prompt
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py:608: FutureWarning: You have passed 4 text prompts (`prompt`), but only 1 initial images (`image`). Initial images are now duplicating to match the number of text prompts. Note that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update your script to pass as many initial images as text prompts to suppress this warning.
    deprecate("len(prompt) != len(image)", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineFastTests::test_float16_inference
  /root/anaconda3/envs/benchmark/lib/python3.7/site-packages/paddle/optimizer/sgd.py:140: UserWarning: Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence.Consider using multi_precision=True option of the Adam optimizer.
    "Accumulating with FP16 in optimizer can lead to poor accuracy or slow convergence."

tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_k_euler
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:99: FutureWarning: The configuration file of this scheduler: EulerDiscreteScheduler {
    "_class_name": "EulerDiscreteScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "interpolation_type": "linear",
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_k_euler_ancestral
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:99: FutureWarning: The configuration file of this scheduler: EulerAncestralDiscreteScheduler {
    "_class_name": "EulerAncestralDiscreteScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_stable_diffusion_depth2img_num_images_per_prompt
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py:387: FutureWarning: You have passed 2 text prompts (`prompt`), but only 1 initial images (`image`). Initial images are now duplicating to match the number of text prompts. Note that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update your script to pass as many initial images as text prompts to suppress this warning.
    deprecate("len(prompt) != len(image)", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_stable_diffusion_depth2img_num_images_per_prompt
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py:387: FutureWarning: You have passed 4 text prompts (`prompt`), but only 1 initial images (`image`). Initial images are now duplicating to match the number of text prompts. Note that this behavior is deprecated and will be removed in a version 1.0.0. Please make sure to update your script to pass as many initial images as text prompts to suppress this warning.
    deprecate("len(prompt) != len(image)", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py::StableDiffusionUpscalePipelineFastTests::test_stable_diffusion_upscale
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py::StableDiffusionUpscalePipelineFastTests::test_stable_diffusion_upscale_batch
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py::StableDiffusionUpscalePipelineFastTests::test_stable_diffusion_upscale_fp16
tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py::StableDiffusionUpscalePipelineIntegrationTests::test_stable_diffusion_upscale_pipeline
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py:104: FutureWarning: The configuration file of the vae does not contain `scaling_factor` or it is set to 0.18215, which seems highly unlikely. If your checkpoint is a fine-tuned version of `stabilityai/stable-diffusion-x4-upscaler` you should change 'scaling_factor' to 0.08333 Please make sure to update the config accordingly, as not doing so might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull Request for the `vae/config.json` file
    deprecate("wrong scaling_factor", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineFastTests::test_stable_diffusion_v_pred_ddim
tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineFastTests::test_stable_diffusion_v_pred_fp16
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py:99: FutureWarning: The configuration file of this scheduler: DDIMScheduler {
    "_class_name": "DDIMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "num_train_timesteps": 1000,
    "prediction_type": "v_prediction",
    "set_alpha_to_one": false,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineFastTests::test_safe_diffusion_ddim
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py:95: FutureWarning: The configuration file of this scheduler: DDIMScheduler {
    "_class_name": "DDIMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.012,
    "beta_schedule": "scaled_linear",
    "beta_start": 0.00085,
    "clip_sample": false,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineFastTests::test_stable_diffusion_fp16
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineFastTests::test_stable_diffusion_pndm
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py:95: FutureWarning: The configuration file of this scheduler: PNDMScheduler {
    "_class_name": "PNDMScheduler",
    "_diffusers_version": "0.13.1",
    "_ppdiffusers_version": "0.13.1",
    "beta_end": 0.02,
    "beta_schedule": "linear",
    "beta_start": 0.0001,
    "num_train_timesteps": 1000,
    "prediction_type": "epsilon",
    "set_alpha_to_one": false,
    "skip_prk_steps": true,
    "steps_offset": 0,
    "trained_betas": null
  }
   is outdated. `steps_offset` should be set to 1 instead of 0. Please make sure to update the config accordingly as leaving `steps_offset` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `scheduler/scheduler_config.json` file
    deprecate("steps_offset!=1", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineFastTests::test_stable_diffusion_no_safety_checker
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py:145: FutureWarning: The configuration file of the unet has set the default `sample_size` to smaller than 64 which seems highly unlikely .If your checkpoint is a fine-tuned version of any of the following: 
  - CompVis/stable-diffusion-v1-4 
  - CompVis/stable-diffusion-v1-3 
  - CompVis/stable-diffusion-v1-2 
  - CompVis/stable-diffusion-v1-1 
  - runwayml/stable-diffusion-v1-5 
  - runwayml/stable-diffusion-inpainting 
   you should change 'sample_size' to 64 in the configuration file. Please make sure to update the config accordingly as leaving `sample_size=32` in the config might lead to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `unet/config.json` file
    deprecate("sample_size<64", "1.0.0", deprecation_message, standard_warn=False)

tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineIntegrationTests::test_harm_safe_stable_diffusion
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineIntegrationTests::test_nudity_safe_stable_diffusion
tests/pipelines/stable_diffusion_safe/test_safe_diffusion.py::SafeDiffusionPipelineIntegrationTests::test_nudity_safetychecker_safe_stable_diffusion
  /root/mttest/testall/ppdiffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py:589: UserWarning: Safety checker disabled!
    warnings.warn("Safety checker disabled!")

tests/pipelines/vq_diffusion/test_vq_diffusion.py::VQDiffusionPipelineFastTests::test_vq_diffusion
tests/pipelines/vq_diffusion/test_vq_diffusion.py::VQDiffusionPipelineFastTests::test_vq_diffusion_classifier_free_sampling
tests/pipelines/vq_diffusion/test_vq_diffusion.py::VQDiffusionPipelineIntegrationTests::test_vq_diffusion_classifier_free_sampling
  /root/mttest/testall/ppdiffusers/models/transformer_2d.py:122: FutureWarning: The configuration file of this model: <class 'ppdiffusers.models.transformer_2d.Transformer2DModel'> is outdated. `norm_type` is either not set or incorrectly set to `'layer_norm'`.Make sure to set `norm_type` to `'ada_norm'` in the config. Please make sure to update the config accordingly as leaving `norm_type` might led to incorrect results in future versions. If you have downloaded this checkpoint from the Hugging Face Hub, it would be very nice if you could open a Pull request for the `transformer/config.json` file
    deprecate("norm_type!=num_embeds_ada_norm", "1.0.0", deprecation_message, standard_warn=False)

-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html
=========================== short test summary info ============================
FAILED tests/pipelines/karras_ve/test_karras_ve.py::KarrasVePipelineFastTests::test_inference
FAILED tests/pipelines/semantic_stable_diffusion/test_semantic_diffusion.py::SafeDiffusionPipelineFastTests::test_semantic_diffusion_ddim
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_attention_slicing
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion.py::StableDiffusionPipelineSlowTests::test_stable_diffusion_fp16_vs_autocast
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_canny
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_depth
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_hed
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_mlsd
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_normal
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_openpose
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_scribble
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_controlnet.py::StableDiffusionControlNetPipelineSlowTests::test_seg
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineSlowTests::test_stable_diffusion_img_variation_intermediate_state
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineSlowTests::test_stable_diffusion_img_variation_pipeline_default
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineNightlyTests::test_img_variation_dpm
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_image_variation.py::StableDiffusionImageVariationPipelineNightlyTests::test_img_variation_pndm
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_img2img.py::StableDiffusionImg2ImgPipelineNightlyTests::test_img2img_pndm
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineSlowTests::test_stable_diffusion_inpaint_fp16
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineNightlyTests::test_inpaint_dpm
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_ddim
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_default
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_intermediate_state
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_k_lms
FAILED tests/pipelines/stable_diffusion/test_stable_diffusion_instruction_pix2pix.py::StableDiffusionInstructPix2PixPipelineSlowTests::test_stable_diffusion_pix2pix_pipeline_multiple_of_8
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_ddim
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_k_euler
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineFastTests::test_stable_diffusion_k_euler_ancestral
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion.py::StableDiffusion2PipelineNightlyTests::test_stable_diffusion_2_0_default_ddim
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_attend_and_excite.py::StableDiffusionAttendAndExcitePipelineIntegrationTests::test_attend_and_excite_fp16
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionDepth2ImgPipelineFastTests::test_stable_diffusion_depth2img_multiple_init_images
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionImg2ImgPipelineNightlyTests::test_depth2img_ddim
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionImg2ImgPipelineNightlyTests::test_depth2img_pndm
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionImg2ImgPipelineNightlyTests::test_img2img_dpm
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_depth.py::StableDiffusionImg2ImgPipelineNightlyTests::test_img2img_lms
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_inpaint.py::StableDiffusionInpaintPipelineIntegrationTests::test_stable_diffusion_inpaint_pipeline
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_latent_upscale.py::StableDiffusionLatentUpscalePipelineFastTests::test_inference
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_upscale.py::StableDiffusionUpscalePipelineFastTests::test_stable_diffusion_upscale
FAILED tests/pipelines/stable_diffusion_2/test_stable_diffusion_v_pred.py::StableDiffusion2VPredictionPipelineIntegrationTests::test_stable_diffusion_attention_slicing_v_pred
FAILED tests/pipelines/unclip/test_unclip_image_variation.py::UnCLIPImageVariationPipelineIntegrationTests::test_unclip_image_variation_karlo
ERROR tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineSlowTests::test_stable_diffusion_pix2pix_zero_default
ERROR tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineSlowTests::test_stable_diffusion_pix2pix_zero_intermediate_state
ERROR tests/pipelines/stable_diffusion/test_stable_diffusion_pix2pix_zero.py::StableDiffusionPix2PixZeroPipelineSlowTests::test_stable_diffusion_pix2pix_zero_k_lms
= 39 failed, 545 passed, 3 skipped, 981 warnings, 3 errors in 6888.19s (1:54:48) =
